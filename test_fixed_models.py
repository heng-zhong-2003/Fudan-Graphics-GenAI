#!/usr/bin/env python3  
"""  
æµ‹è¯•ä¿®å¤åçš„æ¨¡å‹æ•ˆæœ  
"""  

import torch  
from transformers import AutoTokenizer, AutoModelForCausalLM  
import os  
import json  

def load_model_with_params(params_path, model_name):  
    """åŠ è½½å¸¦æœ‰ç‰¹å®šå‚æ•°çš„æ¨¡å‹"""  
    print(f"ğŸ”„ åŠ è½½ {model_name}...")  
    
    try:  
        # åŠ è½½åŸºç¡€æ¨¡å‹  
        tokenizer = AutoTokenizer.from_pretrained("../models/BlenderLLM", trust_remote_code=True)  
        model = AutoModelForCausalLM.from_pretrained(  
            "../models/BlenderLLM",  
            trust_remote_code=True,  
            torch_dtype=torch.float16,  
            device_map={"": 0}  
        )  
        
        # åŠ è½½å‚æ•°  
        if os.path.exists(params_path):  
            custom_params = torch.load(params_path, map_location="cpu")  
            
            updated_count = 0  
            for name, param in model.named_parameters():  
                if name in custom_params:  
                    # æ£€æŸ¥å‚æ•°æ˜¯å¦å®‰å…¨  
                    new_param = custom_params[name]  
                    if not (torch.isnan(new_param).any() or torch.isinf(new_param).any()):  
                        param.data = new_param.to(param.device)  
                        updated_count += 1  
                        print(f"âœ… æ›´æ–°å‚æ•°: {name}")  
                    else:  
                        print(f"âš ï¸ è·³è¿‡æŸåå‚æ•°: {name}")  
            
            print(f"âœ… æ€»å…±æ›´æ–°äº† {updated_count} ä¸ªå‚æ•°")  
        else:  
            print(f"âš ï¸ å‚æ•°æ–‡ä»¶ä¸å­˜åœ¨: {params_path}")  
        
        return model, tokenizer  
        
    except Exception as e:  
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")  
        return None, None  

def safe_generate(model, tokenizer, prompt):  
    """å®‰å…¨ç”Ÿæˆï¼ˆé¿å…CUDAé”™è¯¯ï¼‰"""  
    try:  
        inputs = tokenizer(prompt, return_tensors="pt", max_length=100, truncation=True)  
        device = next(model.parameters()).device  
        inputs = {k: v.to(device) for k, v in inputs.items()}  
        
        with torch.no_grad():  
            outputs = model.generate(  
                **inputs,  
                max_length=len(inputs['input_ids'][0]) + 100,  
                temperature=0.5,  # æ›´ä¿å®ˆçš„æ¸©åº¦  
                do_sample=True,  
                top_p=0.8,  
                pad_token_id=tokenizer.eos_token_id,  
                repetition_penalty=1.1,  
                num_return_sequences=1  
            )  
        
        result = tokenizer.decode(outputs[0], skip_special_tokens=True)  
        generated = result[len(prompt):].strip()  
        return generated  
        
    except Exception as e:  
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")  
        return ""  

def test_all_available_models():  
    """æµ‹è¯•æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹ç‰ˆæœ¬"""  
    print("ğŸ§ª æµ‹è¯•æ‰€æœ‰å¯ç”¨æ¨¡å‹")  
    print("=" * 60)  
    
    test_prompt = "Generate chair design: simple wooden chair"  
    
    # æ¨¡å‹åˆ—è¡¨  
    models_to_test = [  
        ("åŸå§‹æ¨¡å‹", None),  
        ("ä¿®å¤æ¨¡å‹", "./output/fixed_model/trainable_params.pt"),  
        ("å®‰å…¨æ¨¡å‹", "./output/safe_model/trainable_params.pt"),  
    ]  
    
    results = {}  
    
    for model_name, params_path in models_to_test:  
        print(f"\n{'='*20} {model_name} {'='*20}")  
        
        if params_path is None:  
            # åŸå§‹æ¨¡å‹  
            try:  
                tokenizer = AutoTokenizer.from_pretrained("../models/BlenderLLM", trust_remote_code=True)  
                model = AutoModelForCausalLM.from_pretrained(  
                    "../models/BlenderLLM",  
                    trust_remote_code=True,  
                    torch_dtype=torch.float16,  
                    device_map={"": 0}  
                )  
                print("âœ… åŸå§‹æ¨¡å‹åŠ è½½æˆåŠŸ")  
            except Exception as e:  
                print(f"âŒ åŸå§‹æ¨¡å‹åŠ è½½å¤±è´¥: {e}")  
                continue  
        else:  
            # å¸¦å‚æ•°çš„æ¨¡å‹  
            if not os.path.exists(params_path):  
                print(f"âš ï¸ è·³è¿‡ä¸å­˜åœ¨çš„æ¨¡å‹: {params_path}")  
                continue  
            
            model, tokenizer = load_model_with_params(params_path, model_name)  
            if model is None:  
                continue  
        
        # æµ‹è¯•ç”Ÿæˆ  
        print(f"ğŸ¯ æµ‹è¯•æç¤º: {test_prompt}")  
        generated = safe_generate(model, tokenizer, test_prompt)  
        
        if generated:  
            print(f"âœ… ç”ŸæˆæˆåŠŸ ({len(generated)} å­—ç¬¦)")  
            print("ç”Ÿæˆå†…å®¹é¢„è§ˆ:")  
            print(generated[:200] + "..." if len(generated) > 200 else generated)  
            
            # ç®€å•è¯„åˆ†  
            score = 0  
            if 'import bpy' in generated: score += 3  
            if any(word in generated.lower() for word in ['chair', 'cube', 'cylinder']): score += 2  
            if 'bpy.ops' in generated: score += 2  
            if len(generated) > 50: score += 1  
            if not any(word in generated for word in ['error', 'failed']): score += 2  
            
            print(f"ğŸ“Š è´¨é‡è¯„åˆ†: {score}/10")  
            results[model_name] = {"success": True, "score": score, "length": len(generated)}  
        else:  
            print("âŒ ç”Ÿæˆå¤±è´¥")  
            results[model_name] = {"success": False, "score": 0, "length": 0}  
        
        # æ¸…ç†å†…å­˜  
        del model  
        torch.cuda.empty_cache()  
        print("ğŸ§¹ å†…å­˜å·²æ¸…ç†")  
    
    # æ€»ç»“  
    print("\n" + "="*60)  
    print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“")  
    print("="*60)  
    
    for model_name, result in results.items():  
        status = "âœ… æˆåŠŸ" if result["success"] else "âŒ å¤±è´¥"  
        print(f"{model_name}: {status}, è¯„åˆ†: {result['score']}/10")  
    
    # æ¨è  
    successful_models = [name for name, result in results.items() if result["success"]]  
    if successful_models:  
        best_model = max(successful_models, key=lambda x: results[x]["score"])  
        print(f"\nğŸ† æ¨èä½¿ç”¨: {best_model}")  
    else:  
        print("\nâš ï¸ æ‰€æœ‰æ¨¡å‹éƒ½æœ‰é—®é¢˜ï¼Œå»ºè®®æ£€æŸ¥ç¯å¢ƒ")  
    
    return results  

if __name__ == "__main__":  
    test_all_available_models()  
