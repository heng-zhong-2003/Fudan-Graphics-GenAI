#!/usr/bin/env python3  
"""  
å†…å­˜ä¼˜åŒ–çš„æ¤…å­è®¾è®¡å¾®è°ƒ - ç»“åˆæœ€å°åŒ–è®­ç»ƒç­–ç•¥  
"""  

import torch  
import json  
from transformers import AutoTokenizer, AutoModelForCausalLM  
import os  
import gc  

# å¼ºåˆ¶å•GPUå’Œå†…å­˜ä¼˜åŒ–  
os.environ["CUDA_VISIBLE_DEVICES"] = "0"  
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"  

def clear_gpu_memory():  
    """å½»åº•æ¸…ç†GPUå†…å­˜"""  
    gc.collect()  
    torch.cuda.empty_cache()  
    torch.cuda.synchronize()  

def memory_optimized_train():  
    print("ğŸª‘ å†…å­˜ä¼˜åŒ–æ¤…å­è®¾è®¡å¾®è°ƒ...")  
    
    # å½»åº•æ¸…ç†å†…å­˜  
    clear_gpu_memory()  
    
    # æ£€æŸ¥GPUçŠ¶æ€  
    print(f"ğŸ” GPUå†…å­˜çŠ¶æ€:")  
    print(f"  æ€»å†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")  
    print(f"  å·²åˆ†é…: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB")  
    print(f"  ç¼“å­˜: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB")  
    
    # åŠ è½½tokenizer  
    print("ğŸ”„ åŠ è½½tokenizer...")  
    tokenizer = AutoTokenizer.from_pretrained("../models/BlenderLLM", trust_remote_code=True)  
    if tokenizer.pad_token is None:  
        tokenizer.pad_token = tokenizer.eos_token  
    
    # ä½¿ç”¨æœ€ä¿å®ˆçš„æ–¹å¼åŠ è½½æ¨¡å‹  
    print("ğŸ”„ åŠ è½½æ¨¡å‹ï¼ˆå†…å­˜ä¼˜åŒ–æ¨¡å¼ï¼‰...")  
    try:  
        model = AutoModelForCausalLM.from_pretrained(  
            "../models/BlenderLLM",  
            trust_remote_code=True,  
            torch_dtype=torch.float16,  # ä½¿ç”¨åŠç²¾åº¦  
            device_map="auto",          # è‡ªåŠ¨åˆ†é…è®¾å¤‡  
            low_cpu_mem_usage=True,     # ä½CPUå†…å­˜ä½¿ç”¨  
            offload_folder="./temp_offload",  # ä¸´æ—¶å¸è½½æ–‡ä»¶å¤¹  
            max_memory={0: "20GB", "cpu": "30GB"}  # é™åˆ¶GPUå†…å­˜ä½¿ç”¨  
        )  
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")  
    except Exception as e:  
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")  
        print("ğŸ”„ å°è¯•æ›´æ¿€è¿›çš„å†…å­˜ä¼˜åŒ–...")  
        
        # æ›´æ¿€è¿›çš„å†…å­˜ä¼˜åŒ–  
        model = AutoModelForCausalLM.from_pretrained(  
            "../models/BlenderLLM",  
            trust_remote_code=True,  
            torch_dtype=torch.float16,  
            device_map="auto",  
            low_cpu_mem_usage=True,  
            offload_folder="./temp_offload",  
            max_memory={0: "15GB", "cpu": "50GB"}  # æ›´ä¿å®ˆçš„GPUå†…å­˜é™åˆ¶  
        )  
    
    # æ£€æŸ¥æ¨¡å‹å‚æ•°åˆ†å¸ƒ  
    gpu_params = 0  
    cpu_params = 0  
    for name, param in model.named_parameters():  
        if param.device.type == 'cuda':  
            gpu_params += param.numel()  
        else:  
            cpu_params += param.numel()  
    
    print(f"ğŸ“Š å‚æ•°åˆ†å¸ƒ:")  
    print(f"  GPUå‚æ•°: {gpu_params:,}")  
    print(f"  CPUå‚æ•°: {cpu_params:,}")  
    
    # å†»ç»“å¤§éƒ¨åˆ†å‚æ•°ï¼Œåªè®­ç»ƒæœ€åå‡ å±‚  
    print("ğŸ”’ å†»ç»“å¤§éƒ¨åˆ†å‚æ•°...")  
    for name, param in model.named_parameters():  
        param.requires_grad = False  
    
    # åªè§£å†»æœ€å2å±‚å’Œè¾“å‡ºå±‚  
    layers_to_train = []  
    for name, param in model.named_parameters():  
        # æ ¹æ®å®é™…æ¨¡å‹ç»“æ„è°ƒæ•´å±‚å  
        if any(layer in name for layer in ['layers.31', 'layers.30', 'lm_head', 'embed_out']):  
            param.requires_grad = True  
            layers_to_train.append(name)  
    
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)  
    total_params = sum(p.numel() for p in model.parameters())  
    
    print(f"ğŸ¯ è®­ç»ƒå‚æ•°:")  
    print(f"  å¯è®­ç»ƒ: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)")  
    print(f"  è®­ç»ƒå±‚: {len(layers_to_train)}")  
    
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœå†…å­˜  
    if hasattr(model, 'gradient_checkpointing_enable'):  
        model.gradient_checkpointing_enable()  
        print("âœ… å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")  
    
    # åŠ è½½è®­ç»ƒæ•°æ®  
    print("ğŸ“Š åŠ è½½è®­ç»ƒæ•°æ®...")  
    try:  
        with open('./output/new_training_data/chair_training_data.json', 'r', encoding='utf-8') as f:  
            data = json.load(f)  
    except FileNotFoundError:  
        print("âŒ è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºç¤ºä¾‹æ•°æ®...")  
        data = create_sample_data()  
    
    # åªä½¿ç”¨å°‘é‡æ•°æ®è¿›è¡Œè®­ç»ƒ  
    data = data[:20]  # è¿›ä¸€æ­¥å‡å°‘æ•°æ®é‡  
    print(f"ğŸ“ˆ ä½¿ç”¨ {len(data)} ä¸ªè®­ç»ƒæ ·æœ¬")  
    
    # è®¾ç½®ä¼˜åŒ–å™¨  
    optimizer = torch.optim.AdamW(  
        [p for p in model.parameters() if p.requires_grad],   
        lr=5e-6,  # æ›´å°çš„å­¦ä¹ ç‡  
        weight_decay=0.01  
    )  
    
    model.train()  
    
    print("ğŸ‹ï¸ å¼€å§‹è®­ç»ƒ...")  
    
    for epoch in range(1):  # åªè®­ç»ƒ1ä¸ªepoch  
        total_loss = 0  
        valid_steps = 0  
        
        for i, item in enumerate(data):  
            if i % 5 == 0:  # æ›´é¢‘ç¹çš„å†…å­˜æ¸…ç†  
                clear_gpu_memory()  
                print(f"  æ­¥éª¤ {i}/{len(data)} - GPUå†…å­˜: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB")  
            
            try:  
                # æ„å»ºè®­ç»ƒæ–‡æœ¬  
                text = f"Human: {item.get('input', '')}\n\nAssistant: {item.get('output', '')}"  
                
                # é™åˆ¶æ–‡æœ¬é•¿åº¦ä»¥èŠ‚çœå†…å­˜  
                if len(text) > 400:  
                    text = text[:400]  
                
                # Tokenize  
                inputs = tokenizer(  
                    text,   
                    return_tensors="pt",   
                    max_length=256,  # æ›´çŸ­çš„åºåˆ—é•¿åº¦  
                    truncation=True,   
                    padding=True  
                )  
                
                # å°†è¾“å…¥ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡  
                inputs = {k: v.to(model.device) if hasattr(model, 'device') else v.cuda()   
                         for k, v in inputs.items()}  
                
                # å‰å‘ä¼ æ’­  
                outputs = model(**inputs, labels=inputs['input_ids'])  
                loss = outputs.loss  
                
                # æ£€æŸ¥æŸå¤±æ˜¯å¦æœ‰æ•ˆ  
                if torch.isnan(loss) or torch.isinf(loss):  
                    print(f"  âš ï¸ è·³è¿‡æ— æ•ˆæŸå¤±: {loss.item()}")  
                    continue  
                
                # åå‘ä¼ æ’­  
                loss.backward()  
                
                # æ¢¯åº¦ç´¯ç§¯ï¼ˆæ¯4æ­¥æ›´æ–°ä¸€æ¬¡ï¼‰  
                if (i + 1) % 4 == 0:  
                    # æ¢¯åº¦è£å‰ª  
                    torch.nn.utils.clip_grad_norm_(  
                        [p for p in model.parameters() if p.requires_grad],   
                        max_norm=1.0  
                    )  
                    
                    optimizer.step()  
                    optimizer.zero_grad()  
                
                total_loss += loss.item()  
                valid_steps += 1  
                
            except Exception as e:  
                print(f"  âŒ æ­¥éª¤ {i} å¤±è´¥: {e}")  
                clear_gpu_memory()  
                continue  
        
        avg_loss = total_loss / valid_steps if valid_steps > 0 else float('inf')  
        print(f"ğŸ“ˆ Epoch {epoch+1} å¹³å‡æŸå¤±: {avg_loss:.4f} (æœ‰æ•ˆæ­¥éª¤: {valid_steps}/{len(data)})")  
    
    # ä¿å­˜æ¨¡å‹  
    print("ğŸ’¾ ä¿å­˜æ¨¡å‹...")  
    output_dir = "./output/memory_optimized_model"  
    os.makedirs(output_dir, exist_ok=True)  
    
    try:  
        model.save_pretrained(output_dir)  
        tokenizer.save_pretrained(output_dir)  
        print(f"âœ… æ¨¡å‹ä¿å­˜åˆ°: {output_dir}")  
    except Exception as e:  
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")  
    
    # æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹  
    print("\nğŸ§ª æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹...")  
    test_model(model, tokenizer)  
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶  
    if os.path.exists("./temp_offload"):  
        import shutil  
        shutil.rmtree("./temp_offload")  
        print("ğŸ—‘ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶")  

def create_sample_data():  
    """åˆ›å»ºç¤ºä¾‹è®­ç»ƒæ•°æ®"""  
    return [  
        {  
            "input": "Generate chair design: modern office chair",  
            "output": "import bpy\n\n# Create modern office chair\nbpy.ops.mesh.primitive_cube_add(location=(0, 0, 0.5))\nchair_seat = bpy.context.active_object\nchair_seat.scale = (1, 1, 0.1)\n\n# Add backrest\nbpy.ops.mesh.primitive_cube_add(location=(0, -0.9, 1.2))\nbackrest = bpy.context.active_object\nbackrest.scale = (1, 0.1, 0.7)"  
        },  
        {  
            "input": "Generate chair design: wooden dining chair",  
            "output": "import bpy\n\n# Create wooden dining chair\nbpy.ops.mesh.primitive_cube_add(location=(0, 0, 0.4))\nseat = bpy.context.active_object\nseat.scale = (0.8, 0.8, 0.05)\n\n# Add wooden legs\nfor i, pos in enumerate([(-0.7, -0.7, 0.2), (0.7, -0.7, 0.2), (-0.7, 0.7, 0.2), (0.7, 0.7, 0.2)]):\n    bpy.ops.mesh.primitive_cylinder_add(location=pos)\n    leg = bpy.context.active_object\n    leg.scale = (0.05, 0.05, 0.4)"  
        }  
    ]  

def test_model(model, tokenizer):  
    """æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹"""  
    model.eval()  
    
    test_prompts = [  
        "Generate chair design: simple wooden chair",  
        "Generate chair design: ergonomic office chair"  
    ]  
    
    for prompt in test_prompts:  
        try:  
            inputs = tokenizer(prompt, return_tensors="pt")  
            inputs = {k: v.to(model.device) if hasattr(model, 'device') else v.cuda()   
                     for k, v in inputs.items()}  
            
            with torch.no_grad():  
                outputs = model.generate(  
                    **inputs,  
                    max_length=len(inputs['input_ids'][0]) + 200,  
                    temperature=0.7,  
                    do_sample=True,  
                    pad_token_id=tokenizer.eos_token_id,  
                    repetition_penalty=1.1  
                )  
            
            result = tokenizer.decode(outputs[0], skip_special_tokens=True)  
            generated = result[len(prompt):].strip()  
            
            print(f"ğŸ“ æç¤º: {prompt}")  
            print(f"ğŸ¤– ç”Ÿæˆ: {generated[:200]}...")  
            print("-" * 50)  
            
        except Exception as e:  
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")  

if __name__ == "__main__":  
    memory_optimized_train()  
