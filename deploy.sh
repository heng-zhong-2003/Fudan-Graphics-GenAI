#!/bin/bash  
# deploy.sh - ä¸€é”®éƒ¨ç½²è„šæœ¬  

set -e  

echo "ðŸª‘ Chair Style Generation - One-Click Deployment"  
echo "=================================================="  

# æ£€æŸ¥ç³»ç»Ÿè¦æ±‚  
check_requirements() {  
    echo "ðŸ” Checking system requirements..."  
    
    # æ£€æŸ¥Python  
    if ! command -v python3 &> /dev/null; then  
        echo "âŒ Python 3 not found. Please install Python 3.8+"  
        exit 1  
    fi  
    
    # æ£€æŸ¥GPU  
    if command -v nvidia-smi &> /dev/null; then  
        echo "âœ… NVIDIA GPU detected"  
        nvidia-smi --query-gpu=name,memory.total --format=csv,noheader  
    else  
        echo "âš ï¸  No NVIDIA GPU detected. CPU-only mode will be used."  
    fi  
    
    # æ£€æŸ¥ç£ç›˜ç©ºé—´  
    available_space=$(df . | awk 'NR==2{print $4}')  
    required_space=10485760  # 10GB in KB  
    
    if [ "$available_space" -lt "$required_space" ]; then  
        echo "âŒ Insufficient disk space. At least 10GB required."  
        exit 1  
    fi  
    
    echo "âœ… System requirements met"  
}  

# å®‰è£…ä¾èµ–  
install_dependencies() {  
    echo "ðŸ“¦ Installing dependencies..."  
    
    # åˆ›å»ºè™šæ‹ŸçŽ¯å¢ƒ  
    python3 -m venv chair_env  
    source chair_env/bin/activate  
    
    # å‡çº§pip  
    pip install --upgrade pip  
    
    # å®‰è£…PyTorch (æ ¹æ®ç³»ç»Ÿé€‰æ‹©ç‰ˆæœ¬)  
    if command -v nvidia-smi &> /dev/null; then  
        echo "Installing PyTorch with CUDA support..."  
        pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118  
    else  
        echo "Installing PyTorch CPU version..."  
        pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu  
    fi  
    
    # å®‰è£…å…¶ä»–ä¾èµ–  
    pip install transformers datasets accelerate  
    pip install matplotlib pandas numpy scipy  
    pip install tqdm rich psutil GPUtil  
    pip install pytest black pre-commit  
    
    echo "âœ… Dependencies installed"  
}  

# è®¾ç½®é¡¹ç›®ç»“æž„  
setup_project() {  
    echo "ðŸ“ Setting up project structure..."  
    
    # åˆ›å»ºç›®å½•  
    mkdir -p {config,examples,output,logs,tests,scripts}  
    mkdir -p output/{models,generation_results,evaluations,performance}  
    
    # è®¾ç½®æƒé™  
    chmod +x scripts/*.sh 2>/dev/null || true  
    
    echo "âœ… Project structure created"  
}  

# é…ç½®çŽ¯å¢ƒ  
configure_environment() {  
    echo "âš™ï¸  Configuring environment..."  
    
    # åˆ›å»ºçŽ¯å¢ƒå˜é‡æ–‡ä»¶  
    cat > .env << EOF  
# Chair Style Generation Environment Variables  
PYTHONPATH=\$(pwd)  
CUDA_VISIBLE_DEVICES=0  
TRANSFORMERS_CACHE=./cache/transformers  
HF_HOME=./cache/huggingface  
BLENDER_PATH=/usr/bin/blender  
EOF  

    # åˆ›å»ºæ¿€æ´»è„šæœ¬  
    cat > activate.sh << 'EOF'  
#!/bin/bash  
source chair_env/bin/activate  
source .env  
export PYTHONPATH=$(pwd)  
echo "ðŸª‘ Chair Style Generation environment activated"  
echo "Python: $(which python)"  
echo "GPU: $(python -c 'import torch; print("Available" if torch.cuda.is_available() else "Not available")')"  
EOF  
    
    chmod +x activate.sh  
    
    echo "âœ… Environment configured"  
}  

# ä¸‹è½½æˆ–é…ç½®æ¨¡åž‹  
setup_models() {  
    echo "ðŸ¤– Setting up models..."  
    
    # æ£€æŸ¥BlenderLLMæ¨¡åž‹è·¯å¾„  
    if [ ! -d "/home/saisai/graph/models/BlenderLLM" ]; then  
        echo "âš ï¸  BlenderLLM model not found at expected path"  
        echo "Please ensure the model is available or update the path in config/default.json"  
    else  
        echo "âœ… BlenderLLM model found"  
    fi  
    
    # åˆ›å»ºæ¨¡åž‹ç¼“å­˜ç›®å½•  
    mkdir -p cache/{transformers,huggingface}  
    
    echo "âœ… Model setup completed"  
}  

# éªŒè¯å®‰è£…  
validate_installation() {  
    echo "ðŸ”¬ Validating installation..."  
    
    source chair_env/bin/activate  
    
    # æµ‹è¯•Pythonå¯¼å…¥  
    python3 -c "  
import torch  
import transformers  
import matplotlib  
import pandas  
import numpy  
print('âœ… All Python packages imported successfully')  
print(f'PyTorch version: {torch.__version__}')  
print(f'CUDA available: {torch.cuda.is_available()}')  
if torch.cuda.is_available():  
    print(f'GPU count: {torch.cuda.device_count()}')  
    print(f'Current GPU: {torch.cuda.get_device_name(0)}')  
"  
    
    # æµ‹è¯•è„šæœ¬è¯­æ³•  
    echo "ðŸ” Checking script syntax..."  
    python3 -m py_compile quick_start.py  
    python3 -m py_compile train_chair_model.py  
    python3 -m py_compile batch_process.py  
    
    # è¿è¡Œç®€å•æµ‹è¯•  
    echo "ðŸ§ª Running basic tests..."  
    if [ -f "tests/test_style_preprocessing.py" ]; then  
        python3 -m pytest tests/test_style_preprocessing.py -v  
    fi  
    
    echo "âœ… Installation validated"  
}  

# åˆ›å»ºç¤ºä¾‹é…ç½®  
create_sample_config() {  
    echo "ðŸ“ Creating sample configuration..."  
    
    # æ£€æµ‹ç³»ç»Ÿè·¯å¾„  
    DATA_PATH="/home/saisai/graph/Fudan-Graphics-GenAI/data_grouped"  
    MODEL_PATH="/home/saisai/graph/models/BlenderLLM"  
    
    # å¦‚æžœè·¯å¾„ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç›¸å¯¹è·¯å¾„  
    if [ ! -d "$DATA_PATH" ]; then  
        DATA_PATH="./data"  
        echo "âš ï¸  Creating placeholder data directory: $DATA_PATH"  
        mkdir -p "$DATA_PATH"  
    fi  
    
    if [ ! -d "$MODEL_PATH" ]; then  
        MODEL_PATH="./models/BlenderLLM"  
        echo "âš ï¸  Model path not found, using: $MODEL_PATH"  
    fi  
    
    # ç”Ÿæˆé…ç½®æ–‡ä»¶  
    cat > config/production.json << EOF  
{  
    "data_path": "$DATA_PATH",  
    "base_model": "$MODEL_PATH",  
    "styles_file": "examples/chair_styles.txt",  
    "output_dir": "./output",  
    "epochs": 3,  
    "batch_size": 2,  
    "max_workers": 2,  
    "num_test_samples": 10,  
    "training": {  
        "learning_rate": 2e-5,  
        "warmup_steps": 100,  
        "logging_steps": 50,  
        "save_steps": 500,  
        "eval_steps": 500,  
        "max_seq_length": 512,  
        "gradient_accumulation_steps": 1,  
        "fp16": true,  
        "dataloader_pin_memory": true  
    },  
    "generation": {  
        "max_new_tokens": 256,  
        "temperature": 0.7,  
        "top_p": 0.9,  
        "top_k": 50,  
        "do_sample": true,  
        "num_return_sequences": 1  
    },  
    "evaluation": {  
        "metrics": ["bleu", "rouge", "success_rate"],  
        "save_failed_cases": true,  
        "generate_visualizations": true  
    }  
}  
EOF  

    # ç”Ÿæˆå¼€å‘é…ç½®  
    cat > config/development.json << EOF  
{  
    "data_path": "$DATA_PATH",  
    "base_model": "$MODEL_PATH",   
    "styles_file": "examples/chair_styles.txt",  
    "output_dir": "./output/dev",  
    "epochs": 1,  
    "batch_size": 1,  
    "max_workers": 1,  
    "num_test_samples": 3,  
    "training": {  
        "learning_rate": 1e-4,  
        "warmup_steps": 10,  
        "logging_steps": 10,  
        "save_steps": 100,  
        "eval_steps": 100,  
        "max_seq_length": 256,  
        "gradient_accumulation_steps": 2,  
        "fp16": false  
    },  
    "generation": {  
        "max_new_tokens": 128,  
        "temperature": 0.8,  
        "top_p": 0.9,  
        "do_sample": true  
    }  
}  
EOF  

    echo "âœ… Sample configurations created"  
}  

# åˆ›å»ºç®¡ç†è„šæœ¬  
create_management_scripts() {  
    echo "ðŸ“œ Creating management scripts..."  
    
    # è®­ç»ƒè„šæœ¬  
    cat > scripts/train.sh << 'EOF'  
#!/bin/bash  
source ./activate.sh  

echo "ðŸ‹ï¸ Starting model training..."  
python quick_start.py --mode train --config config/production.json "$@"  
EOF  

    # æŽ¨ç†è„šæœ¬  
    cat > scripts/inference.sh << 'EOF'  
#!/bin/bash  
source ./activate.sh  

echo "ðŸŽ¨ Starting inference..."  
python quick_start.py --mode inference --config config/production.json "$@"  
EOF  

    # å®Œæ•´æµæ°´çº¿è„šæœ¬  
    cat > scripts/full_pipeline.sh << 'EOF'  
#!/bin/bash  
source ./activate.sh  

echo "ðŸš€ Starting full pipeline..."  
python quick_start.py --mode full --config config/production.json "$@"  
EOF  

    # å¼€å‘æ¨¡å¼è„šæœ¬  
    cat > scripts/dev.sh << 'EOF'  
#!/bin/bash  
source ./activate.sh  

echo "ðŸ”§ Running in development mode..."  
python quick_start.py --mode full --config config/development.json "$@"  
EOF  

    # æ€§èƒ½ç›‘æŽ§è„šæœ¬  
    cat > scripts/monitor.sh << 'EOF'  
#!/bin/bash  
source ./activate.sh  

echo "ðŸ“Š Starting performance monitoring..."  
python monitor_performance.py --action start --log_file output/performance/monitor.json "$@"  
EOF  

    # æµ‹è¯•è„šæœ¬  
    cat > scripts/test.sh << 'EOF'  
#!/bin/bash  
source ./activate.sh  

echo "ðŸ§ª Running tests..."  
python -m pytest tests/ -v --tb=short  
EOF  

    # æ¸…ç†è„šæœ¬  
    cat > scripts/clean.sh << 'EOF'  
#!/bin/bash  

echo "ðŸ§¹ Cleaning up..."  
rm -rf output/dev/  
rm -rf output/generation_results/temp_*  
rm -rf output/models/temp_*  
rm -rf logs/*.tmp  
rm -rf __pycache__/  
find . -name "*.pyc" -delete  
find . -name ".pytest_cache" -type d -exec rm -rf {} + 2>/dev/null || true  

echo "âœ… Cleanup completed"  
EOF  

    # è®¾ç½®æ‰§è¡Œæƒé™  
    chmod +x scripts/*.sh  
    
    echo "âœ… Management scripts created"  
}  

# ç”Ÿæˆä½¿ç”¨æ–‡æ¡£  
generate_usage_docs() {  
    echo "ðŸ“š Generating usage documentation..."  
    
    cat > QUICK_START.md << 'EOF'  
# ðŸª‘ Chair Style Generation - Quick Start  

## ðŸš€ Fast Track  

```bash  
# 1. Activate environment  
source activate.sh  

# 2. Run development pipeline (fast test)  
./scripts/dev.sh  

# 3. Run full production pipeline  
./scripts/full_pipeline.sh