#!/usr/bin/env python3  
"""  
æµ‹è¯•åŸå§‹BlenderLLMæ¨¡å‹çš„æ¤…å­ç”Ÿæˆèƒ½åŠ›  
"""  

import torch  
from transformers import AutoTokenizer, AutoModelForCausalLM  
import os  

def test_original_model():  
    print("ğŸ§ª æµ‹è¯•åŸå§‹BlenderLLMæ¨¡å‹...")  
    
    os.environ["CUDA_VISIBLE_DEVICES"] = "0"  
    torch.cuda.empty_cache()  
    
    # åŠ è½½åŸå§‹æ¨¡å‹  
    tokenizer = AutoTokenizer.from_pretrained("../models/BlenderLLM", trust_remote_code=True)  
    model = AutoModelForCausalLM.from_pretrained(  
        "../models/BlenderLLM",  
        trust_remote_code=True,  
        torch_dtype=torch.float16,  
        device_map="auto"  
    )  
    
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")  
    
    # è¯¦ç»†æµ‹è¯•æ¤…å­ç”Ÿæˆ  
    test_prompts = [  
        "Generate chair design: modern minimalist",  
        "Generate chair design: vintage wooden dining chair",  
        "Generate chair design: ergonomic office chair",  
        "Generate chair design: comfortable armchair",  
        "Generate chair design: industrial bar stool",  
        "Create a simple wooden chair",  
        "Design a modern office chair",  
        "Make a comfortable reading chair",  
        "Build a dining room chair",  
        "Create a stylish accent chair"  
    ]  
    
    print(f"\nğŸ¯ æµ‹è¯• {len(test_prompts)} ä¸ªæ¤…å­è®¾è®¡æç¤º...")  
    
    results = []  
    
    for i, prompt in enumerate(test_prompts):  
        print(f"\n{'='*60}")  
        print(f"æµ‹è¯• {i+1}/{len(test_prompts)}: {prompt}")  
        
        try:  
            inputs = tokenizer(prompt, return_tensors="pt")  
            inputs = {k: v.to(model.device) for k, v in inputs.items()}  
            
            with torch.no_grad():  
                outputs = model.generate(  
                    **inputs,  
                    max_length=len(inputs['input_ids'][0]) + 500,  
                    temperature=0.7,  
                    do_sample=True,  
                    top_p=0.9,  
                    pad_token_id=tokenizer.eos_token_id,  
                    repetition_penalty=1.1  
                )  
            
            result = tokenizer.decode(outputs[0], skip_special_tokens=True)  
            generated = result[len(prompt):].strip()  
            
            print(f"ğŸ“ ç”Ÿæˆå†…å®¹ ({len(generated)} å­—ç¬¦):")  
            print(f"{generated}")  
            
            # åˆ†æç”Ÿæˆå†…å®¹  
            analysis = analyze_output(generated)  
            print(f"\nğŸ“Š å†…å®¹åˆ†æ:")  
            print(f"  âœ… åŒ…å«Blenderä»£ç : {analysis['has_blender_code']}")  
            print(f"  âœ… åŒ…å«æ¤…å­ç›¸å…³: {analysis['has_chair_content']}")  
            print(f"  âœ… ä»£ç ç»“æ„åˆç†: {analysis['has_good_structure']}")  
            print(f"  ğŸ“ é•¿åº¦é€‚ä¸­: {analysis['good_length']}")  
            print(f"  â­ æ€»ä½“è¯„åˆ†: {analysis['score']}/5")  
            
            results.append({  
                'prompt': prompt,  
                'generated': generated,  
                'analysis': analysis  
            })  
            
        except Exception as e:  
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")  
            results.append({  
                'prompt': prompt,  
                'generated': '',  
                'analysis': {'score': 0}  
            })  
    
    # æ€»ç»“  
    print(f"\n{'='*60}")  
    print("ğŸ“Š æµ‹è¯•æ€»ç»“:")  
    
    total_score = sum(r['analysis'].get('score', 0) for r in results)  
    avg_score = total_score / len(results)  
    
    successful_generations = len([r for r in results if r['generated']])  
    
    print(f"  æˆåŠŸç”Ÿæˆ: {successful_generations}/{len(results)}")  
    print(f"  å¹³å‡è¯„åˆ†: {avg_score:.2f}/5")  
    
    # æ˜¾ç¤ºæœ€ä½³ç»“æœ  
    best_result = max(results, key=lambda x: x['analysis'].get('score', 0))  
    print(f"\nğŸ† æœ€ä½³ç”Ÿæˆç»“æœ:")  
    print(f"  æç¤º: {best_result['prompt']}")  
    print(f"  è¯„åˆ†: {best_result['analysis'].get('score', 0)}/5")  
    print(f"  å†…å®¹: {best_result['generated'][:300]}...")  
    
    return results  

def analyze_output(text):  
    """åˆ†æç”Ÿæˆå†…å®¹çš„è´¨é‡"""  
    analysis = {  
        'has_blender_code': False,  
        'has_chair_content': False,  
        'has_good_structure': False,  
        'good_length': False,  
        'score': 0  
    }  
    
    text_lower = text.lower()  
    
    # æ£€æŸ¥Blenderä»£ç   
    blender_keywords = ['bpy.', 'import bpy', 'mesh.', 'object.', 'add_object', 'ops.']  
    if any(keyword in text for keyword in blender_keywords):  
        analysis['has_blender_code'] = True  
        analysis['score'] += 2  
    
    # æ£€æŸ¥æ¤…å­ç›¸å…³å†…å®¹  
    chair_keywords = ['chair', 'seat', 'leg', 'backrest', 'armrest', 'cushion']  
    if any(keyword in text_lower for keyword in chair_keywords):  
        analysis['has_chair_content'] = True  
        analysis['score'] += 1  
    
    # æ£€æŸ¥ä»£ç ç»“æ„  
    if ('def ' in text or 'bpy.ops.' in text) and 'location' in text:  
        analysis['has_good_structure'] = True  
        analysis['score'] += 1  
    
    # æ£€æŸ¥é•¿åº¦  
    if 100 < len(text) < 2000:  
        analysis['good_length'] = True  
        analysis['score'] += 1  
    
    return analysis  

if __name__ == "__main__":  
    test_original_model()  
