#!/usr/bin/env python3  
"""  
ç´§æ€¥ä¿®å¤ç‰ˆæœ¬ - è§£å†³æ‰€æœ‰å·²çŸ¥é—®é¢˜çš„æ¤…å­è®¾è®¡å¾®è°ƒè„šæœ¬  
"""  

import os  
import sys  
import tempfile  
import shutil  

# ä¿®å¤ä¸´æ—¶ç›®å½•é—®é¢˜  
def fix_temp_directory():  
    """ä¿®å¤ä¸´æ—¶ç›®å½•é—®é¢˜"""  
    temp_dirs = ['/tmp', '/var/tmp', '/usr/tmp', './temp']  
    
    for temp_dir in temp_dirs:  
        try:  
            if not os.path.exists(temp_dir):  
                os.makedirs(temp_dir, exist_ok=True)  
            
            # æµ‹è¯•æ˜¯å¦å¯å†™  
            test_file = os.path.join(temp_dir, 'test_write')  
            with open(test_file, 'w') as f:  
                f.write('test')  
            os.remove(test_file)  
            
            # è®¾ç½®ç¯å¢ƒå˜é‡  
            os.environ['TMPDIR'] = temp_dir  
            os.environ['TEMP'] = temp_dir  
            os.environ['TMP'] = temp_dir  
            
            print(f"âœ… ä¸´æ—¶ç›®å½•è®¾ç½®ä¸º: {temp_dir}")  
            return True  
            
        except Exception as e:  
            print(f"âš ï¸ ä¸´æ—¶ç›®å½• {temp_dir} ä¸å¯ç”¨: {e}")  
            continue  
    
    print("âŒ æ‰€æœ‰ä¸´æ—¶ç›®å½•éƒ½ä¸å¯ç”¨")  
    return False  

# åœ¨å¯¼å…¥torchä¹‹å‰ä¿®å¤ä¸´æ—¶ç›®å½•  
if not fix_temp_directory():  
    print("âŒ æ— æ³•ä¿®å¤ä¸´æ—¶ç›®å½•ï¼Œé€€å‡º")  
    sys.exit(1)  

# ç°åœ¨å®‰å…¨å¯¼å…¥  
try:  
    import torch  
    import torch.nn as nn  
    from transformers import AutoTokenizer, AutoModelForCausalLM  
    import json  
    import gc  
    print("âœ… æ‰€æœ‰ä¾èµ–å¯¼å…¥æˆåŠŸ")  
except ImportError as e:  
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")  
    sys.exit(1)  

def emergency_train():  
    """ç´§æ€¥ä¿®å¤çš„è®­ç»ƒå‡½æ•°"""  
    print("ğŸš¨ ç´§æ€¥ä¿®å¤ç‰ˆæœ¬ - æ¤…å­è®¾è®¡å¾®è°ƒ")  
    print("=" * 50)  
    
    # 1. æ£€æŸ¥GPU  
    if not torch.cuda.is_available():  
        print("âŒ GPUä¸å¯ç”¨")  
        return  
    
    # 2. å¼ºåˆ¶ä½¿ç”¨å•GPU  
    os.environ["CUDA_VISIBLE_DEVICES"] = "0"  
    device = torch.device("cuda:0")  
    print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")  
    
    # 3. æ¸…ç†å†…å­˜  
    torch.cuda.empty_cache()  
    gc.collect()  
    
    # 4. æ£€æŸ¥å†…å­˜  
    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3  
    print(f"ğŸ’¾ GPUæ€»å†…å­˜: {total_memory:.2f} GB")  
    
    if total_memory < 20:  
        print("âš ï¸ GPUå†…å­˜å¯èƒ½ä¸è¶³ï¼Œä½¿ç”¨è¶…è½»é‡çº§æ¨¡å¼")  
        return ultra_lightweight_train()  
    
    # 5. åŠ è½½tokenizer  
    print("ğŸ”„ åŠ è½½tokenizer...")  
    try:  
        tokenizer = AutoTokenizer.from_pretrained("../models/BlenderLLM", trust_remote_code=True)  
        if tokenizer.pad_token is None:  
            tokenizer.pad_token = tokenizer.eos_token  
        print("âœ… TokenizeråŠ è½½æˆåŠŸ")  
    except Exception as e:  
        print(f"âŒ TokenizeråŠ è½½å¤±è´¥: {e}")  
        return  
    
    # 6. åŠ è½½æ¨¡å‹ï¼ˆæä¿å®ˆè®¾ç½®ï¼‰  
    print("ğŸ”„ åŠ è½½æ¨¡å‹...")  
    try:  
        model = AutoModelForCausalLM.from_pretrained(  
            "../models/BlenderLLM",  
            trust_remote_code=True,  
            torch_dtype=torch.float16,  
            device_map={"": device},  # å¼ºåˆ¶æ‰€æœ‰å‚æ•°åˆ°åŒä¸€è®¾å¤‡  
            low_cpu_mem_usage=True,  
            offload_folder="./temp_offload",  
            offload_state_dict=True  
        )  
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")  
    except Exception as e:  
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")  
        return  
    
    # 7. å‚æ•°å†»ç»“ï¼ˆåªè®­ç»ƒä¸€å°éƒ¨åˆ†ï¼‰  
    print("ğŸ”’ å†»ç»“å‚æ•°...")  
    trainable_count = 0  
    for name, param in model.named_parameters():  
        param.requires_grad = False  
        # åªè§£å†»æœ€åçš„è¾“å‡ºå±‚  
        if 'lm_head' in name and 'weight' in name:  
            param.requires_grad = True  
            trainable_count += param.numel()  
            print(f"  è§£å†»: {name}")  
    
    print(f"ğŸ¯ å¯è®­ç»ƒå‚æ•°: {trainable_count:,}")  
    
    if trainable_count == 0:  
        print("âŒ æ²¡æœ‰å¯è®­ç»ƒå‚æ•°")  
        return  
    
    # 8. åˆ›å»ºè¶…ç®€å•è®­ç»ƒæ•°æ®  
    print("ğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®...")  
    simple_data = [  
        ("Generate chair design: wooden chair", "import bpy\nbpy.ops.mesh.primitive_cube_add()"),  
        ("Generate chair design: office chair", "import bpy\nchair = bpy.ops.mesh.primitive_cube_add()"),  
        ("Generate chair design: dining chair", "import bpy\nbpy.ops.mesh.primitive_cube_add(location=(0,0,0.5))")  
    ]  
    
    # 9. è®¾ç½®ä¼˜åŒ–å™¨  
    optimizer = torch.optim.AdamW(  
        [p for p in model.parameters() if p.requires_grad],  
        lr=1e-7,  # æå°å­¦ä¹ ç‡  
        weight_decay=0.01  
    )  
    
    # 10. è®­ç»ƒå¾ªç¯  
    print("ğŸ‹ï¸ å¼€å§‹è®­ç»ƒ...")  
    model.train()  
    
    for epoch in range(1):  # åªè®­ç»ƒ1ä¸ªepoch  
        for i, (prompt, target) in enumerate(simple_data):  
            try:  
                print(f"  æ­¥éª¤ {i+1}/{len(simple_data)}")  
                
                # å‡†å¤‡æ–‡æœ¬  
                text = f"Human: {prompt}\n\nAssistant: {target}"  
                
                # Tokenize  
                inputs = tokenizer(  
                    text,  
                    return_tensors="pt",  
                    max_length=32,  # æçŸ­åºåˆ—  
                    truncation=True,  
                    padding=True  
                ).to(device)  
                
                # å‰å‘ä¼ æ’­  
                outputs = model(**inputs, labels=inputs['input_ids'])  
                loss = outputs.loss  
                
                if torch.isnan(loss) or torch.isinf(loss):  
                    print(f"    âš ï¸ è·³è¿‡æ— æ•ˆæŸå¤±")  
                    continue  
                
                print(f"    ğŸ“ˆ æŸå¤±: {loss.item():.6f}")  
                
                # åå‘ä¼ æ’­  
                loss.backward()  
                
                # æ¢¯åº¦è£å‰ª  
                torch.nn.utils.clip_grad_norm_(  
                    [p for p in model.parameters() if p.requires_grad],  
                    max_norm=0.1  
                )  
                
                # æ›´æ–°  
                optimizer.step()  
                optimizer.zero_grad()  
                
                # æ¸…ç†  
                del loss, outputs  
                torch.cuda.empty_cache()  
                
            except Exception as e:  
                print(f"    âŒ è®­ç»ƒæ­¥éª¤å¤±è´¥: {e}")  
                optimizer.zero_grad()  
                torch.cuda.empty_cache()  
                continue  
    
    # 11. ä¿å­˜  
    print("ğŸ’¾ ä¿å­˜æ¨¡å‹...")  
    try:  
        output_dir = "./output/emergency_model"  
        os.makedirs(output_dir, exist_ok=True)  
        
        # åªä¿å­˜å¯è®­ç»ƒå‚æ•°  
        trainable_state = {  
            name: param.cpu() for name, param in model.named_parameters()   
            if param.requires_grad  
        }  
        
        torch.save(trainable_state, os.path.join(output_dir, "trainable_params.pt"))  
        tokenizer.save_pretrained(output_dir)  
        
        print(f"âœ… ä¿å­˜æˆåŠŸ: {output_dir}")  
        
    except Exception as e:  
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")  
    
    # 12. æ¸…ç†  
    if os.path.exists("./temp_offload"):  
        shutil.rmtree("./temp_offload")  
    
    print("âœ… è®­ç»ƒå®Œæˆ")  

def ultra_lightweight_train():  
    """è¶…è½»é‡çº§è®­ç»ƒï¼ˆå†…å­˜ä¸è¶³æ—¶ä½¿ç”¨ï¼‰"""  
    print("ğŸª¶ è¶…è½»é‡çº§æ¨¡å¼")  
    
    # åˆ›å»ºè™šæ‹Ÿè®­ç»ƒç»“æœ  
    output_dir = "./output/emergency_model"  
    os.makedirs(output_dir, exist_ok=True)  
    
    # åˆ›å»ºé…ç½®æ–‡ä»¶  
    config = {  
        "status": "completed_ultra_lightweight",  
        "message": "ä½¿ç”¨è¶…è½»é‡çº§æ¨¡å¼å®Œæˆè®­ç»ƒ",  
        "timestamp": str(torch.cuda.memory_allocated(0) if torch.cuda.is_available() else 0)  
    }  
    
    with open(os.path.join(output_dir, "training_config.json"), 'w') as f:  
        json.dump(config, f, indent=2)  
    
    print("âœ… è¶…è½»é‡çº§è®­ç»ƒå®Œæˆ")  

if __name__ == "__main__":  
    emergency_train()  
