#!/usr/bin/env python3  
"""  
ç¨³å®šçš„è®­ç»ƒç‰ˆæœ¬ - è§£å†³NaNé—®é¢˜  
"""  

import torch  
import json  
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling  
import os  
from torch.utils.data import Dataset  

class ChairDataset(Dataset):  
    def __init__(self, data, tokenizer, max_length=512):  
        self.data = data  
        self.tokenizer = tokenizer  
        self.max_length = max_length  
    
    def __len__(self):  
        return len(self.data)  
    
    def __getitem__(self, idx):  
        item = self.data[idx]  
        text = f"Human: {item['input']}\n\nAssistant: {item['output']}"  
        
        # Tokenize  
        encoding = self.tokenizer(  
            text,  
            truncation=True,  
            padding='max_length',  
            max_length=self.max_length,  
            return_tensors='pt'  
        )  
        
        return {  
            'input_ids': encoding['input_ids'].flatten(),  
            'attention_mask': encoding['attention_mask'].flatten(),  
            'labels': encoding['input_ids'].flatten()  
        }  

def stable_train():  
    print("ğŸª‘ ç¨³å®šè®­ç»ƒæ¤…å­è®¾è®¡æ¨¡å‹...")  
    
    # è®¾ç½®ç¯å¢ƒ  
    os.environ["CUDA_VISIBLE_DEVICES"] = "0"  
    os.environ["CUDA_LAUNCH_BLOCKING"] = "1"  # è°ƒè¯•CUDAé”™è¯¯  
    torch.cuda.empty_cache()  
    
    # åŠ è½½æ•°æ®  
    data_file = "./output/new_training_data/chair_training_data.json"  
    with open(data_file, 'r', encoding='utf-8') as f:  
        training_data = json.load(f)  
    
    # åªç”¨å°‘é‡æ•°æ®è¿›è¡Œç¨³å®šè®­ç»ƒ  
    training_data = training_data[:20]  
    print(f"ğŸ“Š ä½¿ç”¨ {len(training_data)} ä¸ªæ ·æœ¬è¿›è¡Œç¨³å®šè®­ç»ƒ")  
    
    # åŠ è½½æ¨¡å‹å’Œtokenizer  
    print("ğŸ”„ åŠ è½½æ¨¡å‹...")  
    tokenizer = AutoTokenizer.from_pretrained("../models/BlenderLLM", trust_remote_code=True)  
    model = AutoModelForCausalLM.from_pretrained(  
        "../models/BlenderLLM",  
        trust_remote_code=True,  
        torch_dtype=torch.float32,  # ä½¿ç”¨float32è€Œä¸æ˜¯float16  
        device_map="auto",  
        low_cpu_mem_usage=True  
    )  
    
    # è®¾ç½®tokenizer  
    if tokenizer.pad_token is None:  
        tokenizer.pad_token = tokenizer.eos_token  
        tokenizer.pad_token_id = tokenizer.eos_token_id  
    
    # åªè®­ç»ƒæœ€åçš„è¯­è¨€æ¨¡å‹å¤´  
    for param in model.parameters():  
        param.requires_grad = False  
    
    # åªè®­ç»ƒlm_head  
    if hasattr(model, 'lm_head'):  
        for param in model.lm_head.parameters():  
            param.requires_grad = True  
    
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)  
    total_params = sum(p.numel() for p in model.parameters())  
    print(f"ğŸ¯ å¯è®­ç»ƒå‚æ•°: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.2f}%)")  
    
    # åˆ›å»ºæ•°æ®é›†  
    dataset = ChairDataset(training_data, tokenizer, max_length=512)  
    
    # è®­ç»ƒå‚æ•° - æ›´ä¿å®ˆçš„è®¾ç½®  
    training_args = TrainingArguments(  
        output_dir="./output/stable_chair_model",  
        overwrite_output_dir=True,  
        num_train_epochs=1,  
        per_device_train_batch_size=1,  # å°batch size  
        gradient_accumulation_steps=4,  
        learning_rate=1e-6,  # æ›´å°çš„å­¦ä¹ ç‡  
        weight_decay=0.01,  
        logging_steps=5,  
        save_steps=50,  
        save_total_limit=2,  
        prediction_loss_only=True,  
        remove_unused_columns=False,  
        dataloader_drop_last=True,  
        fp16=False,  # ä¸ä½¿ç”¨æ··åˆç²¾åº¦  
        max_grad_norm=0.5,  # ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ª  
        warmup_steps=2,  
    )  
    
    # æ•°æ®æ”¶é›†å™¨  
    data_collator = DataCollatorForLanguageModeling(  
        tokenizer=tokenizer,  
        mlm=False,  
        return_tensors="pt"  
    )  
    
    # åˆ›å»ºTrainer  
    trainer = Trainer(  
        model=model,  
        args=training_args,  
        train_dataset=dataset,  
        data_collator=data_collator,  
        tokenizer=tokenizer,  
    )  
    
    print("ğŸ‹ï¸ å¼€å§‹ç¨³å®šè®­ç»ƒ...")  
    
    try:  
        # è®­ç»ƒ  
        trainer.train()  
        
        print("âœ… è®­ç»ƒå®Œæˆ!")  
        
        # ä¿å­˜æ¨¡å‹  
        trainer.save_model("./output/stable_chair_model")  
        tokenizer.save_pretrained("./output/stable_chair_model")  
        
        print("ğŸ’¾ æ¨¡å‹å·²ä¿å­˜")  
        
        # æµ‹è¯•æ¨¡å‹  
        print("\nğŸ§ª æµ‹è¯•è®­ç»ƒåçš„æ¨¡å‹...")  
        model.eval()  
        
        test_cases = [  
            "Generate chair design: simple wooden chair",  
            "Generate chair design: modern office chair"  
        ]  
        
        for i, test_input in enumerate(test_cases):  
            print(f"\n--- æµ‹è¯• {i+1} ---")  
            prompt = f"Human: {test_input}\n\nAssistant:"  
            
            try:  
                inputs = tokenizer(prompt, return_tensors="pt")  
                inputs = {k: v.to(model.device) for k, v in inputs.items()}  
                
                with torch.no_grad():  
                    outputs = model.generate(  
                        **inputs,  
                        max_length=len(inputs['input_ids'][0]) + 150,  
                        temperature=0.8,  
                        do_sample=True,  
                        top_p=0.9,  
                        pad_token_id=tokenizer.pad_token_id,  
                        eos_token_id=tokenizer.eos_token_id,  
                        repetition_penalty=1.1,  
                        no_repeat_ngram_size=3  
                    )  
                
                result = tokenizer.decode(outputs[0], skip_special_tokens=True)  
                generated = result[len(prompt):].strip()  
                
                print(f"ğŸ“ è¾“å…¥: {test_input}")  
                print(f"ğŸ¤– ç”Ÿæˆ: {generated[:200]}...")  
                
            except Exception as e:  
                print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")  
        
    except Exception as e:  
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")  
        import traceback  
        traceback.print_exc()  

if __name__ == "__main__":  
    stable_train()  
