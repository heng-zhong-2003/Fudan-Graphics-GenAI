#!/usr/bin/env python3  
"""  
æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹æ•ˆæœ  
"""  

import os  
import torch  
from transformers import AutoTokenizer, AutoModelForCausalLM  
import json  

def load_original_model():  
    """åŠ è½½åŸå§‹æ¨¡å‹"""  
    print("ğŸ”„ åŠ è½½åŸå§‹BlenderLLMæ¨¡å‹...")  
    try:  
        tokenizer = AutoTokenizer.from_pretrained("../models/BlenderLLM", trust_remote_code=True)  
        model = AutoModelForCausalLM.from_pretrained(  
            "../models/BlenderLLM",  
            trust_remote_code=True,  
            torch_dtype=torch.float16,  
            device_map={"": 0}  
        )  
        return model, tokenizer  
    except Exception as e:  
        print(f"âŒ åŸå§‹æ¨¡å‹åŠ è½½å¤±è´¥: {e}")  
        return None, None  

def load_finetuned_model():  
    """åŠ è½½å¾®è°ƒåçš„æ¨¡å‹"""  
    print("ğŸ”„ åŠ è½½å¾®è°ƒåçš„æ¨¡å‹...")  
    
    # å…ˆåŠ è½½åŸå§‹æ¨¡å‹  
    base_model, tokenizer = load_original_model()  
    if base_model is None:  
        return None, None  
    
    # åŠ è½½å¾®è°ƒçš„å‚æ•°  
    try:  
        finetuned_params_path = "./output/emergency_model/trainable_params.pt"  
        if os.path.exists(finetuned_params_path):  
            finetuned_params = torch.load(finetuned_params_path, map_location="cpu")  
            
            # æ›´æ–°æ¨¡å‹å‚æ•°  
            updated_count = 0  
            for name, param in base_model.named_parameters():  
                if name in finetuned_params:  
                    param.data = finetuned_params[name].to(param.device)  
                    updated_count += 1  
            
            print(f"âœ… æ›´æ–°äº† {updated_count} ä¸ªå‚æ•°")  
            return base_model, tokenizer  
        else:  
            print("âŒ å¾®è°ƒå‚æ•°æ–‡ä»¶ä¸å­˜åœ¨")  
            return None, None  
            
    except Exception as e:  
        print(f"âŒ å¾®è°ƒæ¨¡å‹åŠ è½½å¤±è´¥: {e}")  
        return None, None  

def test_generation(model, tokenizer, model_name, test_prompts):  
    """æµ‹è¯•ç”Ÿæˆæ•ˆæœ"""  
    print(f"\nğŸ§ª æµ‹è¯• {model_name}")  
    print("-" * 50)  
    
    results = []  
    
    for i, prompt in enumerate(test_prompts):  
        print(f"\nğŸ“ æµ‹è¯• {i+1}: {prompt}")  
        
        try:  
            # ä½¿ç”¨æç¤ºæ¨¡æ¿  
            full_prompt = f"Generate chair design: {prompt}"  
            
            inputs = tokenizer(full_prompt, return_tensors="pt")  
            device = next(model.parameters()).device  
            inputs = {k: v.to(device) for k, v in inputs.items()}  
            
            with torch.no_grad():  
                outputs = model.generate(  
                    **inputs,  
                    max_length=len(inputs['input_ids'][0]) + 150,  
                    temperature=0.7,  
                    do_sample=True,  
                    top_p=0.9,  
                    pad_token_id=tokenizer.eos_token_id,  
                    repetition_penalty=1.1  
                )  
            
            result = tokenizer.decode(outputs[0], skip_special_tokens=True)  
            generated = result[len(full_prompt):].strip()  
            
            print(f"ğŸ¤– ç”Ÿæˆç»“æœ:")  
            print(generated[:300] + "..." if len(generated) > 300 else generated)  
            
            # ç®€å•è¯„åˆ†  
            score = evaluate_output(generated, prompt)  
            print(f"ğŸ“Š è¯„åˆ†: {score}/10")  
            
            results.append({  
                'prompt': prompt,  
                'output': generated,  
                'score': score  
            })  
            
        except Exception as e:  
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")  
            results.append({  
                'prompt': prompt,  
                'output': "",  
                'score': 0  
            })  
    
    return results  

def evaluate_output(output, prompt):  
    """ç®€å•çš„è¾“å‡ºè¯„ä¼°"""  
    score = 0  
    output_lower = output.lower()  
    prompt_lower = prompt.lower()  
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«Blenderä»£ç   
    if 'import bpy' in output:  
        score += 3  
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¤…å­ç›¸å…³è¯æ±‡  
    chair_keywords = ['chair', 'seat', 'leg', 'back', 'arm']  
    if any(keyword in output_lower for keyword in chair_keywords):  
        score += 2  
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«Blenderæ“ä½œ  
    blender_ops = ['primitive', 'cube', 'cylinder', 'mesh', 'add', 'location', 'scale']  
    if any(op in output_lower for op in blender_ops):  
        score += 2  
    
    # æ£€æŸ¥æ˜¯å¦å“åº”äº†ç‰¹å®šæç¤º  
    if any(word in output_lower for word in prompt_lower.split()):  
        score += 2  
    
    # æ£€æŸ¥ä»£ç è´¨é‡  
    if 'bpy.ops' in output:  
        score += 1  
    
    return min(score, 10)  

def compare_models():  
    """å¯¹æ¯”æ¨¡å‹æ•ˆæœ"""  
    print("ğŸ”¬ BlenderLLM å¾®è°ƒæ•ˆæœå¯¹æ¯”æµ‹è¯•")  
    print("=" * 60)  
    
    # æµ‹è¯•æç¤º  
    test_prompts = [  
        "wooden dining chair",  
        "modern office chair with wheels",  
        "simple stool",  
        "comfortable armchair",  
        "bar stool with back support"  
    ]  
    
    # åŠ è½½åŸå§‹æ¨¡å‹  
    original_model, tokenizer = load_original_model()  
    if original_model is None:  
        print("âŒ æ— æ³•åŠ è½½åŸå§‹æ¨¡å‹")  
        return  
    
    # æµ‹è¯•åŸå§‹æ¨¡å‹  
    print("\n" + "="*60)  
    original_results = test_generation(original_model, tokenizer, "åŸå§‹BlenderLLM", test_prompts)  
    
    # æ¸…ç†å†…å­˜  
    del original_model  
    torch.cuda.empty_cache()  
    
    # åŠ è½½å¾®è°ƒæ¨¡å‹  
    finetuned_model, tokenizer = load_finetuned_model()  
    if finetuned_model is None:  
        print("âŒ æ— æ³•åŠ è½½å¾®è°ƒæ¨¡å‹")  
        return  
    
    # æµ‹è¯•å¾®è°ƒæ¨¡å‹  
    print("\n" + "="*60)  
    finetuned_results = test_generation(finetuned_model, tokenizer, "å¾®è°ƒåBlenderLLM", test_prompts)  
    
    # å¯¹æ¯”ç»“æœ  
    print("\n" + "="*60)  
    print("ğŸ“Š å¯¹æ¯”ç»“æœæ±‡æ€»")  
    print("="*60)  
    
    original_avg = sum(r['score'] for r in original_results) / len(original_results)  
    finetuned_avg = sum(r['score'] for r in finetuned_results) / len(finetuned_results)  
    
    print(f"ğŸ“ˆ åŸå§‹æ¨¡å‹å¹³å‡åˆ†: {original_avg:.2f}/10")  
    print(f"ğŸ“ˆ å¾®è°ƒæ¨¡å‹å¹³å‡åˆ†: {finetuned_avg:.2f}/10")  
    print(f"ğŸ“ˆ æ”¹è¿›å¹…åº¦: {finetuned_avg - original_avg:+.2f}")  
    
    if finetuned_avg > original_avg:  
        print("âœ… å¾®è°ƒæœ‰æ•ˆï¼æ¨¡å‹æ€§èƒ½æå‡")  
    elif finetuned_avg == original_avg:  
        print("â¡ï¸ å¾®è°ƒæ•ˆæœä¸­æ€§")  
    else:  
        print("âš ï¸ å¾®è°ƒå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™")  
    
    # ä¿å­˜è¯¦ç»†ç»“æœ  
    results = {  
        'original': original_results,  
        'finetuned': finetuned_results,  
        'summary': {  
            'original_avg': original_avg,  
            'finetuned_avg': finetuned_avg,  
            'improvement': finetuned_avg - original_avg  
        }  
    }  
    
    os.makedirs("./output/evaluation", exist_ok=True)  
    with open("./output/evaluation/comparison_results.json", 'w') as f:  
        json.dump(results, f, indent=2, ensure_ascii=False)  
    
    print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: ./output/evaluation/comparison_results.json")  

if __name__ == "__main__":  
    compare_models()  
