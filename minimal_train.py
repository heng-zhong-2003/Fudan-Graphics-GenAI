#!/usr/bin/env python3  
"""  
æœ€å°åŒ–è®­ç»ƒ - ä»…è®­ç»ƒéƒ¨åˆ†å±‚  
"""  

import torch  
import json  
from transformers import AutoTokenizer, AutoModelForCausalLM  
import os  

# å¼ºåˆ¶å•GPU  
os.environ["CUDA_VISIBLE_DEVICES"] = "0"  
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"  

def minimal_train():  
    print("ğŸª‘ Minimal Chair Training...")  
    
    # æ¸…ç†å†…å­˜  
    torch.cuda.empty_cache()  
    
    # åŠ è½½æ¨¡å‹  
    print("Loading model...")  
    tokenizer = AutoTokenizer.from_pretrained("../models/BlenderLLM", trust_remote_code=True)  
    
    # åªåŠ è½½æ¨¡å‹çš„æŸäº›å±‚è¿›è¡Œè®­ç»ƒ  
    model = AutoModelForCausalLM.from_pretrained(  
        "../models/BlenderLLM",  
        trust_remote_code=True,  
        torch_dtype=torch.float16,  
        device_map="auto",  
        low_cpu_mem_usage=True  
    )  
    
    # å†»ç»“å¤§éƒ¨åˆ†å‚æ•°ï¼Œåªè®­ç»ƒæœ€åå‡ å±‚  
    for name, param in model.named_parameters():  
        param.requires_grad = False  
    
    # åªè§£å†»æœ€å2å±‚  
    layers_to_train = []  
    for name, param in model.named_parameters():  
        if 'layers.31' in name or 'layers.30' in name or 'lm_head' in name:  
            param.requires_grad = True  
            layers_to_train.append(name)  
    
    print(f"Training layers: {len(layers_to_train)}")  
    
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹  
    model.gradient_checkpointing_enable()  
    
    if tokenizer.pad_token is None:  
        tokenizer.pad_token = tokenizer.eos_token  
    
    # åŠ è½½å°‘é‡æ•°æ®  
    with open('./output/cleaned_data/cleaned_data.json', 'r') as f:  
        data = json.load(f)  
    
    # åªç”¨å‰50ä¸ªæ ·æœ¬  
    data = data[:50]  
    print(f"Using {len(data)} samples")  
    
    # ç®€å•çš„è®­ç»ƒå¾ªç¯  
    optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=1e-5)  
    
    model.train()  
    
    for epoch in range(1):  
        total_loss = 0  
        for i, item in enumerate(data):  
            if i % 10 == 0:  
                print(f"Step {i}/{len(data)}")  
                torch.cuda.empty_cache()  
            
            text = f"{item.get('input', '')}\n{item.get('output', '')}"  
            if len(text) > 500:  
                text = text[:500]  
            
            inputs = tokenizer(text, return_tensors="pt", max_length=128, truncation=True, padding=True)  
            inputs = {k: v.to(model.device) for k, v in inputs.items()}  
            
            outputs = model(**inputs, labels=inputs['input_ids'])  
            loss = outputs.loss  
            
            loss.backward()  
            
            if (i + 1) % 4 == 0:  # æ¢¯åº¦ç´¯ç§¯  
                optimizer.step()  
                optimizer.zero_grad()  
            
            total_loss += loss.item()  
        
        print(f"Epoch loss: {total_loss/len(data):.4f}")  
    
    # ä¿å­˜æ¨¡å‹  
    print("Saving model...")  
    os.makedirs("./output/minimal_model", exist_ok=True)  
    model.save_pretrained("./output/minimal_model")  
    tokenizer.save_pretrained("./output/minimal_model")  
    
    print("âœ… Minimal training done!")  

if __name__ == "__main__":  
    minimal_train()  
