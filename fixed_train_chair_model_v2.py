#!/usr/bin/env python3  
"""  
ä¿®å¤åçš„æ¤…å­é£æ ¼ç”Ÿæˆæ¨¡å‹è®­ç»ƒè„šæœ¬ v2  
"""  

import argparse  
import json  
import os  
from pathlib import Path  
import torch  
from fixed_fine_tune_blender_llm import BlenderLLMFineTuner  

def load_config(config_path):  
    """åŠ è½½é…ç½®æ–‡ä»¶"""  
    if not config_path or not Path(config_path).exists():  
        return {}  
    
    with open(config_path, 'r') as f:  
        return json.load(f)  

def check_data_structure(data_path):  
    """æ£€æŸ¥æ•°æ®ç»“æ„ - æ”¯æŒæ–‡ä»¶å’Œç›®å½•"""  
    data_path = Path(data_path)  
    print(f"ğŸ” Checking data structure at: {data_path}")  
    
    if not data_path.exists():  
        print(f"âŒ Data path does not exist: {data_path}")  
        return False  
    
    total_samples = 0  
    
    if data_path.is_file():  
        # å¦‚æœæ˜¯æ–‡ä»¶ï¼Œç›´æ¥æ£€æŸ¥æ–‡ä»¶å†…å®¹  
        if data_path.suffix == '.json':  
            try:  
                with open(data_path, 'r', encoding='utf-8') as f:  
                    data = json.load(f)  
                    if isinstance(data, list):  
                        total_samples = len(data)  
                    else:  
                        total_samples = 1  
                print(f"ğŸ“„ JSON file contains {total_samples} samples")  
                return total_samples > 0  
            except Exception as e:  
                print(f"âŒ Error reading JSON file: {e}")  
                return False  
        else:  
            print(f"âŒ Unsupported file type: {data_path.suffix}")  
            return False  
    else:  
        # å¦‚æœæ˜¯ç›®å½•ï¼ŒæŸ¥æ‰¾æ–‡ä»¶  
        json_files = list(data_path.rglob('*.json'))  
        txt_files = list(data_path.rglob('*.txt'))  
        
        print(f"ğŸ“ Found {len(json_files)} JSON files and {len(txt_files)} TXT files")  
        
        if json_files:  
            print("ğŸ“„ Sample JSON files:")  
            for f in json_files[:3]:  
                print(f"   - {f}")  
                
        if txt_files:  
            print("ğŸ“„ Sample TXT files:")  
            for f in txt_files[:3]:  
                print(f"   - {f}")  
        
        return len(json_files) > 0 or len(txt_files) > 0  

def main():  
    parser = argparse.ArgumentParser(description='Train Chair Style Generation Model (Fixed v2)')  
    parser.add_argument('--data_path', type=str, required=True, help='Path to training data (file or directory)')  
    parser.add_argument('--base_model', type=str, required=True, help='Path to base model')  
    parser.add_argument('--config', type=str, help='Configuration file path')  
    parser.add_argument('--output_dir', type=str, default='./output', help='Output directory')  
    parser.add_argument('--epochs', type=int, default=3, help='Number of training epochs')  
    parser.add_argument('--batch_size', type=int, default=2, help='Batch size')  
    parser.add_argument('--learning_rate', type=float, default=2e-5, help='Learning rate')  
    parser.add_argument('--max_length', type=int, default=1024, help='Maximum sequence length')  
    
    args = parser.parse_args()  
    
    # åŠ è½½é…ç½®  
    config = load_config(args.config)  
    
    # åˆå¹¶é…ç½®å’Œå‘½ä»¤è¡Œå‚æ•°  
    data_path = args.data_path or config.get('data_path')  
    base_model = args.base_model or config.get('base_model')  
    output_dir = args.output_dir or config.get('output_dir', './output')  
    
    print("ğŸª‘ Starting Chair Style Generation Model Training (Fixed v2)...")  
    print(f"ğŸ“Š Data path: {data_path}")  
    print(f"ğŸ¤– Base model: {base_model}")  
    print(f"ğŸ“ Output directory: {output_dir}")  
    print(f"âš™ï¸  Training parameters:")  
    print(f"   - Epochs: {args.epochs}")  
    print(f"   - Batch size: {args.batch_size}")  
    print(f"   - Learning rate: {args.learning_rate}")  
    print(f"   - Max length: {args.max_length}")  
    
    # æ£€æŸ¥æ•°æ®ç»“æ„  
    if not check_data_structure(data_path):  
        print("âŒ No valid data found. Please check your data path.")  
        return  
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„  
    if not Path(base_model).exists():  
        print(f"âŒ Base model path does not exist: {base_model}")  
        return  
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§  
    if torch.cuda.is_available():  
        print(f"ğŸš€ CUDA available: {torch.cuda.get_device_name()}")  
        print(f"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")  
    else:  
        print("âš ï¸  CUDA not available, using CPU")  
    
    try:  
        # åˆ›å»ºfine-tuner  
        fine_tuner = BlenderLLMFineTuner(base_model)  
        
        # åŠ è½½æ¨¡å‹  
        print("ğŸ”„ Loading model and tokenizer...")  
        fine_tuner.load_model()  
        
        # åˆ›å»ºæ•°æ®é›†  
        print("ğŸ“š Creating dataset...")  
        dataset = fine_tuner.create_dataset(data_path, args.max_length)  
        
        if len(dataset) == 0:  
            print("âŒ Dataset is empty! Please check your data format.")  
            return  
        
        print(f"âœ… Dataset loaded with {len(dataset)} samples")  
        
        # æ˜¾ç¤ºæ ·æœ¬æ•°æ®  
        print("ğŸ“ Sample data:")  
        sample = dataset[0]  
        print(f"   Input: {fine_tuner.tokenizer.decode(sample['input_ids'][:50])}...")  
        print(f"   Target: {fine_tuner.tokenizer.decode(sample['labels'][:50])}...")  
        
        # å¼€å§‹è®­ç»ƒ  
        print("ğŸ‹ï¸ Starting training...")  
        model_path = fine_tuner.train(  
            dataset=dataset,  
            output_dir=output_dir,  
            epochs=args.epochs,  
            batch_size=args.batch_size,  
            learning_rate=args.learning_rate,  
            warmup_steps=config.get('training', {}).get('warmup_steps', 100),  
            logging_steps=config.get('training', {}).get('logging_steps', 10),  
            save_steps=config.get('training', {}).get('save_steps', 100),  
            fp16=config.get('training', {}).get('fp16', True),  
            gradient_accumulation_steps=config.get('training', {}).get('gradient_accumulation_steps', 1)  
        )  
        
        print(f"ğŸ‰ Training completed successfully!")  
        print(f"ğŸ’¾ Model saved to: {model_path}")  
        
        # ç®€å•æµ‹è¯•  
        print("ğŸ§ª Testing model with sample prompt...")  
        test_prompts = [  
            "Generate chair design: modern minimalist style",  
            "Generate chair design: vintage wooden chair",  
            "Generate chair design: ergonomic office chair"  
        ]  
        
        for prompt in test_prompts[:1]:  # åªæµ‹è¯•ç¬¬ä¸€ä¸ª  
            try:  
                result = fine_tuner.generate(prompt, max_length=256)  
                print(f"ğŸ“ Test prompt: {prompt}")  
                print(f"   Result: {result[:200]}...")  
                break  
            except Exception as e:  
                print(f"âš ï¸  Test generation failed: {e}")  
        
    except Exception as e:  
        print(f"âŒ Training failed with error: {e}")  
        import traceback  
        traceback.print_exc()  
        return  
    
    print("âœ… All done!")  

if __name__ == '__main__':  
    main()  
