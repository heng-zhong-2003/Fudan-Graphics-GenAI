#!/usr/bin/env python3  
"""  
åˆ†ç¦»å¼æ¨¡å‹è¯„ä¼° - é¿å…åŒæ—¶åŠ è½½ä¸¤ä¸ªæ¨¡å‹  
å…ˆè¿è¡ŒåŸå§‹æ¨¡å‹ï¼Œä¿å­˜ç»“æœï¼Œæ¸…ç†å†…å­˜ï¼Œå†è¿è¡ŒLoRAæ¨¡å‹  
"""  

import torch  
from transformers import AutoTokenizer, AutoModelForCausalLM  
from peft import PeftModel  
import json  
import os  
import gc  
import time  
from datetime import datetime  
from utils.blender_evaluator import BlenderImageEvaluator

class SeparateModelEvaluator:  
    """åˆ†ç¦»å¼æ¨¡å‹è¯„ä¼°å™¨"""  
    
    def __init__(self):  
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")  
        self.test_prompts = [  
            # åŸºç¡€é£æ ¼ç±»å‹  
            # "Design a chair: modern minimalist office chair with wheels",  
            # "Design a chair: comfortable recliner with armrests and cushions",   
            # "Design a chair: dining chair with tall backrest and no armrests",  
            # "Design a chair: ergonomic gaming chair with adjustable height",  
            # "Design a chair: vintage wooden chair with carved details",  
            
            # # å·¥ä¸šå’Œç°ä»£é£æ ¼  
            # "Design a chair: industrial metal chair with reinforced frame",  
            # "Design a chair: contemporary sleek chair with leather upholstery",  
            # "Design a chair: scandinavian wooden chair with minimalist design",  
            # "Design a chair: modern acrylic transparent chair with simple lines",  
            
            # # åŠŸèƒ½ç‰¹åŒ–æ¤…å­  
            # "Design a chair: high bar stool with footrest and swivel base",  
            # "Design a chair: folding portable chair with compact design",  
            # "Design a chair: office chair with wheels, armrests and lumbar support",  
            # "Design a chair: gaming racing chair with headrest and massage nodes",  
            # "Design a chair: lounge recliner with adjustable backrest and footrest",  
            
            # # æè´¨å’Œèˆ’é€‚æ€§  
            # "Design a chair: fabric cushioned chair with padded seat and backrest",  
            # "Design a chair: wide spacious chair with soft upholstery and arms",  
            # "Design a chair: small compact chair for space-saving dining room",  
            # "Design a chair: traditional chair with carved wooden details and high back",  
            
            # # ç‰¹æ®ŠåŠŸèƒ½  
            # "Design a chair: height adjustable desk chair with wheels and cup holder",  
            # "Design a chair: ergonomic chair with lumbar support, armrests and mesh backrest"  
            "Design a chair: modern minimalist office chair with wheels",  
            "Design a chair: ergonomic gaming chair with adjustable height",   
            "Design a chair: vintage wooden chair with carved details",  
            "Design a chair: industrial metal chair with reinforced frame",  
            "Design a chair: scandinavian wooden chair with minimalist design"  
        ]  
        
    def clear_gpu_memory(self):  
        """æ¸…ç†GPUå†…å­˜"""  
        if torch.cuda.is_available():  
            torch.cuda.empty_cache()  
            torch.cuda.synchronize()  
            gc.collect()  
            print(f"ğŸ§¹ GPUå†…å­˜å·²æ¸…ç†")  
            
    def load_tokenizer(self):  
        """åŠ è½½tokenizer"""  
        tokenizer = AutoTokenizer.from_pretrained("../models/BlenderLLM", trust_remote_code=True)  
        if tokenizer.pad_token is None:  
            tokenizer.pad_token = tokenizer.eos_token  
        return tokenizer  
    
    def generate_responses(self, model, tokenizer, prompts, model_name):  
        """ç”Ÿæˆæ¨¡å‹å“åº”"""  
        print(f"\nğŸ“ ä½¿ç”¨{model_name}ç”Ÿæˆå“åº”...")  
        responses = {}  
        
        for i, prompt in enumerate(prompts, 1):  
            print(f"  å¤„ç† {i}/{len(prompts)}: {prompt[:40]}...")  
            
            input_text = f"User: {prompt}\n\nAssistant:"  
            inputs = tokenizer(input_text, return_tensors="pt").to(self.device)  
            
            with torch.no_grad():  
                outputs = model.generate(  
                    **inputs,  
                    max_new_tokens=200,  
                    temperature=0.7,  
                    do_sample=True,  
                    pad_token_id=tokenizer.pad_token_id,  
                    eos_token_id=tokenizer.eos_token_id  
                )  
            
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)  
            assistant_response = response.split("Assistant:")[-1].strip()  
            responses[prompt] = assistant_response  
            
            print(f"    ç”Ÿæˆé•¿åº¦: {len(assistant_response)} å­—ç¬¦")  
        
        return responses  
    
    def evaluate_original_model(self):  
        """è¯„ä¼°åŸå§‹æ¨¡å‹"""  
        print("ğŸ”µ è¯„ä¼°åŸå§‹BlenderLLMæ¨¡å‹")  
        print("=" * 40)  
        
        # æ¸…ç†å†…å­˜  
        self.clear_gpu_memory()  
        
        # åŠ è½½tokenizer  
        tokenizer = self.load_tokenizer()  
        
        # åŠ è½½åŸå§‹æ¨¡å‹  
        print("ğŸ“¦ åŠ è½½åŸå§‹BlenderLLM...")  
        original_model = AutoModelForCausalLM.from_pretrained(  
            "../models/BlenderLLM",  
            trust_remote_code=True,  
            torch_dtype=torch.float16,  
            device_map={"": 0}  
        )  
        
        # ç”Ÿæˆå“åº”  
        original_responses = self.generate_responses(  
            original_model, tokenizer, self.test_prompts, "åŸå§‹BlenderLLM"  
        )  
        
        # ä¿å­˜ç»“æœ  
        os.makedirs('./output/evaluation_results', exist_ok=True)  
        with open('./output/evaluation_results/original_responses.json', 'w') as f:  
            json.dump({  
                'model': 'original_blenderllm',  
                'timestamp': datetime.now().isoformat(),  
                'responses': original_responses  
            }, f, indent=2, ensure_ascii=False)  
        
        # æ¸…ç†æ¨¡å‹  
        del original_model  
        del tokenizer  
        self.clear_gpu_memory()  
        
        print("âœ… åŸå§‹æ¨¡å‹è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜")  
        return original_responses  
    
    def evaluate_lora_model(self):  
        """è¯„ä¼°LoRAæ¨¡å‹"""  
        print("\nğŸŸ¢ è¯„ä¼°LoRAå¢å¼ºæ¨¡å‹")  
        print("=" * 40)  
        
        # æ¸…ç†å†…å­˜  
        self.clear_gpu_memory()  
        time.sleep(2)  # ç­‰å¾…å†…å­˜å®Œå…¨é‡Šæ”¾  
        
        # åŠ è½½tokenizer  
        tokenizer = self.load_tokenizer()  
        
        # åŠ è½½LoRAæ¨¡å‹  
        print("ğŸ”§ åŠ è½½LoRAå¢å¼ºæ¨¡å‹...")  
        base_model = AutoModelForCausalLM.from_pretrained(  
            "../models/BlenderLLM",  
            trust_remote_code=True,  
            torch_dtype=torch.float16,  
            device_map={"": 0}  
        )  
        lora_model = PeftModel.from_pretrained(base_model, "./output/lora_blender_enhanced")  
        
        # ç”Ÿæˆå“åº”  
        lora_responses = self.generate_responses(  
            lora_model, tokenizer, self.test_prompts, "LoRAå¢å¼ºæ¨¡å‹"  
        )  
        
        # ä¿å­˜ç»“æœ  
        with open('./output/evaluation_results/lora_responses.json', 'w') as f:  
            json.dump({  
                'model': 'lora_enhanced_blenderllm',  
                'timestamp': datetime.now().isoformat(),  
                'responses': lora_responses  
            }, f, indent=2, ensure_ascii=False)  
        
        # æ¸…ç†æ¨¡å‹  
        del lora_model  
        del base_model  
        del tokenizer  
        self.clear_gpu_memory()  
        
        print("âœ… LoRAæ¨¡å‹è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜")  
        return lora_responses  
    
    def simple_compare_responses(self, original_responses, lora_responses):  
        """ç®€å•å¯¹æ¯”å“åº”è´¨é‡"""  
        print("\nğŸ“Š å“åº”è´¨é‡å¯¹æ¯”")  
        print("=" * 50)  
        
        def simple_score(response):  
            """ç®€å•è¯„åˆ†ç³»ç»Ÿ"""  
            score = 0  
            if 'import bpy' in response: score += 2  
            if 'primitive' in response: score += 2  
            if 'chair' in response.lower(): score += 1  
            if any(word in response.lower() for word in ['seat', 'back', 'leg', 'arm']): score += 2  
            if '.scale' in response or '.location' in response: score += 2  
            if len(response.split('\n')) > 5: score += 1  
            return score  
        
        total_original = 0  
        total_lora = 0  
        
        for i, prompt in enumerate(self.test_prompts, 1):  
            orig_resp = original_responses.get(prompt, "")  
            lora_resp = lora_responses.get(prompt, "")  
            
            orig_score = simple_score(orig_resp)  
            lora_score = simple_score(lora_resp)  
            
            total_original += orig_score  
            total_lora += lora_score  
            
            print(f"\nğŸ“ æµ‹è¯• {i}: {prompt[:50]}...")  
            print(f"  ğŸ”µ åŸå§‹æ¨¡å‹: {orig_score}/10")  
            print(f"  ğŸŸ¢ LoRAæ¨¡å‹: {lora_score}/10")  
            print(f"  ğŸ“ˆ æ”¹è¿›: {lora_score - orig_score:+d}")  
            
            # æ˜¾ç¤ºéƒ¨åˆ†å“åº”  
            print(f"  åŸå§‹å“åº”: {orig_resp[:100]}...")  
            print(f"  LoRAå“åº”: {lora_resp[:100]}...")  
        
        print(f"\nğŸ¯ æ€»ä½“ç»“æœ:")  
        print(f"  åŸå§‹æ¨¡å‹æ€»åˆ†: {total_original}/{len(self.test_prompts)*10}")  
        print(f"  LoRAæ¨¡å‹æ€»åˆ†: {total_lora}/{len(self.test_prompts)*10}")  
        print(f"  æ•´ä½“æ”¹è¿›: {total_lora - total_original:+d} ({(total_lora-total_original)/total_original*100:+.1f}%)")  
        
        return {  
            'original_total': total_original,  
            'lora_total': total_lora,  
            'improvement': total_lora - total_original  
        }  
    

    def enhanced_compare_responses(self, original_responses, lora_responses):  
        """å¢å¼ºçš„å“åº”è´¨é‡å¯¹æ¯” - åŒ…å«å›¾åƒæ¸²æŸ“è¯„ä¼°"""  
        print("\nğŸ“Š å¢å¼ºç‰ˆå“åº”è´¨é‡å¯¹æ¯”")  
        print("=" * 60)  
        
        evaluator = BlenderImageEvaluator()  
        
        def enhanced_code_score(response):  
            """å¢å¼ºçš„ä»£ç è¯„åˆ†ç³»ç»Ÿ"""  
            score = 0  
            
            # åŸºç¡€è¯­æ³•æ£€æŸ¥ (0-20åˆ†)  
            if 'import bpy' in response: score += 5  
            if 'primitive' in response: score += 5  
            if any(op in response for op in ['add', 'create', 'mesh']): score += 5  
            if 'location' in response and 'scale' in response: score += 5  
            
            # æ¤…å­ç‰¹å¾æ£€æŸ¥ (0-30åˆ†)  
            chair_features = ['seat', 'backrest', 'leg', 'arm', 'chair']  
            feature_count = sum(1 for f in chair_features if f in response.lower())  
            score += min(feature_count * 6, 30)  
            
            # ä»£ç å¤æ‚åº¦ (0-20åˆ†)  
            lines = len(response.split('\n'))  
            if lines > 10: score += 5  
            if lines > 20: score += 5  
            if lines > 30: score += 5  
            if 'for' in response or 'while' in response: score += 5  
            
            # æè´¨å’Œç»†èŠ‚ (0-15åˆ†)  
            if 'material' in response: score += 5  
            if 'modifier' in response: score += 5  
            if any(mat in response.lower() for mat in ['wood', 'metal', 'fabric', 'leather']): score += 5  
            
            # é«˜çº§åŠŸèƒ½ (0-15åˆ†)  
            advanced_features = ['wheel', 'adjust', 'cushion', 'armrest', 'headrest']  
            advanced_count = sum(1 for f in advanced_features if f in response.lower())  
            score += min(advanced_count * 3, 15)  
            
            return min(score, 100)  
        
        # é€‰æ‹©å…³é”®æµ‹è¯•ç”¨ä¾‹è¿›è¡Œå›¾åƒè¯„ä¼°  
        key_prompts = [  
            "Design a chair: modern minimalist office chair with wheels",  
            "Design a chair: ergonomic gaming chair with adjustable height",   
            "Design a chair: vintage wooden chair with carved details",  
            "Design a chair: industrial metal chair with reinforced frame",  
            "Design a chair: scandinavian wooden chair with minimalist design"  
        ]  
        
        total_original_code = 0  
        total_lora_code = 0  
        total_original_image = 0  
        total_lora_image = 0  
        
        for i, prompt in enumerate(self.test_prompts, 1):  
            orig_resp = original_responses.get(prompt, "")  
            lora_resp = lora_responses.get(prompt, "")  
            
            # ä»£ç è´¨é‡è¯„åˆ†  
            orig_code_score = enhanced_code_score(orig_resp)  
            lora_code_score = enhanced_code_score(lora_resp)  
            
            total_original_code += orig_code_score  
            total_lora_code += lora_code_score  
            
            print(f"\nğŸ“ æµ‹è¯• {i}: {prompt[:50]}...")  
            print(f"  ğŸ”µ åŸå§‹æ¨¡å‹ä»£ç åˆ†: {orig_code_score}/100")  
            print(f"  ğŸŸ¢ LoRAæ¨¡å‹ä»£ç åˆ†: {lora_code_score}/100")  
            print(f"  ğŸ“ˆ ä»£ç æ”¹è¿›: {lora_code_score - orig_code_score:+d}")  
            
            # å¯¹å…³é”®ç”¨ä¾‹è¿›è¡Œå›¾åƒè¯„ä¼°  
            if prompt in key_prompts:  
                print(f"  ğŸ¨ æ­£åœ¨æ¸²æŸ“å›¾åƒè¯„ä¼°...")  
                
                # æ¸²æŸ“åŸå§‹æ¨¡å‹å›¾åƒ  
                orig_image = evaluator.run_blender_script_and_render(orig_resp, f"orig_{i}")  
                if orig_image:  
                    orig_image_score = evaluator.evaluate_image_with_openai(orig_image, prompt)  
                    total_original_image += orig_image_score.get('total_score', 0)  
                    print(f"  ğŸ”µ åŸå§‹å›¾åƒåˆ†: {orig_image_score.get('total_score', 0)}/100")  
                
                # æ¸²æŸ“LoRAæ¨¡å‹å›¾åƒ  
                lora_image = evaluator.run_blender_script_and_render(lora_resp, f"lora_{i}")  
                if lora_image:  
                    lora_image_score = evaluator.evaluate_image_with_openai(lora_image, prompt)  
                    total_lora_image += lora_image_score.get('total_score', 0)  
                    print(f"  ğŸŸ¢ LoRAå›¾åƒåˆ†: {lora_image_score.get('total_score', 0)}/100")  
                    print(f"  ğŸ“ˆ å›¾åƒæ”¹è¿›: {lora_image_score.get('total_score', 0) - orig_image_score.get('total_score', 0):+d}")  
        
        # ç»¼åˆè¯„ä¼°ç»“æœ  
        print(f"\nğŸ¯ ç»¼åˆè¯„ä¼°ç»“æœ:")  
        print(f"  ğŸ“ ä»£ç è´¨é‡:")  
        print(f"    åŸå§‹æ¨¡å‹: {total_original_code}/{len(self.test_prompts)*100}")  
        print(f"    LoRAæ¨¡å‹: {total_lora_code}/{len(self.test_prompts)*100}")  
        print(f"    æ”¹è¿›: {total_lora_code - total_original_code:+d} ({(total_lora_code-total_original_code)/max(total_original_code,1)*100:+.1f}%)")  
        
        if total_original_image > 0:  
            print(f"  ğŸ¨ å›¾åƒè´¨é‡:")  
            print(f"    åŸå§‹æ¨¡å‹: {total_original_image}/{len(key_prompts)*100}")  
            print(f"    LoRAæ¨¡å‹: {total_lora_image}/{len(key_prompts)*100}")  
            print(f"    æ”¹è¿›: {total_lora_image - total_original_image:+d} ({(total_lora_image-total_original_image)/max(total_original_image,1)*100:+.1f}%)")  
        
        return {  
            'code_scores': {'original': total_original_code, 'lora': total_lora_code},  
            'image_scores': {'original': total_original_image, 'lora': total_lora_image}  
        }

    def create_comparison_report(self, comparison_result):  
        """åˆ›å»ºå¯¹æ¯”æŠ¥å‘Š"""  
        report = f"""# BlenderLLM æ¨¡å‹å¯¹æ¯”æŠ¥å‘Š  

## ğŸ“Š è¯„ä¼°ç»“æœ  

**è¯„ä¼°æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**æµ‹è¯•æ ·æœ¬**: {len(self.test_prompts)} ä¸ªæ¤…å­è®¾è®¡æç¤º  

### ğŸ¯ æ€»ä½“æ€§èƒ½  

| æ¨¡å‹ | æ€»åˆ† | å¹³å‡åˆ† |  
|------|------|--------|  
| åŸå§‹BlenderLLM | {comparison_result['original_total']}/{len(self.test_prompts)*10} | {comparison_result['original_total']/len(self.test_prompts):.1f}/10 |  
| LoRAå¢å¼ºç‰ˆæœ¬ | {comparison_result['lora_total']}/{len(self.test_prompts)*10} | {comparison_result['lora_total']/len(self.test_prompts):.1f}/10 |  

### ğŸ“ˆ æ”¹è¿›æ•ˆæœ  

- **æ€»åˆ†æ”¹è¿›**: {comparison_result['improvement']:+d}  
- **ç™¾åˆ†æ¯”æ”¹è¿›**: {comparison_result['improvement']/comparison_result['original_total']*100:+.1f}%  

### ğŸ¯ ç»“è®º  

{'ğŸ‰ LoRAå¾®è°ƒæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼' if comparison_result['improvement'] > 2 else 'âœ… LoRAå¾®è°ƒæœ‰æ•ˆæ”¹å–„äº†æ¨¡å‹èƒ½åŠ›' if comparison_result['improvement'] > 0 else 'âš ï¸ éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–è®­ç»ƒç­–ç•¥'}  

## ğŸ“ è¯¦ç»†å“åº”  

è¯¦ç»†çš„æ¨¡å‹å“åº”è¯·æŸ¥çœ‹:  
- `original_responses.json` - åŸå§‹æ¨¡å‹å“åº”  
- `lora_responses.json` - LoRAæ¨¡å‹å“åº”  
"""  
        
        with open('./output/evaluation_results/comparison_report.md', 'w') as f:  
            f.write(report)  
        
        print("ğŸ“‹ å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜: ./output/evaluation_results/comparison_report.md")  


    def create_comprehensive_report(self, simple_result, enhanced_result):  
        """åˆ›å»ºç»¼åˆè¯„ä¼°æŠ¥å‘Š"""  
        print("\n" + "="*60)  
        print("ğŸ“Š BlenderLLMæ¤…å­è®¾è®¡å¾®è°ƒ - ç»¼åˆè¯„ä¼°æŠ¥å‘Š")  
        print("="*60)  
        
        # ç®€å•è¯„ä¼°ç»“æœ  
        print(f"\nğŸ“ åŸºç¡€ä»£ç è¯„ä¼°:")  
        print(f"  åŸå§‹æ¨¡å‹: {simple_result['original_total']}/200")  
        print(f"  LoRAæ¨¡å‹: {simple_result['lora_total']}/200")   
        print(f"  æ”¹è¿›å¹…åº¦: {simple_result['improvement']:+d} ({simple_result['improvement']/simple_result['original_total']*100:+.1f}%)")  
        
        # å¢å¼ºè¯„ä¼°ç»“æœ  
        if enhanced_result.get('code_scores'):  
            code_scores = enhanced_result['code_scores']  
            print(f"\nğŸ”§ å¢å¼ºä»£ç è´¨é‡è¯„ä¼°:")  
            print(f"  åŸå§‹æ¨¡å‹: {code_scores['original']}/{len(self.test_prompts)*100}")  
            print(f"  LoRAæ¨¡å‹: {code_scores['lora']}/{len(self.test_prompts)*100}")  
            improvement = code_scores['lora'] - code_scores['original']  
            print(f"  æ”¹è¿›å¹…åº¦: {improvement:+d} ({improvement/max(code_scores['original'],1)*100:+.1f}%)")  
        
        # å›¾åƒè´¨é‡è¯„ä¼°ç»“æœ  
        if enhanced_result.get('image_scores') and enhanced_result['image_scores']['original'] > 0:  
            image_scores = enhanced_result['image_scores']  
            print(f"\nğŸ¨ å›¾åƒè´¨é‡è¯„ä¼°:")  
            print(f"  åŸå§‹æ¨¡å‹: {image_scores['original']}/500")  # 5ä¸ªå…³é”®æ ·æœ¬ * 100åˆ†  
            print(f"  LoRAæ¨¡å‹: {image_scores['lora']}/500")  
            improvement = image_scores['lora'] - image_scores['original']  
            print(f"  æ”¹è¿›å¹…åº¦: {improvement:+d} ({improvement/max(image_scores['original'],1)*100:+.1f}%)")  
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Šåˆ°æ–‡ä»¶  
        report_path = "./output/evaluation_report.txt"  
        with open(report_path, 'w', encoding='utf-8') as f:  
            f.write("BlenderLLMæ¤…å­è®¾è®¡å¾®è°ƒ - è¯¦ç»†è¯„ä¼°æŠ¥å‘Š\n")  
            f.write("="*50 + "\n\n")  
            f.write(f"è¯„ä¼°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")  
            f.write(f"æµ‹è¯•æ ·æœ¬æ•°: {len(self.test_prompts)}\n\n")  
            
            # å†™å…¥è¯¦ç»†ç»“æœ...  
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_path}")  

def main():  
    """ä¸»å‡½æ•°"""  
    print("ğŸš€ åˆ†ç¦»å¼BlenderLLMæ¨¡å‹å¯¹æ¯”è¯„ä¼°")  
    print("=" * 50)  
    
    evaluator = SeparateModelEvaluator()  
    
    try:  
        # è¯„ä¼°åŸå§‹æ¨¡å‹  
        original_responses = evaluator.evaluate_original_model()  
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´ç¡®ä¿å†…å­˜å®Œå…¨é‡Šæ”¾  
        print("\nâ³ ç­‰å¾…å†…å­˜é‡Šæ”¾...")  
        time.sleep(3)  
        
        # è¯„ä¼°LoRAæ¨¡å‹  
        lora_responses = evaluator.evaluate_lora_model()  
        
        # å¯¹æ¯”ç»“æœ  
        comparison_result = evaluator.simple_compare_responses(original_responses, lora_responses)  

        # æ›´ä»”ç»†å¯¹æ¯”
        comparison_result_more = evaluator.enhanced_compare_responses(original_responses, lora_responses) 
        
        # ç”ŸæˆæŠ¥å‘Š  
        # evaluator.create_comparison_report(comparison_result)  
        evaluator.create_comprehensive_report(comparison_result, comparison_result_more)  
        
        print("\nâœ… è¯„ä¼°å®Œæˆï¼")  
        
    except Exception as e:  
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")  
        import traceback  
        traceback.print_exc()  

if __name__ == "__main__":  
    main()  
