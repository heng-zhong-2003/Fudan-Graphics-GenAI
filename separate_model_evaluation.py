#!/usr/bin/env python3  
"""  
åˆ†ç¦»å¼æ¨¡å‹è¯„ä¼° - é¿å…åŒæ—¶åŠ è½½ä¸¤ä¸ªæ¨¡å‹  
å…ˆè¿è¡ŒåŸå§‹æ¨¡å‹ï¼Œä¿å­˜ç»“æœï¼Œæ¸…ç†å†…å­˜ï¼Œå†è¿è¡ŒLoRAæ¨¡å‹  
"""  

import torch  
from transformers import AutoTokenizer, AutoModelForCausalLM  
from peft import PeftModel  
import json  
import os  
import gc  
import time  
from datetime import datetime  

class SeparateModelEvaluator:  
    """åˆ†ç¦»å¼æ¨¡å‹è¯„ä¼°å™¨"""  
    
    def __init__(self):  
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")  
        self.test_prompts = [  
            "Design a chair: modern minimalist office chair with wheels",  
            "Design a chair: comfortable recliner with armrests and cushions",   
            "Design a chair: dining chair with tall backrest and no armrests",  
            "Design a chair: ergonomic gaming chair with adjustable height",  
            "Design a chair: vintage wooden chair with carved details"  
        ]  
        
    def clear_gpu_memory(self):  
        """æ¸…ç†GPUå†…å­˜"""  
        if torch.cuda.is_available():  
            torch.cuda.empty_cache()  
            torch.cuda.synchronize()  
            gc.collect()  
            print(f"ğŸ§¹ GPUå†…å­˜å·²æ¸…ç†")  
            
    def load_tokenizer(self):  
        """åŠ è½½tokenizer"""  
        tokenizer = AutoTokenizer.from_pretrained("../models/BlenderLLM", trust_remote_code=True)  
        if tokenizer.pad_token is None:  
            tokenizer.pad_token = tokenizer.eos_token  
        return tokenizer  
    
    def generate_responses(self, model, tokenizer, prompts, model_name):  
        """ç”Ÿæˆæ¨¡å‹å“åº”"""  
        print(f"\nğŸ“ ä½¿ç”¨{model_name}ç”Ÿæˆå“åº”...")  
        responses = {}  
        
        for i, prompt in enumerate(prompts, 1):  
            print(f"  å¤„ç† {i}/{len(prompts)}: {prompt[:40]}...")  
            
            input_text = f"User: {prompt}\n\nAssistant:"  
            inputs = tokenizer(input_text, return_tensors="pt").to(self.device)  
            
            with torch.no_grad():  
                outputs = model.generate(  
                    **inputs,  
                    max_new_tokens=200,  
                    temperature=0.7,  
                    do_sample=True,  
                    pad_token_id=tokenizer.pad_token_id,  
                    eos_token_id=tokenizer.eos_token_id  
                )  
            
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)  
            assistant_response = response.split("Assistant:")[-1].strip()  
            responses[prompt] = assistant_response  
            
            print(f"    ç”Ÿæˆé•¿åº¦: {len(assistant_response)} å­—ç¬¦")  
        
        return responses  
    
    def evaluate_original_model(self):  
        """è¯„ä¼°åŸå§‹æ¨¡å‹"""  
        print("ğŸ”µ è¯„ä¼°åŸå§‹BlenderLLMæ¨¡å‹")  
        print("=" * 40)  
        
        # æ¸…ç†å†…å­˜  
        self.clear_gpu_memory()  
        
        # åŠ è½½tokenizer  
        tokenizer = self.load_tokenizer()  
        
        # åŠ è½½åŸå§‹æ¨¡å‹  
        print("ğŸ“¦ åŠ è½½åŸå§‹BlenderLLM...")  
        original_model = AutoModelForCausalLM.from_pretrained(  
            "../models/BlenderLLM",  
            trust_remote_code=True,  
            torch_dtype=torch.float16,  
            device_map={"": 0}  
        )  
        
        # ç”Ÿæˆå“åº”  
        original_responses = self.generate_responses(  
            original_model, tokenizer, self.test_prompts, "åŸå§‹BlenderLLM"  
        )  
        
        # ä¿å­˜ç»“æœ  
        os.makedirs('./output/evaluation_results', exist_ok=True)  
        with open('./output/evaluation_results/original_responses.json', 'w') as f:  
            json.dump({  
                'model': 'original_blenderllm',  
                'timestamp': datetime.now().isoformat(),  
                'responses': original_responses  
            }, f, indent=2, ensure_ascii=False)  
        
        # æ¸…ç†æ¨¡å‹  
        del original_model  
        del tokenizer  
        self.clear_gpu_memory()  
        
        print("âœ… åŸå§‹æ¨¡å‹è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜")  
        return original_responses  
    
    def evaluate_lora_model(self):  
        """è¯„ä¼°LoRAæ¨¡å‹"""  
        print("\nğŸŸ¢ è¯„ä¼°LoRAå¢å¼ºæ¨¡å‹")  
        print("=" * 40)  
        
        # æ¸…ç†å†…å­˜  
        self.clear_gpu_memory()  
        time.sleep(2)  # ç­‰å¾…å†…å­˜å®Œå…¨é‡Šæ”¾  
        
        # åŠ è½½tokenizer  
        tokenizer = self.load_tokenizer()  
        
        # åŠ è½½LoRAæ¨¡å‹  
        print("ğŸ”§ åŠ è½½LoRAå¢å¼ºæ¨¡å‹...")  
        base_model = AutoModelForCausalLM.from_pretrained(  
            "../models/BlenderLLM",  
            trust_remote_code=True,  
            torch_dtype=torch.float16,  
            device_map={"": 0}  
        )  
        lora_model = PeftModel.from_pretrained(base_model, "./output/lora_blender_enhanced")  
        
        # ç”Ÿæˆå“åº”  
        lora_responses = self.generate_responses(  
            lora_model, tokenizer, self.test_prompts, "LoRAå¢å¼ºæ¨¡å‹"  
        )  
        
        # ä¿å­˜ç»“æœ  
        with open('./output/evaluation_results/lora_responses.json', 'w') as f:  
            json.dump({  
                'model': 'lora_enhanced_blenderllm',  
                'timestamp': datetime.now().isoformat(),  
                'responses': lora_responses  
            }, f, indent=2, ensure_ascii=False)  
        
        # æ¸…ç†æ¨¡å‹  
        del lora_model  
        del base_model  
        del tokenizer  
        self.clear_gpu_memory()  
        
        print("âœ… LoRAæ¨¡å‹è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜")  
        return lora_responses  
    
    def simple_compare_responses(self, original_responses, lora_responses):  
        """ç®€å•å¯¹æ¯”å“åº”è´¨é‡"""  
        print("\nğŸ“Š å“åº”è´¨é‡å¯¹æ¯”")  
        print("=" * 50)  
        
        def simple_score(response):  
            """ç®€å•è¯„åˆ†ç³»ç»Ÿ"""  
            score = 0  
            if 'import bpy' in response: score += 2  
            if 'primitive' in response: score += 2  
            if 'chair' in response.lower(): score += 1  
            if any(word in response.lower() for word in ['seat', 'back', 'leg', 'arm']): score += 2  
            if '.scale' in response or '.location' in response: score += 2  
            if len(response.split('\n')) > 5: score += 1  
            return score  
        
        total_original = 0  
        total_lora = 0  
        
        for i, prompt in enumerate(self.test_prompts, 1):  
            orig_resp = original_responses.get(prompt, "")  
            lora_resp = lora_responses.get(prompt, "")  
            
            orig_score = simple_score(orig_resp)  
            lora_score = simple_score(lora_resp)  
            
            total_original += orig_score  
            total_lora += lora_score  
            
            print(f"\nğŸ“ æµ‹è¯• {i}: {prompt[:50]}...")  
            print(f"  ğŸ”µ åŸå§‹æ¨¡å‹: {orig_score}/10")  
            print(f"  ğŸŸ¢ LoRAæ¨¡å‹: {lora_score}/10")  
            print(f"  ğŸ“ˆ æ”¹è¿›: {lora_score - orig_score:+d}")  
            
            # æ˜¾ç¤ºéƒ¨åˆ†å“åº”  
            print(f"  åŸå§‹å“åº”: {orig_resp[:100]}...")  
            print(f"  LoRAå“åº”: {lora_resp[:100]}...")  
        
        print(f"\nğŸ¯ æ€»ä½“ç»“æœ:")  
        print(f"  åŸå§‹æ¨¡å‹æ€»åˆ†: {total_original}/{len(self.test_prompts)*10}")  
        print(f"  LoRAæ¨¡å‹æ€»åˆ†: {total_lora}/{len(self.test_prompts)*10}")  
        print(f"  æ•´ä½“æ”¹è¿›: {total_lora - total_original:+d} ({(total_lora-total_original)/total_original*100:+.1f}%)")  
        
        return {  
            'original_total': total_original,  
            'lora_total': total_lora,  
            'improvement': total_lora - total_original  
        }  
    
    def create_comparison_report(self, comparison_result):  
        """åˆ›å»ºå¯¹æ¯”æŠ¥å‘Š"""  
        report = f"""# BlenderLLM æ¨¡å‹å¯¹æ¯”æŠ¥å‘Š  

## ğŸ“Š è¯„ä¼°ç»“æœ  

**è¯„ä¼°æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**æµ‹è¯•æ ·æœ¬**: {len(self.test_prompts)} ä¸ªæ¤…å­è®¾è®¡æç¤º  

### ğŸ¯ æ€»ä½“æ€§èƒ½  

| æ¨¡å‹ | æ€»åˆ† | å¹³å‡åˆ† |  
|------|------|--------|  
| åŸå§‹BlenderLLM | {comparison_result['original_total']}/{len(self.test_prompts)*10} | {comparison_result['original_total']/len(self.test_prompts):.1f}/10 |  
| LoRAå¢å¼ºç‰ˆæœ¬ | {comparison_result['lora_total']}/{len(self.test_prompts)*10} | {comparison_result['lora_total']/len(self.test_prompts):.1f}/10 |  

### ğŸ“ˆ æ”¹è¿›æ•ˆæœ  

- **æ€»åˆ†æ”¹è¿›**: {comparison_result['improvement']:+d}  
- **ç™¾åˆ†æ¯”æ”¹è¿›**: {comparison_result['improvement']/comparison_result['original_total']*100:+.1f}%  

### ğŸ¯ ç»“è®º  

{'ğŸ‰ LoRAå¾®è°ƒæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼' if comparison_result['improvement'] > 2 else 'âœ… LoRAå¾®è°ƒæœ‰æ•ˆæ”¹å–„äº†æ¨¡å‹èƒ½åŠ›' if comparison_result['improvement'] > 0 else 'âš ï¸ éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–è®­ç»ƒç­–ç•¥'}  

## ğŸ“ è¯¦ç»†å“åº”  

è¯¦ç»†çš„æ¨¡å‹å“åº”è¯·æŸ¥çœ‹:  
- `original_responses.json` - åŸå§‹æ¨¡å‹å“åº”  
- `lora_responses.json` - LoRAæ¨¡å‹å“åº”  
"""  
        
        with open('./output/evaluation_results/comparison_report.md', 'w') as f:  
            f.write(report)  
        
        print("ğŸ“‹ å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜: ./output/evaluation_results/comparison_report.md")  

def main():  
    """ä¸»å‡½æ•°"""  
    print("ğŸš€ åˆ†ç¦»å¼BlenderLLMæ¨¡å‹å¯¹æ¯”è¯„ä¼°")  
    print("=" * 50)  
    
    evaluator = SeparateModelEvaluator()  
    
    try:  
        # è¯„ä¼°åŸå§‹æ¨¡å‹  
        original_responses = evaluator.evaluate_original_model()  
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´ç¡®ä¿å†…å­˜å®Œå…¨é‡Šæ”¾  
        print("\nâ³ ç­‰å¾…å†…å­˜é‡Šæ”¾...")  
        time.sleep(3)  
        
        # è¯„ä¼°LoRAæ¨¡å‹  
        lora_responses = evaluator.evaluate_lora_model()  
        
        # å¯¹æ¯”ç»“æœ  
        comparison_result = evaluator.simple_compare_responses(original_responses, lora_responses)  
        
        # ç”ŸæˆæŠ¥å‘Š  
        evaluator.create_comparison_report(comparison_result)  
        
        print("\nâœ… è¯„ä¼°å®Œæˆï¼")  
        
    except Exception as e:  
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")  
        import traceback  
        traceback.print_exc()  

if __name__ == "__main__":  
    main()  
