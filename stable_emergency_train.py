#!/usr/bin/env python3  
"""  
ç¨³å®šç‰ˆç´§æ€¥è®­ç»ƒ - ä¿®å¤NaN/Infé—®é¢˜  
"""  

import os  
import torch  
import torch.nn as nn  
from transformers import AutoTokenizer, AutoModelForCausalLM  
import json  
import gc  
import numpy as np  

def safe_load_model():  
    """å®‰å…¨åŠ è½½æ¨¡å‹"""  
    print("ğŸ”„ å®‰å…¨åŠ è½½æ¨¡å‹...")  
    
    try:  
        tokenizer = AutoTokenizer.from_pretrained("../models/BlenderLLM", trust_remote_code=True)  
        if tokenizer.pad_token is None:  
            tokenizer.pad_token = tokenizer.eos_token  
        
        model = AutoModelForCausalLM.from_pretrained(  
            "../models/BlenderLLM",  
            trust_remote_code=True,  
            torch_dtype=torch.float16,  
            device_map={"": 0},  
            low_cpu_mem_usage=True  
        )  
        
        return model, tokenizer  
        
    except Exception as e:  
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")  
        return None, None  

def check_gradients(model):  
    """æ£€æŸ¥æ¢¯åº¦æ˜¯å¦æ­£å¸¸"""  
    total_norm = 0  
    param_count = 0  
    
    for name, param in model.named_parameters():  
        if param.grad is not None:  
            param_norm = param.grad.data.norm(2)  
            total_norm += param_norm.item() ** 2  
            param_count += 1  
            
            # æ£€æŸ¥å¼‚å¸¸å€¼  
            if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():  
                print(f"âš ï¸ å¼‚å¸¸æ¢¯åº¦åœ¨ {name}")  
                param.grad.zero_()  # æ¸…é›¶å¼‚å¸¸æ¢¯åº¦  
    
    total_norm = total_norm ** (1. / 2)  
    print(f"ğŸ“Š æ¢¯åº¦èŒƒæ•°: {total_norm:.4f}, å‚æ•°æ•°: {param_count}")  
    return total_norm  

def stable_train():  
    """ç¨³å®šè®­ç»ƒå‡½æ•°"""  
    print("ğŸ‹ï¸ ç¨³å®šç‰ˆæ¤…å­è®¾è®¡å¾®è°ƒ")  
    print("=" * 50)  
    
    # 1. åŠ è½½æ¨¡å‹  
    model, tokenizer = safe_load_model()  
    if model is None:  
        return create_dummy_model()  
    
    device = torch.device("cuda:0")  
    
    # 2. è®¾ç½®è®­ç»ƒå‚æ•° - æ›´ä¿å®ˆçš„è®¾ç½®  
    for name, param in model.named_parameters():  
        param.requires_grad = False  
        # åªè®­ç»ƒè¾“å‡ºå±‚çš„ä¸€å°éƒ¨åˆ†  
        if 'lm_head' in name and 'weight' in name:  
            param.requires_grad = True  
            print(f"ï¿½ï¿½ è®­ç»ƒå‚æ•°: {name}")  
    
    # 3. å‡†å¤‡è®­ç»ƒæ•°æ®  
    chair_data = [  
        ("wooden dining chair", "import bpy\nbpy.ops.mesh.primitive_cube_add(location=(0, 0, 0.5))"),  
        ("office chair", "import bpy\nbpy.ops.mesh.primitive_cylinder_add(location=(0, 0, 0.3))"),  
        ("simple stool", "import bpy\nbpy.ops.mesh.primitive_cube_add(scale=(0.5, 0.5, 0.3))"),  
    ]  
    
    # 4. ä¼˜åŒ–å™¨ - éå¸¸å°çš„å­¦ä¹ ç‡  
    optimizer = torch.optim.AdamW(  
        [p for p in model.parameters() if p.requires_grad],  
        lr=1e-7,  # æå°çš„å­¦ä¹ ç‡  
        weight_decay=0.01,  
        eps=1e-8  
    )  
    
    # 5. è®­ç»ƒå¾ªç¯  
    model.train()  
    total_loss = 0  
    valid_steps = 0  
    
    for epoch in range(2):  # åªè®­ç»ƒ2ä¸ªepoch  
        print(f"\nğŸ“š Epoch {epoch + 1}/2")  
        
        for i, (prompt, target) in enumerate(chair_data):  
            try:  
                # æ¸…ç†GPUå†…å­˜  
                torch.cuda.empty_cache()  
                
                # å‡†å¤‡è¾“å…¥  
                text = f"Generate chair design: {prompt}\n{target}"  
                inputs = tokenizer(  
                    text,   
                    return_tensors="pt",   
                    max_length=128,  # æ›´çŸ­çš„åºåˆ—  
                    truncation=True,  
                    padding=True  
                ).to(device)  
                
                # å‰å‘ä¼ æ’­  
                optimizer.zero_grad()  
                
                with torch.autocast(device_type='cuda', dtype=torch.float16):  
                    outputs = model(**inputs, labels=inputs['input_ids'])  
                    loss = outputs.loss  
                
                # æ£€æŸ¥æŸå¤±æ˜¯å¦æœ‰æ•ˆ  
                if torch.isnan(loss) or torch.isinf(loss):  
                    print(f"  âš ï¸ è·³è¿‡æ— æ•ˆæŸå¤±: {loss.item()}")  
                    continue  
                
                # åå‘ä¼ æ’­ï¼Œä½¿ç”¨æ¢¯åº¦ç¼©æ”¾  
                scaler = torch.cuda.amp.GradScaler()  
                scaler.scale(loss).backward()  
                
                # æ£€æŸ¥æ¢¯åº¦  
                grad_norm = check_gradients(model)  
                
                # æ¢¯åº¦è£å‰ª  
                if grad_norm > 1.0:  
                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  
                
                # ä¼˜åŒ–å™¨æ­¥éª¤  
                scaler.step(optimizer)  
                scaler.update()  
                
                total_loss += loss.item()  
                valid_steps += 1  
                
                print(f"  æ­¥éª¤ {i+1}: æŸå¤±={loss.item():.4f}, æ¢¯åº¦èŒƒæ•°={grad_norm:.4f}")  
                
            except Exception as e:  
                print(f"  âŒ è®­ç»ƒæ­¥éª¤å¤±è´¥: {e}")  
                continue  
    
    # 6. ä¿å­˜æ¨¡å‹  
    if valid_steps > 0:  
        avg_loss = total_loss / valid_steps  
        print(f"\nğŸ“Š å¹³å‡æŸå¤±: {avg_loss:.4f}")  
        
        # ä¿å­˜è®­ç»ƒå¥½çš„å‚æ•°  
        save_model(model, avg_loss)  
    else:  
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ­¥éª¤")  
        create_dummy_model()  

def save_model(model, avg_loss):  
    """ä¿å­˜æ¨¡å‹"""  
    print("ğŸ’¾ ä¿å­˜ç¨³å®šè®­ç»ƒæ¨¡å‹...")  
    
    output_dir = "./output/stable_model"  
    os.makedirs(output_dir, exist_ok=True)  
    
    # åªä¿å­˜è®­ç»ƒè¿‡çš„å‚æ•°  
    trainable_params = {}  
    for name, param in model.named_parameters():  
        if param.requires_grad:  
            # æ£€æŸ¥å‚æ•°æ˜¯å¦æ­£å¸¸  
            if not (torch.isnan(param).any() or torch.isinf(param).any()):  
                trainable_params[name] = param.cpu().clone()  
                print(f"âœ… ä¿å­˜å‚æ•°: {name}")  
            else:  
                print(f"âš ï¸ è·³è¿‡å¼‚å¸¸å‚æ•°: {name}")  
    
    if trainable_params:  
        torch.save(trainable_params, os.path.join(output_dir, "trainable_params.pt"))  
        
        # ä¿å­˜è®­ç»ƒä¿¡æ¯  
        info = {  
            "status": "stable_training_completed",  
            "avg_loss": avg_loss,  
            "saved_params": list(trainable_params.keys()),  
            "timestamp": str(torch.cuda.memory_allocated(0))  
        }  
        
        with open(os.path.join(output_dir, "training_info.json"), 'w') as f:  
            json.dump(info, f, indent=2)  
        
        print(f"âœ… æ¨¡å‹ä¿å­˜åˆ°: {output_dir}")  
    else:  
        print("âŒ æ²¡æœ‰æœ‰æ•ˆå‚æ•°å¯ä¿å­˜")  

def create_dummy_model():  
    """åˆ›å»ºè™šæ‹Ÿæ¨¡å‹ï¼ˆå½“è®­ç»ƒå¤±è´¥æ—¶ï¼‰"""  
    print("ğŸ”§ åˆ›å»ºè™šæ‹Ÿæ¨¡å‹...")  
    
    output_dir = "./output/dummy_model"  
    os.makedirs(output_dir, exist_ok=True)  
    
    info = {  
        "status": "dummy_model",  
        "message": "è®­ç»ƒå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ¨¡å‹",  
        "recommendation": "ç›´æ¥ä½¿ç”¨åŸå§‹BlenderLLMæ¨¡å‹"  
    }  
    
    with open(os.path.join(output_dir, "info.json"), 'w') as f:  
        json.dump(info, f, indent=2)  
    
    print("âœ… è™šæ‹Ÿæ¨¡å‹åˆ›å»ºå®Œæˆ")  

if __name__ == "__main__":  
    # æ¸…ç†GPUå†…å­˜  
    torch.cuda.empty_cache()  
    gc.collect()  
    
    stable_train()  
    
    print("\nğŸ¯ å»ºè®®:")  
    print("1. å¦‚æœç¨³å®šè®­ç»ƒæˆåŠŸï¼Œä½¿ç”¨ ./output/stable_model")  
    print("2. å¦‚æœè®­ç»ƒå¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹BlenderLLMæ¨¡å‹")  
    print("3. åŸå§‹æ¨¡å‹å·²ç»è¡¨ç°å¾ˆå¥½ (å¹³å‡åˆ†8.4/10)")  
