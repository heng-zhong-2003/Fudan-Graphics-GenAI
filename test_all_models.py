#!/usr/bin/env python3  
"""  
æµ‹è¯•æ‰€æœ‰è®­ç»ƒçš„æ¨¡å‹  
"""  

import torch  
from transformers import AutoTokenizer, AutoModelForCausalLM  
import os  
from pathlib import Path  

def test_model(model_path, model_name):  
    """æµ‹è¯•å•ä¸ªæ¨¡å‹"""  
    print(f"\nğŸ§ª æµ‹è¯•æ¨¡å‹: {model_name}")  
    print(f"ğŸ“ è·¯å¾„: {model_path}")  
    
    try:  
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨  
        if not Path(model_path).exists():  
            print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨")  
            return  
        
        # æ£€æŸ¥å¿…è¦æ–‡ä»¶  
        config_file = Path(model_path) / "config.json"  
        model_file = Path(model_path) / "pytorch_model.bin"  
        safetensors_file = Path(model_path) / "model.safetensors"  
        
        if not config_file.exists():  
            print(f"âŒ ç¼ºå°‘ config.json")  
            return  
        
        if not model_file.exists() and not any(Path(model_path).glob("*.safetensors")):  
            print(f"âŒ ç¼ºå°‘æ¨¡å‹æƒé‡æ–‡ä»¶")  
            return  
        
        # åŠ è½½æ¨¡å‹  
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)  
        model = AutoModelForCausalLM.from_pretrained(  
            model_path,   
            trust_remote_code=True,  
            torch_dtype=torch.float16,  
            device_map="auto"  
        )  
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")  
        
        # æµ‹è¯•ç”Ÿæˆ  
        test_prompts = [  
            "User: Generate chair design: modern\nAssistant:",  
            "User: Create a simple wooden chair\nAssistant:",  
            "User: Design a comfortable office chair\nAssistant:"  
        ]  
        
        for i, prompt in enumerate(test_prompts):  
            try:  
                inputs = tokenizer(prompt, return_tensors="pt")  
                
                with torch.no_grad():  
                    outputs = model.generate(  
                        **inputs,  
                        max_length=len(inputs['input_ids'][0]) + 150,  
                        temperature=0.7,  
                        do_sample=True,  
                        pad_token_id=tokenizer.eos_token_id,  
                        repetition_penalty=1.1,  
                        top_p=0.9  
                    )  
                
                result = tokenizer.decode(outputs[0], skip_special_tokens=True)  
                generated = result[len(prompt):].strip()  
                
                print(f"ğŸ“ æµ‹è¯• {i+1}:")  
                print(f"   è¾“å…¥: {prompt.split('Assistant:')[0].replace('User: ', '').strip()}")  
                print(f"   ç”Ÿæˆ: {generated[:200]}...")  
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¤…å­ç›¸å…³å†…å®¹  
                chair_keywords = ['chair', 'seat', 'backrest', 'leg', 'armrest', 'cushion', 'wood', 'metal']  
                if any(keyword.lower() in generated.lower() for keyword in chair_keywords):  
                    print(f"   âœ… åŒ…å«æ¤…å­ç›¸å…³å†…å®¹")  
                else:  
                    print(f"   âš ï¸  å¯èƒ½ä¸æ˜¯æ¤…å­ç›¸å…³å†…å®¹")  
                
            except Exception as e:  
                print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")  
        
        # æ¸…ç†å†…å­˜  
        del model, tokenizer  
        torch.cuda.empty_cache()  
        
    except Exception as e:  
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")  

def main():  
    print("ğŸ§ª æµ‹è¯•æ‰€æœ‰è®­ç»ƒçš„æ¨¡å‹")  
    
    # æ¨¡å‹åˆ—è¡¨  
    models = [  
        ("output/fixed_minimal_model", "Fixed Minimal Model"),  
        ("output/minimal_model", "Minimal Model"),  
        ("output/ultra_light_model", "Ultra Light Model"),  
        ("output/simple_model", "Simple Model"),  
        ("output/fixed_model", "Fixed Model"),  
        ("output/test_model", "Test Model"),  
    ]  
    
    # åŸºç¡€æ¨¡å‹ä½œä¸ºå¯¹æ¯”  
    base_model = ("../models/BlenderLLM", "Original BlenderLLM")  
    
    print(f"\nğŸ” é¦–å…ˆæµ‹è¯•åŸå§‹æ¨¡å‹ä½œä¸ºåŸºå‡†:")  
    test_model(base_model[0], base_model[1])  
    
    print(f"\n" + "="*60)  
    print(f"æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹:")  
    
    for model_path, model_name in models:  
        test_model(model_path, model_name)  
        print("-" * 40)  

if __name__ == "__main__":  
    main()  
