"""  
å¯¹æ¯”åŸå§‹æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹çš„æ€§èƒ½  
"""  

import torch  
from transformers import AutoTokenizer, AutoModelForCausalLM  
import json  
import os  
from datetime import datetime  

class ModelComparator:  
    def __init__(self):  
        self.test_prompts = [  
            "Generate chair design: modern minimalist chair",  
            "Generate chair design: vintage wooden armchair",  
            "Generate chair design: ergonomic office chair with lumbar support",  
            "Generate chair design: outdoor metal bar stool",  
            "Generate chair design: luxury leather executive chair"  
        ]  
    
    def load_model(self, model_path, model_name):  
        """åŠ è½½æ¨¡å‹"""  
        print(f"ğŸ”„ åŠ è½½ {model_name}...")  
        try:  
            tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)  
            model = AutoModelForCausalLM.from_pretrained(  
                model_path,  
                trust_remote_code=True,  
                torch_dtype=torch.float16,  
                device_map="auto"  
            )  
            print(f"âœ… {model_name} åŠ è½½æˆåŠŸ")  
            return model, tokenizer  
        except Exception as e:  
            print(f"âŒ {model_name} åŠ è½½å¤±è´¥: {e}")  
            return None, None  
    
    def generate_and_evaluate(self, model, tokenizer, prompt):  
        """ç”Ÿæˆå¹¶è¯„ä¼°å•ä¸ªæç¤º"""  
        try:  
            inputs = tokenizer(prompt, return_tensors="pt")  
            inputs = {k: v.to(model.device) for k, v in inputs.items()}  
            
            with torch.no_grad():  
                outputs = model.generate(  
                    **inputs,  
                    max_length=len(inputs['input_ids'][0]) + 400,  
                    temperature=0.7,  
                    do_sample=True,  
                    top_p=0.9,  
                    pad_token_id=tokenizer.eos_token_id,  
                    repetition_penalty=1.1  
                )  
            
            result = tokenizer.decode(outputs[0], skip_special_tokens=True)  
            generated = result[len(prompt):].strip()  
            
            # è¯„ä¼°ç”Ÿæˆè´¨é‡  
            score = self.evaluate_quality(generated)  
            
            return {  
                'prompt': prompt,  
                'generated': generated,  
                'score': score,  
                'length': len(generated),  
                'success': True  
            }  
            
        except Exception as e:  
            return {  
                'prompt': prompt,  
                'generated': '',  
                'score': 0,  
                'length': 0,  
                'success': False,  
                'error': str(e)  
            }  
    
    def evaluate_quality(self, generated_code):  
        """è¯„ä¼°ç”Ÿæˆä»£ç çš„è´¨é‡"""  
        score = 0  
        max_score = 10  
        
        # åŸºç¡€æ£€æŸ¥  
        if 'import bpy' in generated_code:  
            score += 2  
        
        # æ¤…å­ç›¸å…³å…ƒç´   
        chair_elements = ['seat', 'leg', 'back', 'chair', 'armrest']  
        if any(elem in generated_code.lower() for elem in chair_elements):  
            score += 2  
        
        # Blenderæ“ä½œ  
        blender_ops = ['add_object', 'primitive', 'mesh.', 'location', 'scale']  
        if any(op in generated_code for op in blender_ops):  
            score += 2  
        
        # ä»£ç ç»“æ„  
        if generated_code.count('\n') >= 5:  # å¤šè¡Œä»£ç   
            score += 1  
        
        if '#' in generated_code:  # æœ‰æ³¨é‡Š  
            score += 1  
        
        # è¯­æ³•æ£€æŸ¥ï¼ˆç®€å•ï¼‰  
        if generated_code.count('(') == generated_code.count(')'):  
            score += 1  
        
        # é•¿åº¦åˆç†æ€§  
        if 100 < len(generated_code) < 2000:  
            score += 1  
        
        return min(score, max_score)  
    
    def compare_models(self):  
        """å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹"""  
        print("ğŸ” æ¤…å­è®¾è®¡æ¨¡å‹å¯¹æ¯”è¯„ä¼°")  
        print("=" * 60)  
        
        # åŠ è½½åŸå§‹æ¨¡å‹  
        original_model, original_tokenizer = self.load_model(  
            "../models/BlenderLLM", "åŸå§‹æ¨¡å‹"  
        )  
        
        # åŠ è½½å¾®è°ƒæ¨¡å‹  
        finetuned_model, finetuned_tokenizer = self.load_model(  
            "./output/memory_optimized_model", "å¾®è°ƒæ¨¡å‹"  
        )  
        
        if not original_model or not finetuned_model:  
            print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œå¯¹æ¯”")  
            return  
        
        results = {  
            'original': [],  
            'finetuned': [],  
            'comparison_time': datetime.now().isoformat()  
        }  
        
        print("\nğŸ§ª å¼€å§‹å¯¹æ¯”æµ‹è¯•...")  
        
        for i, prompt in enumerate(self.test_prompts):  
            print(f"\n--- æµ‹è¯• {i+1}/{len(self.test_prompts)} ---")  
            print(f"ğŸ“ æç¤º: {prompt}")  
            
            # æµ‹è¯•åŸå§‹æ¨¡å‹  
            print("ğŸ”„ æµ‹è¯•åŸå§‹æ¨¡å‹...")  
            original_result = self.generate_and_evaluate(  
                original_model, original_tokenizer, prompt  
            )  
            results['original'].append(original_result)  
            
            if original_result['success']:  
                print(f"âœ… åŸå§‹æ¨¡å‹ - åˆ†æ•°: {original_result['score']}/10, é•¿åº¦: {original_result['length']}")  
            else:  
                print(f"âŒ åŸå§‹æ¨¡å‹ç”Ÿæˆå¤±è´¥")  
            
            # æµ‹è¯•å¾®è°ƒæ¨¡å‹  
            print("ğŸ”„ æµ‹è¯•å¾®è°ƒæ¨¡å‹...")  
            finetuned_result = self.generate_and_evaluate(  
                finetuned_model, finetuned_tokenizer, prompt  
            )  
            results['finetuned'].append(finetuned_result)  
            
            if finetuned_result['success']:  
                print(f"âœ… å¾®è°ƒæ¨¡å‹ - åˆ†æ•°: {finetuned_result['score']}/10, é•¿åº¦: {finetuned_result['length']}")  
            else:  
                print(f"âŒ å¾®è°ƒæ¨¡å‹ç”Ÿæˆå¤±è´¥")  
            
            # å¯¹æ¯”  
            if original_result['success'] and finetuned_result['success']:  
                if finetuned_result['score'] > original_result['score']:  
                    print("ğŸ¯ å¾®è°ƒæ¨¡å‹è¡¨ç°æ›´å¥½!")  
                elif finetuned_result['score'] < original_result['score']:  
                    print("ğŸ“‰ åŸå§‹æ¨¡å‹è¡¨ç°æ›´å¥½")  
                else:  
                    print("ğŸ¤ ä¸¤æ¨¡å‹è¡¨ç°ç›¸å½“")  
        
        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š  
        self.generate_comparison_report(results)  
        
        return results  
    
    def generate_comparison_report(self, results):  
        """ç”Ÿæˆè¯¦ç»†çš„å¯¹æ¯”æŠ¥å‘Š"""  
        print("\n" + "=" * 60)  
        print("ğŸ“Š å¯¹æ¯”æŠ¥å‘Š")  
        print("=" * 60)  
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡  
        original_scores = [r['score'] for r in results['original'] if r['success']]  
        finetuned_scores = [r['score'] for r in results['finetuned'] if r['success']]  
        
        original_success_rate = len(original_scores) / len(results['original']) * 100  
        finetuned_success_rate = len(finetuned_scores) / len(results['finetuned']) * 100  
        
        original_avg_score = sum(original_scores) / len(original_scores) if original_scores else 0  
        finetuned_avg_score = sum(finetuned_scores) / len(finetuned_scores) if finetuned_scores else 0  
        
        print(f"ğŸ“ˆ æˆåŠŸç‡å¯¹æ¯”:")  
        print(f"  åŸå§‹æ¨¡å‹: {original_success_rate:.1f}% ({len(original_scores)}/{len(results['original'])})")  
        print(f"  å¾®è°ƒæ¨¡å‹: {finetuned_success_rate:.1f}% ({len(finetuned_scores)}/{len(results['finetuned'])})")  
        
        print(f"\nğŸ¯ å¹³å‡è´¨é‡åˆ†æ•°:")  
        print(f"  åŸå§‹æ¨¡å‹: {original_avg_score:.2f}/10")  
        print(f"  å¾®è°ƒæ¨¡å‹: {finetuned_avg_score:.2f}/10")  
        
        # è¯¦ç»†å¯¹æ¯”  
        print(f"\nğŸ“‹ è¯¦ç»†å¯¹æ¯”:")  
        for i, (orig, fine) in enumerate(zip(results['original'], results['finetuned'])):  
            prompt = orig['prompt'][:50] + "..." if len(orig['prompt']) > 50 else orig['prompt']  
            
            orig_status = f"{orig['score']}/10" if orig['success'] else "å¤±è´¥"  
            fine_status = f"{fine['score']}/10" if fine['success'] else "å¤±è´¥"  
            
            winner = ""  
            if orig['success'] and fine['success']:  
                if fine['score'] > orig['score']:  
                    winner = "ğŸ“ˆ å¾®è°ƒèƒœ"  
                elif fine['score'] < orig['score']:  
                    winner = "ğŸ“‰ åŸå§‹èƒœ"  
                else:  
                    winner = "ğŸ¤ å¹³å±€"  
            
            print(f"  {i+1}. {prompt}")  
            print(f"     åŸå§‹: {orig_status} | å¾®è°ƒ: {fine_status} {winner}")  
        
        # ä¿å­˜è¯¦ç»†ç»“æœ  
        os.makedirs("./output/model_comparison", exist_ok=True)  
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")  
        
        with open(f"./output/model_comparison/comparison_{timestamp}.json", 'w', encoding='utf-8') as f:  
            json.dump(results, f, indent=2, ensure_ascii=False)  
        
        # ç”Ÿæˆä»£ç ç¤ºä¾‹å¯¹æ¯”  
        self.generate_code_examples(results, timestamp)  
        
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœä¿å­˜åˆ°: ./output/model_comparison/comparison_{timestamp}.json")  
    
    def generate_code_examples(self, results, timestamp):  
        """ç”Ÿæˆä»£ç ç¤ºä¾‹å¯¹æ¯”æ–‡ä»¶"""  
        examples_file = f"./output/model_comparison/code_examples_{timestamp}.md"  
        
        with open(examples_file, 'w', encoding='utf-8') as f:  
            f.write("# æ¤…å­è®¾è®¡ä»£ç ç”Ÿæˆå¯¹æ¯”\n\n")  
            f.write(f"ç”Ÿæˆæ—¶é—´: {results['comparison_time']}\n\n")  
            
            for i, (orig, fine) in enumerate(zip(results['original'], results['finetuned'])):  
                f.write(f"## æµ‹è¯• {i+1}: {orig['prompt']}\n\n")  
                
                f.write(f"### åŸå§‹æ¨¡å‹ (åˆ†æ•°: {orig['score'] if orig['success'] else 'å¤±è´¥'}/10)\n")  
                if orig['success']:  
                    f.write("```python\n")  
                    f.write(orig['generated'])  
                    f.write("\n```\n\n")  
                else:  
                    f.write("âŒ ç”Ÿæˆå¤±è´¥\n\n")  
                
                f.write(f"### å¾®è°ƒæ¨¡å‹ (åˆ†æ•°: {fine['score'] if fine['success'] else 'å¤±è´¥'}/10)\n")  
                if fine['success']:  
                    f.write("```python\n")  
                    f.write(fine['generated'])  
                    f.write("\n```\n\n")  
                else:  
                    f.write("âŒ ç”Ÿæˆå¤±è´¥\n\n")  
                
                f.write("---\n\n")  
        
        print(f"ğŸ“ ä»£ç ç¤ºä¾‹ä¿å­˜åˆ°: {examples_file}")  

if __name__ == "__main__":  
    comparator = ModelComparator()  
    comparator.compare_models()  
