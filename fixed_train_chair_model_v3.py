"""  
ä¿®å¤è®¾å¤‡é—®é¢˜çš„æ¤…å­é£æ ¼ç”Ÿæˆæ¨¡å‹è®­ç»ƒè„šæœ¬ v3  
"""  

import argparse  
import json  
import os  
from pathlib import Path  
import torch  
from fixed_fine_tune_blender_llm_v2 import BlenderLLMFineTuner  

def setup_single_gpu():  
    """è®¾ç½®ä½¿ç”¨å•ä¸ªGPU"""  
    if torch.cuda.is_available():  
        # å¼ºåˆ¶ä½¿ç”¨ç¬¬ä¸€ä¸ªGPU  
        torch.cuda.set_device(0)  
        os.environ["CUDA_VISIBLE_DEVICES"] = "0"  
        print(f"ğŸ¯ Set to use only GPU 0: {torch.cuda.get_device_name(0)}")  
        return "cuda:0"  
    else:  
        print("âš ï¸  CUDA not available, using CPU")  
        return "cpu"  

def main():  
    parser = argparse.ArgumentParser(description='Train Chair Style Generation Model (Device Fixed v3)')  
    parser.add_argument('--data_path', type=str, required=True, help='Path to training data')  
    parser.add_argument('--base_model', type=str, required=True, help='Path to base model')  
    parser.add_argument('--output_dir', type=str, default='./output', help='Output directory')  
    parser.add_argument('--epochs', type=int, default=1, help='Number of training epochs')  
    parser.add_argument('--batch_size', type=int, default=1, help='Batch size')  
    parser.add_argument('--learning_rate', type=float, default=2e-5, help='Learning rate')  
    parser.add_argument('--max_length', type=int, default=512, help='Maximum sequence length')  
    
    args = parser.parse_args()  
    
    print("ğŸª‘ Starting Chair Style Generation Model Training (Device Fixed v3)...")  
    print(f"ğŸ“Š Data path: {args.data_path}")  
    print(f"ğŸ¤– Base model: {args.base_model}")  
    print(f"ğŸ“ Output directory: {args.output_dir}")  
    
    # è®¾ç½®å•GPU  
    device = setup_single_gpu()  
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶  
    if not Path(args.data_path).exists():  
        print(f"âŒ Data file not found: {args.data_path}")  
        return  
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„  
    if not Path(args.base_model).exists():  
        print(f"âŒ Base model path does not exist: {args.base_model}")  
        return  
    
    try:  
        # åˆ›å»ºfine-tunerï¼ŒæŒ‡å®šè®¾å¤‡  
        print(f"ğŸ”§ Creating fine-tuner with device: {device}")  
        fine_tuner = BlenderLLMFineTuner(args.base_model, device=device)  
        
        # åŠ è½½æ¨¡å‹  
        print("ğŸ”„ Loading model and tokenizer...")  
        fine_tuner.load_model()  
        
        # åˆ›å»ºæ•°æ®é›†  
        print("ğŸ“š Creating dataset...")  
        dataset = fine_tuner.create_dataset(args.data_path, args.max_length)
        if len(dataset) == 0:  
            print("âŒ Dataset is empty! Please check your data format.")  
            return  
        
        print(f"âœ… Dataset loaded with {len(dataset)} samples")  
        
        # æ˜¾ç¤ºæ ·æœ¬æ•°æ®ï¼ˆåŸå§‹æ•°æ®ï¼Œä¸æ˜¯tokenizedçš„ï¼‰  
        print("ğŸ“ Sample data preview:")  
        with open(args.data_path, 'r', encoding='utf-8') as f:  
            sample_data = json.load(f)  
            if sample_data:  
                sample = sample_data[0]  
                print(f"   Input: {sample.get('input', '')[:100]}...")  
                print(f"   Output: {sample.get('output', '')[:100]}...")  
        
        # å¼€å§‹è®­ç»ƒ  
        print("ğŸ‹ï¸ Starting training...")  
        model_path = fine_tuner.train(  
            dataset=dataset,  
            output_dir=args.output_dir,  
            epochs=args.epochs,  
            batch_size=args.batch_size,  
            learning_rate=args.learning_rate,  
            warmup_steps=50,  
            logging_steps=10,  
            save_steps=200,  
            fp16=True,  
            gradient_accumulation_steps=1  
        )  
        
        print(f"ğŸ‰ Training completed successfully!")  
        print(f"ğŸ’¾ Model saved to: {model_path}")  
        
        # ç®€å•æµ‹è¯•  
        print("ğŸ§ª Testing model with sample prompt...")  
        test_prompt = "Generate chair design: modern minimalist"  
        
        try:  
            result = fine_tuner.generate(test_prompt, max_length=200)  
            print(f"ğŸ“ Test prompt: {test_prompt}")  
            print(f"   Result: {result[:150]}...")  
        except Exception as e:  
            print(f"âš ï¸  Test generation failed: {e}")  
        
    except Exception as e:  
        print(f"âŒ Training failed with error: {e}")  
        import traceback  
        traceback.print_exc()  
        return  
    
    print("âœ… All done!")  

if __name__ == '__main__':  
    main()  
