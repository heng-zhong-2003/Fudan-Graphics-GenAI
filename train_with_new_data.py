#!/usr/bin/env python3  
"""  
ä½¿ç”¨æ–°ç”Ÿæˆçš„æ•°æ®è¿›è¡Œè®­ç»ƒ  
"""  

import torch  
import json  
from transformers import AutoTokenizer, AutoModelForCausalLM  
import os  

def train_with_new_data():  
    print("ğŸª‘ ä½¿ç”¨æ–°æ•°æ®è®­ç»ƒæ¤…å­è®¾è®¡æ¨¡å‹...")  
    
    os.environ["CUDA_VISIBLE_DEVICES"] = "0"  
    torch.cuda.empty_cache()  
    
    # åŠ è½½æ–°æ•°æ®  
    data_file = "./output/new_training_data/chair_training_data.json"  
    if not os.path.exists(data_file):  
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")  
        print("è¯·å…ˆè¿è¡Œ regenerate_training_data.py")  
        return  
    
    with open(data_file, 'r', encoding='utf-8') as f:  
        training_data = json.load(f)  
    
    print(f"ğŸ“Š è®­ç»ƒæ•°æ®é‡: {len(training_data)}")  
    
    # éªŒè¯æ•°æ®è´¨é‡  
    valid_data = []  
    for item in training_data:  
        input_text = item.get('input', '').strip()  
        output_text = item.get('output', '').strip()  
        
        if input_text and output_text and len(output_text) > 50:  
            valid_data.append(item)  
    
    print(f"âœ… æœ‰æ•ˆæ•°æ®é‡: {len(valid_data)}")  
    
    # é™åˆ¶è®­ç»ƒæ•°æ®é‡ä»¥åŠ å¿«è®­ç»ƒ  
    if len(valid_data) > 100:  
        valid_data = valid_data[:100]  
        print(f"ğŸ¯ ä½¿ç”¨å‰100ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒ")  
    
    # æ˜¾ç¤ºæ ·æœ¬  
    print("\nğŸ“ æ•°æ®æ ·æœ¬:")  
    for i in range(min(2, len(valid_data))):  
        sample = valid_data[i]  
        print(f"\næ ·æœ¬ {i+1}:")  
        print(f"  è¾“å…¥: {sample['input']}")  
        print(f"  è¾“å‡º: {sample['output'][:200]}...")  
    
    # åŠ è½½æ¨¡å‹  
    print("\nğŸ”„ åŠ è½½æ¨¡å‹...")  
    tokenizer = AutoTokenizer.from_pretrained("../models/BlenderLLM", trust_remote_code=True)  
    model = AutoModelForCausalLM.from_pretrained(  
        "../models/BlenderLLM",  
        trust_remote_code=True,  
        torch_dtype=torch.float16,  
        device_map="auto",  
        low_cpu_mem_usage=True  
    )  
    
    if tokenizer.pad_token is None:  
        tokenizer.pad_token = tokenizer.eos_token  
        tokenizer.pad_token_id = tokenizer.eos_token_id  
    
    # åªè®­ç»ƒéƒ¨åˆ†å‚æ•°ä»¥åŠ å¿«è®­ç»ƒ  
    for param in model.parameters():  
        param.requires_grad = False  
    
    # è§£å†»æœ€åå‡ å±‚  
    if hasattr(model, 'lm_head'):  
        for param in model.lm_head.parameters():  
            param.requires_grad = True  
    
    # å¦‚æœæœ‰transformerå±‚ï¼Œè§£å†»æœ€å1-2å±‚  
    if hasattr(model, 'transformer') and hasattr(model.transformer, 'h'):  
        for layer in model.transformer.h[-2:]:  # æœ€å2å±‚  
            for param in layer.parameters():  
                param.requires_grad = True  
    elif hasattr(model, 'model') and hasattr(model.model, 'layers'):  
        for layer in model.model.layers[-2:]:  # æœ€å2å±‚  
            for param in layer.parameters():  
                param.requires_grad = True  
    
    # ç»Ÿè®¡å¯è®­ç»ƒå‚æ•°  
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)  
    total_params = sum(p.numel() for p in model.parameters())  
    print(f"ğŸ¯ å¯è®­ç»ƒå‚æ•°: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.2f}%)")  
    
    # ä¼˜åŒ–å™¨  
    optimizer = torch.optim.AdamW(  
        [p for p in model.parameters() if p.requires_grad],   
        lr=1e-5,  
        weight_decay=0.01,  
        eps=1e-8  
    )  
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨  
    from transformers import get_linear_schedule_with_warmup  
    total_steps = len(valid_data)  
    scheduler = get_linear_schedule_with_warmup(  
        optimizer,  
        num_warmup_steps=max(1, total_steps // 10),  
        num_training_steps=total_steps  
    )  
    
    model.train()  
    
    print(f"\nğŸ‹ï¸ å¼€å§‹è®­ç»ƒ {total_steps} æ­¥...")  
    
    total_loss = 0  
    valid_steps = 0  
    log_interval = max(1, len(valid_data) // 10)  
    
    for i, item in enumerate(valid_data):  
        if i % log_interval == 0:  
            print(f"æ­¥éª¤ {i}/{len(valid_data)} ({100*i/len(valid_data):.1f}%)")  
            torch.cuda.empty_cache()  
        
        # æ„å»ºè®­ç»ƒæ–‡æœ¬ - ä½¿ç”¨æ›´æ¸…æ™°çš„æ ¼å¼  
        conversation = f"Human: {item['input']}\n\nAssistant: {item['output']}"  
        
        # Tokenize  
        try:  
            inputs = tokenizer(  
                conversation,  
                return_tensors="pt",  
                max_length=1024,  # å¢åŠ æœ€å¤§é•¿åº¦  
                truncation=True,  
                padding=True  
            )  
            inputs = {k: v.to(model.device) for k, v in inputs.items()}  
            
            # å‰å‘ä¼ æ’­  
            outputs = model(**inputs, labels=inputs['input_ids'])  
            loss = outputs.loss  
            
            # æ£€æŸ¥æŸå¤±æœ‰æ•ˆæ€§  
            if torch.isnan(loss) or torch.isinf(loss):  
                print(f"âš ï¸  æ­¥éª¤ {i} æŸå¤±æ— æ•ˆ: {loss}")  
                optimizer.zero_grad()  
                continue  
            
            # åå‘ä¼ æ’­  
            loss.backward()  
            
            # æ¢¯åº¦è£å‰ª  
            torch.nn.utils.clip_grad_norm_(  
                [p for p in model.parameters() if p.requires_grad],  
                max_norm=1.0  
            )  
            
            # æ›´æ–°å‚æ•°  
            optimizer.step()  
            scheduler.step()  
            optimizer.zero_grad()  
            
            total_loss += loss.item()  
            valid_steps += 1  
            
            # å®šæœŸæ˜¾ç¤ºæŸå¤±  
            if i % log_interval == 0:  
                current_lr = scheduler.get_last_lr()[0]  
                print(f"  å½“å‰æŸå¤±: {loss.item():.4f}, å­¦ä¹ ç‡: {current_lr:.2e}")  
                
        except Exception as e:  
            print(f"âš ï¸  æ­¥éª¤ {i} å‡ºé”™: {e}")  
            optimizer.zero_grad()  
            continue  
    
    avg_loss = total_loss / max(valid_steps, 1)  
    print(f"\nâœ… è®­ç»ƒå®Œæˆ!")  
    print(f"ğŸ“Š å¹³å‡æŸå¤±: {avg_loss:.4f}")  
    print(f"ğŸ“Š æœ‰æ•ˆæ­¥éª¤: {valid_steps}/{len(valid_data)}")  
    
    # ä¿å­˜æ¨¡å‹  
    output_dir = "./output/new_chair_model"  
    print(f"\nğŸ’¾ ä¿å­˜æ¨¡å‹åˆ°: {output_dir}")  
    os.makedirs(output_dir, exist_ok=True)  
    
    model.save_pretrained(output_dir)  
    tokenizer.save_pretrained(output_dir)  
    
    # ä¿å­˜è®­ç»ƒä¿¡æ¯  
    training_info = {  
        "training_samples": len(valid_data),  
        "valid_steps": valid_steps,  
        "average_loss": avg_loss,  
        "trainable_params": trainable_params,  
        "total_params": total_params  
    }  
    
    with open(os.path.join(output_dir, "training_info.json"), 'w') as f:  
        json.dump(training_info, f, indent=2)  
    
    # ç«‹å³æµ‹è¯•æ¨¡å‹  
    print(f"\nğŸ§ª æµ‹è¯•è®­ç»ƒåçš„æ¨¡å‹...")  
    model.eval()  
    
    test_cases = [  
        "Generate chair design: modern office chair",  
        "Generate chair design: vintage wooden dining chair",   
        "Generate chair design: comfortable armchair",  
        "Generate chair design: sleek bar stool",  
        "Generate chair design: ergonomic gaming chair"  
    ]  
    
    for j, test_input in enumerate(test_cases):  
        print(f"\n--- æµ‹è¯• {j+1} ---")  
        prompt = f"Human: {test_input}\n\nAssistant:"  
        
        try:  
            inputs = tokenizer(prompt, return_tensors="pt")  
            inputs = {k: v.to(model.device) for k, v in inputs.items()}  
            
            with torch.no_grad():  
                outputs = model.generate(  
                    **inputs,  
                    max_length=len(inputs['input_ids'][0]) + 300,  
                    temperature=0.7,  
                    do_sample=True,  
                    top_p=0.9,  
                    pad_token_id=tokenizer.pad_token_id,  
                    eos_token_id=tokenizer.eos_token_id,  
                    repetition_penalty=1.1  
                )  
            
            result = tokenizer.decode(outputs[0], skip_special_tokens=True)  
            generated = result[len(prompt):].strip()  
            
            print(f"ğŸ“ è¾“å…¥: {test_input}")  
            print(f"ğŸ¤– ç”Ÿæˆ ({len(generated)} å­—ç¬¦):")  
            print(f"   {generated[:400]}...")  
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«Blenderç›¸å…³å†…å®¹  
            blender_keywords = ['bpy.', 'import bpy', 'mesh.', 'object.', 'location', 'scale']  
            if any(keyword in generated for keyword in blender_keywords):  
                print("   âœ… åŒ…å«Blenderä»£ç ")  
            else:  
                print("   âš ï¸  å¯èƒ½ä¸åŒ…å«Blenderä»£ç ")  
                
        except Exception as e:  
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")  
    
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ! æ¨¡å‹ä¿å­˜åœ¨: {output_dir}")  
    
    return output_dir  

if __name__ == "__main__":  
    train_with_new_data()  
