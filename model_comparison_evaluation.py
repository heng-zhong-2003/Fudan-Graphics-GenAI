#!/usr/bin/env python3  
"""  
BlenderLLMåŸå§‹æ¨¡å‹ vs LoRAå¢å¼ºæ¨¡å‹å¯¹æ¯”è¯„ä¼°  
åŒ…å«ä»£ç è´¨é‡ã€æ¤…å­è®¾è®¡ç†è§£ã€å¯è§†åŒ–è¯„åˆ†ç­‰  
"""  

import torch  
from transformers import AutoTokenizer, AutoModelForCausalLM  
from peft import PeftModel  
import matplotlib.pyplot as plt  
import seaborn as sns  
import pandas as pd  
import numpy as np  
import re  
import json  
import os  
from datetime import datetime  

class BlenderCodeEvaluator:  
    """Blenderä»£ç è´¨é‡è¯„ä¼°å™¨"""  
    
    def __init__(self):  
        self.syntax_patterns = {  
            'import_bpy': r'import bpy',  
            'clear_objects': r'bpy\.ops\.object\.(select_all|delete)',  
            'create_primitive': r'bpy\.ops\.mesh\.primitive_\w+_add',  
            'object_naming': r'\.name\s*=\s*["\']',  
            'scaling': r'\.scale\s*=',  
            'location': r'location\s*=\s*\(',  
            'context_usage': r'bpy\.context\.',  
        }  
        
        self.chair_features = {  
            'seat': ['seat', 'cushion'],  
            'backrest': ['back', 'backrest'],  
            'legs': ['leg', 'support'],  
            'armrest': ['arm', 'armrest'],  
            'base': ['base', 'foundation']  
        }  
    
    def evaluate_code_quality(self, code):  
        """è¯„ä¼°ä»£ç è´¨é‡ (0-10åˆ†)"""  
        if not code or len(code.strip()) < 20:  
            return 0  
        
        score = 0  
        total_checks = len(self.syntax_patterns)  
        
        for pattern_name, pattern in self.syntax_patterns.items():  
            if re.search(pattern, code, re.IGNORECASE):  
                score += 1  
        
        # é¢å¤–æ£€æŸ¥  
        if 'import bpy' in code:  
            score += 1  
        if len(code.split('\n')) > 5:  # ä»£ç é•¿åº¦åˆç†  
            score += 1  
        if '#' in code:  # æœ‰æ³¨é‡Š  
            score += 1  
        
        return min(10, (score / total_checks) * 10)  
    
    def evaluate_chair_understanding(self, prompt, code):  
        """è¯„ä¼°æ¤…å­è®¾è®¡ç†è§£ (0-10åˆ†)"""  
        if not code:  
            return 0  
        
        prompt_lower = prompt.lower()  
        code_lower = code.lower()  
        
        score = 0  
        
        # åŸºç¡€æ¤…å­ç»„ä»¶æ£€æŸ¥  
        for component, keywords in self.chair_features.items():  
            if any(kw in prompt_lower for kw in keywords):  
                if any(kw in code_lower for kw in keywords):  
                    score += 2  
        
        # ç‰¹æ®Šç‰¹å¾æ£€æŸ¥  
        special_features = {  
            'office': ['cylinder', 'swivel', 'wheel'],  
            'recliner': ['angle', 'tilt', 'rotation'],  
            'minimalist': ['simple', 'clean', 'basic'],  
            'armrest': ['arm', 'support'],  
            'cushion': ['soft', 'padding', 'comfort']  
        }  
        
        for feature, indicators in special_features.items():  
            if feature in prompt_lower:  
                if any(ind in code_lower for ind in indicators):  
                    score += 1  
        
        return min(10, score)  
    
    def evaluate_blender_completeness(self, code):  
        """è¯„ä¼°Blenderä»£ç å®Œæ•´æ€§ (0-10åˆ†)"""  
        if not code:  
            return 0  
        
        completeness_checks = {  
            'has_import': 'import bpy' in code,  
            'has_clear': any(x in code for x in ['clear', 'delete', 'select_all']),  
            'has_creation': 'primitive' in code and 'add' in code,  
            'has_naming': '.name' in code,  
            'has_transform': any(x in code for x in ['scale', 'location', 'rotation']),  
            'has_multiple_objects': code.count('bpy.ops.mesh') > 1,  
            'proper_structure': len(code.split('\n')) > 8  
        }  
        
        score = sum(completeness_checks.values()) * (10 / len(completeness_checks))  
        return min(10, score)  

class ModelComparator:  
    """æ¨¡å‹å¯¹æ¯”å™¨"""  
    
    def __init__(self):  
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")  
        self.evaluator = BlenderCodeEvaluator()  
        
    def load_models(self):  
        """åŠ è½½åŸå§‹æ¨¡å‹å’ŒLoRAæ¨¡å‹"""  
        print("ğŸ”„ åŠ è½½æ¨¡å‹...")  
        
        # åŠ è½½tokenizer  
        self.tokenizer = AutoTokenizer.from_pretrained("../models/BlenderLLM", trust_remote_code=True)  
        if self.tokenizer.pad_token is None:  
            self.tokenizer.pad_token = self.tokenizer.eos_token  
        
        # åŠ è½½åŸå§‹æ¨¡å‹  
        print("  ğŸ“¦ åŠ è½½åŸå§‹BlenderLLM...")  
        self.original_model = AutoModelForCausalLM.from_pretrained(  
            "../models/BlenderLLM",  
            trust_remote_code=True,  
            torch_dtype=torch.float16,  
            device_map={"": 0}  
        )  
        
        # åŠ è½½LoRAå¢å¼ºæ¨¡å‹  
        print("  ğŸ”§ åŠ è½½LoRAå¢å¼ºæ¨¡å‹...")  
        base_model = AutoModelForCausalLM.from_pretrained(  
            "../models/BlenderLLM",  
            trust_remote_code=True,  
            torch_dtype=torch.float16,  
            device_map={"": 0}  
        )  
        self.lora_model = PeftModel.from_pretrained(base_model, "./output/lora_blender_enhanced")  
        
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")  
    
    def generate_response(self, model, prompt, max_tokens=200):  
        """ç”Ÿæˆæ¨¡å‹å“åº”"""  
        input_text = f"User: {prompt}\n\nAssistant:"  
        inputs = self.tokenizer(input_text, return_tensors="pt").to(self.device)  
        
        with torch.no_grad():  
            outputs = model.generate(  
                **inputs,  
                max_new_tokens=max_tokens,  
                temperature=0.7,  
                do_sample=True,  
                pad_token_id=self.tokenizer.pad_token_id,  
                eos_token_id=self.tokenizer.eos_token_id  
            )  
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)  
        assistant_response = response.split("Assistant:")[-1].strip()  
        return assistant_response  
    
    def evaluate_models(self, test_prompts):  
        """è¯„ä¼°ä¸¤ä¸ªæ¨¡å‹"""  
        print("\nğŸ¯ å¼€å§‹æ¨¡å‹å¯¹æ¯”è¯„ä¼°...")  
        
        results = []  
        
        for i, prompt in enumerate(test_prompts, 1):  
            print(f"\nğŸ“ æµ‹è¯• {i}/{len(test_prompts)}: {prompt[:50]}...")  
            
            # ç”Ÿæˆå“åº”  
            original_response = self.generate_response(self.original_model, prompt)  
            lora_response = self.generate_response(self.lora_model, prompt)  
            
            # è¯„ä¼°åŸå§‹æ¨¡å‹  
            original_scores = {  
                'code_quality': self.evaluator.evaluate_code_quality(original_response),  
                'chair_understanding': self.evaluator.evaluate_chair_understanding(prompt, original_response),  
                'completeness': self.evaluator.evaluate_blender_completeness(original_response)  
            }  
            
            # è¯„ä¼°LoRAæ¨¡å‹  
            lora_scores = {  
                'code_quality': self.evaluator.evaluate_code_quality(lora_response),  
                'chair_understanding': self.evaluator.evaluate_chair_understanding(prompt, lora_response),  
                'completeness': self.evaluator.evaluate_blender_completeness(lora_response)  
            }  
            
            # ä¿å­˜ç»“æœ  
            result = {  
                'prompt': prompt,  
                'original_response': original_response,  
                'lora_response': lora_response,  
                'original_scores': original_scores,  
                'lora_scores': lora_scores  
            }  
            results.append(result)  
            
            print(f"  ğŸ“Š åŸå§‹æ¨¡å‹å¾—åˆ†: {sum(original_scores.values()):.1f}/30")  
            print(f"  ğŸ”§ LoRAæ¨¡å‹å¾—åˆ†: {sum(lora_scores.values()):.1f}/30")  
        
        return results  
    
    def create_visualizations(self, results):  
        """åˆ›å»ºå¯è§†åŒ–è¯„åˆ†å›¾è¡¨"""  
        print("\nğŸ“Š ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")  
        
        # å‡†å¤‡æ•°æ®  
        categories = ['Code Quality', 'Chair Understanding', 'Completeness']  
        original_avg = []  
        lora_avg = []  
        
        for category in ['code_quality', 'chair_understanding', 'completeness']:  
            orig_scores = [r['original_scores'][category] for r in results]  
            lora_scores = [r['lora_scores'][category] for r in results]  
            
            original_avg.append(np.mean(orig_scores))  
            lora_avg.append(np.mean(lora_scores))  
        
        # è®¾ç½®å›¾è¡¨æ ·å¼  
        plt.style.use('seaborn-v0_8')  
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))  
        
        # 1. é›·è¾¾å›¾å¯¹æ¯”  
        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False)  
        angles = np.concatenate((angles, [angles[0]]))  
        
        original_avg_radar = original_avg + [original_avg[0]]  
        lora_avg_radar = lora_avg + [lora_avg[0]]  
        
        ax1.plot(angles, original_avg_radar, 'o-', linewidth=2, label='Original BlenderLLM', color='#FF6B6B')  
        ax1.fill(angles, original_avg_radar, alpha=0.25, color='#FF6B6B')  
        ax1.plot(angles, lora_avg_radar, 'o-', linewidth=2, label='LoRA Enhanced', color='#4ECDC4')  
        ax1.fill(angles, lora_avg_radar, alpha=0.25, color='#4ECDC4')  
        
        ax1.set_xticks(angles[:-1])  
        ax1.set_xticklabels(categories)  
        ax1.set_ylim(0, 10)  
        ax1.set_title('Model Performance Radar Chart', fontsize=14, fontweight='bold')  
        ax1.legend()  
        ax1.grid(True)  
        
        # 2. æŸ±çŠ¶å›¾å¯¹æ¯”  
        x = np.arange(len(categories))  
        width = 0.35  
        
        bars1 = ax2.bar(x - width/2, original_avg, width, label='Original BlenderLLM', color='#FF6B6B', alpha=0.8)  
        bars2 = ax2.bar(x + width/2, lora_avg, width, label='LoRA Enhanced', color='#4ECDC4', alpha=0.8)  
        
        ax2.set_xlabel('Evaluation Categories')  
        ax2.set_ylabel('Average Score (0-10)')  
        ax2.set_title('Average Performance Comparison', fontsize=14, fontweight='bold')  
        ax2.set_xticks(x)  
        ax2.set_xticklabels(categories)  
        ax2.legend()  
        ax2.grid(True, alpha=0.3)  
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾  
        for bar in bars1:  
            height = bar.get_height()  
            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.1, f'{height:.1f}',   
                    ha='center', va='bottom')  
        for bar in bars2:  
            height = bar.get_height()  
            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.1, f'{height:.1f}',   
                    ha='center', va='bottom')  
        
        # 3. è¯¦ç»†åˆ†æ•°åˆ†å¸ƒ  
        all_original_scores = []  
        all_lora_scores = []  
        
        for result in results:  
            all_original_scores.extend(result['original_scores'].values())  
            all_lora_scores.extend(result['lora_scores'].values())  
        
        ax3.hist(all_original_scores, bins=10, alpha=0.7, label='Original BlenderLLM', color='#FF6B6B')  
        ax3.hist(all_lora_scores, bins=10, alpha=0.7, label='LoRA Enhanced', color='#4ECDC4')  
        ax3.set_xlabel('Score')  
        ax3.set_ylabel('Frequency')  
        ax3.set_title('Score Distribution', fontsize=14, fontweight='bold')  
        ax3.legend()  
        ax3.grid(True, alpha=0.3)  
        
        # 4. æ”¹è¿›åº¦é‡åŒ–  
        improvements = [(lora_avg[i] - original_avg[i]) for i in range(len(categories))]  
        colors = ['#45B7D1' if imp > 0 else '#F96167' for imp in improvements]  
        
        bars = ax4.bar(categories, improvements, color=colors, alpha=0.8)  
        ax4.axhline(y=0, color='black', linestyle='-', alpha=0.3)  
        ax4.set_ylabel('Improvement Score')  
        ax4.set_title('LoRA Enhancement Impact', fontsize=14, fontweight='bold')  
        ax4.grid(True, alpha=0.3)  
        
        # æ·»åŠ æ”¹è¿›æ•°å€¼  
        for bar, imp in zip(bars, improvements):  
            height = bar.get_height()  
            ax4.text(bar.get_x() + bar.get_width()/2., height + (0.1 if height > 0 else -0.2),   
                    f'{imp:+.1f}', ha='center', va='bottom' if height > 0 else 'top')  
        
        plt.tight_layout()  
        
        # ä¿å­˜å›¾è¡¨  
        os.makedirs('./output/evaluation_results', exist_ok=True)  
        plt.savefig('./output/evaluation_results/model_comparison.png', dpi=300, bbox_inches='tight')  
        print("ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: ./output/evaluation_results/model_comparison.png")  
        
        return fig  
    
    def generate_report(self, results):  
        """ç”Ÿæˆè¯¦ç»†è¯„ä¼°æŠ¥å‘Š"""  
        print("\nğŸ“‹ ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š...")  
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®  
        original_total = [sum(r['original_scores'].values()) for r in results]  
        lora_total = [sum(r['lora_scores'].values()) for r in results]  
        
        report = {  
            "evaluation_timestamp": datetime.now().isoformat(),  
            "test_samples": len(results),  
            "summary": {  
                "original_model": {  
                    "avg_total_score": np.mean(original_total),  
                    "avg_code_quality": np.mean([r['original_scores']['code_quality'] for r in results]),  
                    "avg_chair_understanding": np.mean([r['original_scores']['chair_understanding'] for r in results]),  
                    "avg_completeness": np.mean([r['original_scores']['completeness'] for r in results])  
                },  
                "lora_model": {  
                    "avg_total_score": np.mean(lora_total),  
                    "avg_code_quality": np.mean([r['lora_scores']['code_quality'] for r in results]),  
                    "avg_chair_understanding": np.mean([r['lora_scores']['chair_understanding'] for r in results]),  
                    "avg_completeness": np.mean([r['lora_scores']['completeness'] for r in results])  
                },  
                "improvement": {  
                    "total_score_improvement": np.mean(lora_total) - np.mean(original_total),  
                    "code_quality_improvement": np.mean([r['lora_scores']['code_quality'] for r in results]) - np.mean([r['original_scores']['code_quality'] for r in results]),  
                    "chair_understanding_improvement": np.mean([r['lora_scores']['chair_understanding'] for r in results]) - np.mean([r['original_scores']['chair_understanding'] for r in results]),  
                    "completeness_improvement": np.mean([r['lora_scores']['completeness'] for r in results]) - np.mean([r['original_scores']['completeness'] for r in results])  
                }  
            },  
            "detailed_results": results  
        }  
        
        # ä¿å­˜æŠ¥å‘Š  
        with open('./output/evaluation_results/detailed_report.json', 'w') as f:  
            json.dump(report, f, indent=2)  
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š  
        markdown_report = self.create_markdown_report(report)  
        with open('./output/evaluation_results/evaluation_report.md', 'w') as f:  
            f.write(markdown_report)  
        
        print("ğŸ“Š è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜:")  
        print("  - JSONæ ¼å¼: ./output/evaluation_results/detailed_report.json")  
        print("  - Markdownæ ¼å¼: ./output/evaluation_results/evaluation_report.md")  
        
        return report  
    
    def create_markdown_report(self, report):  
        """åˆ›å»ºMarkdownæ ¼å¼çš„æŠ¥å‘Š"""  
        md = f"""# BlenderLLM æ¨¡å‹å¯¹æ¯”è¯„ä¼°æŠ¥å‘Š  

            ## ğŸ“Š è¯„ä¼°æ¦‚å†µ  

            **è¯„ä¼°æ—¶é—´**: {report['evaluation_timestamp']}  
            **æµ‹è¯•æ ·æœ¬æ•°**: {report['test_samples']}  

            ## ğŸ¯ æ€»ä½“æ€§èƒ½å¯¹æ¯”  

            | æ¨¡å‹ | æ€»åˆ† | ä»£ç è´¨é‡ | æ¤…å­ç†è§£ | å®Œæ•´æ€§ |  
            |------|------|----------|----------|--------|  
            | åŸå§‹BlenderLLM | {report['summary']['original_model']['avg_total_score']:.2f}/30 | {report['summary']['original_model']['avg_code_quality']:.2f}/10 | {report['summary']['original_model']['avg_chair_understanding']:.2f}/10 | {report['summary']['original_model']['avg_completeness']:.2f}/10 |  
            | LoRAå¢å¼ºç‰ˆæœ¬ | {report['summary']['lora_model']['avg_total_score']:.2f}/30 | {report['summary']['lora_model']['avg_code_quality']:.2f}/10 | {report['summary']['lora_model']['avg_chair_understanding']:.2f}/10 | {report['summary']['lora_model']['avg_completeness']:.2f}/10 |  

            ## ğŸ“ˆ æ”¹è¿›æ•ˆæœ  

            | è¯„ä¼°ç»´åº¦ | æ”¹è¿›åˆ†æ•° | æ”¹è¿›ç™¾åˆ†æ¯” |  
            |----------|----------|------------|  
            | **æ€»ä½“æ€§èƒ½** | {report['summary']['improvement']['total_score_improvement']:+.2f} | {(report['summary']['improvement']['total_score_improvement']/report['summary']['original_model']['avg_total_score']*100):+.1f}% |  
            | **ä»£ç è´¨é‡** | {report['summary']['improvement']['code_quality_improvement']:+.2f} | {(report['summary']['improvement']['code_quality_improvement']/report['summary']['original_model']['avg_code_quality']*100 if report['summary']['original_model']['avg_code_quality'] > 0 else 0):+.1f}% |  
            | **æ¤…å­ç†è§£** | {report['summary']['improvement']['chair_understanding_improvement']:+.2f} | {(report['summary']['improvement']['chair_understanding_improvement']/report['summary']['original_model']['avg_chair_understanding']*100 if report['summary']['original_model']['avg_chair_understanding'] > 0 else 0):+.1f}% |  
            | **ä»£ç å®Œæ•´æ€§** | {report['summary']['improvement']['completeness_improvement']:+.2f} | {(report['summary']['improvement']['completeness_improvement']/report['summary']['original_model']['avg_completeness']*100 if report['summary']['original_model']['avg_completeness'] > 0 else 0):+.1f}% |  

            ## ğŸ“ è¯¦ç»†æµ‹è¯•ç»“æœ  

            """  
        
        for i, result in enumerate(report['detailed_results'], 1):  
            md += f"""  
            ### æµ‹è¯•æ¡ˆä¾‹ {i}  

            **æç¤º**: {result['prompt']}  

            **è¯„åˆ†å¯¹æ¯”**:  
            - åŸå§‹æ¨¡å‹: {sum(result['original_scores'].values()):.1f}/30 (ä»£ç :{result['original_scores']['code_quality']:.1f} | ç†è§£:{result['original_scores']['chair_understanding']:.1f} | å®Œæ•´:{result['original_scores']['completeness']:.1f})  
            - LoRAæ¨¡å‹: {sum(result['lora_scores'].values()):.1f}/30 (ä»£ç :{result['lora_scores']['code_quality']:.1f} | ç†è§£:{result['lora_scores']['chair_understanding']:.1f} | å®Œæ•´:{result['lora_scores']['completeness']:.1f})  

            **åŸå§‹æ¨¡å‹ç”Ÿæˆ**:  
            ```python  
            {result['original_response'][:300]}{'...' if len(result['original_response']) > 300 else ''}
            LoRAæ¨¡å‹ç”Ÿæˆ:
            {result['lora_response'][:300]}{'...' if len(result['lora_response']) > 300 else ''}  
            """

        md += f"""  
        ğŸ¯ ç»“è®º
        {self.generate_conclusion(report)}
        æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
        """
        return md

def generate_conclusion(self, report):  
    """ç”Ÿæˆç»“è®º"""  
    improvement = report['summary']['improvement']['total_score_improvement']  
    
    if improvement > 2:  
        conclusion = "ğŸ‰ **æ˜¾è‘—æ”¹è¿›**: LoRAå¾®è°ƒæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½"  
    elif improvement > 0.5:  
        conclusion = "âœ… **æ˜æ˜¾æ”¹è¿›**: LoRAå¾®è°ƒæœ‰æ•ˆæå‡äº†æ¨¡å‹èƒ½åŠ›"  
    elif improvement > 0:  
        conclusion = "ğŸ“ˆ **è½»å¾®æ”¹è¿›**: LoRAå¾®è°ƒå¸¦æ¥äº†å°å¹…æå‡"  
    else:  
        conclusion = "âš ï¸ **éœ€è¦ä¼˜åŒ–**: å»ºè®®è°ƒæ•´è®­ç»ƒç­–ç•¥"  
    
    best_category = max(report['summary']['improvement'], key=report['summary']['improvement'].get)  
    
    conclusion += f"\n\n**æœ€å¤§æ”¹è¿›é¢†åŸŸ**: {best_category.replace('_', ' ').title()}"  
    conclusion += f"\n\n**æ¨è**: åŸºäºè¯„ä¼°ç»“æœï¼ŒLoRAå¢å¼ºç‰ˆæœ¬åœ¨æ¤…å­è®¾è®¡ç†è§£æ–¹é¢è¡¨ç°æ›´å¥½ï¼Œå»ºè®®åœ¨å®é™…åº”ç”¨ä¸­ä½¿ç”¨ã€‚"  
    
    return conclusion  

def main():
    """ä¸»è¯„ä¼°å‡½æ•°"""
    print("ğŸ¯ BlenderLLM æ¨¡å‹å¯¹æ¯”è¯„ä¼°ç³»ç»Ÿ")
    print("=" * 50)

    # æµ‹è¯•ç”¨ä¾‹  
    test_prompts = [  
        "Design a chair: modern minimalist office chair with wheels",  
        "Design a chair: comfortable recliner with armrests and cushions",  
        "Design a chair: dining chair with tall backrest and no armrests",  
        "Design a chair: ergonomic gaming chair with adjustable height",  
        "Design a chair: vintage wooden chair with carved details",  
        "Design a chair: bar stool with footrest and swivel function",  
        "Design a chair: bean bag chair for relaxation",  
        "Design a chair: folding chair for outdoor use"  
    ]  

    # åˆ›å»ºå¯¹æ¯”å™¨  
    comparator = ModelComparator()  

    try:  
        # åŠ è½½æ¨¡å‹  
        comparator.load_models()  
        
        # è¯„ä¼°æ¨¡å‹  
        results = comparator.evaluate_models(test_prompts)  
        
        # åˆ›å»ºå¯è§†åŒ–  
        fig = comparator.create_visualizations(results)  
        
        # ç”ŸæˆæŠ¥å‘Š  
        report = comparator.generate_report(results)  
        
        # æ˜¾ç¤ºæ€»ç»“  
        print("\nğŸ¯ è¯„ä¼°æ€»ç»“:")  
        print("=" * 30)  
        orig_avg = report['summary']['original_model']['avg_total_score']  
        lora_avg = report['summary']['lora_model']['avg_total_score']  
        improvement = report['summary']['improvement']['total_score_improvement']  
        
        print(f"ğŸ“Š åŸå§‹BlenderLLMå¹³å‡å¾—åˆ†: {orig_avg:.2f}/30")  
        print(f"ğŸ”§ LoRAå¢å¼ºç‰ˆæœ¬å¹³å‡å¾—åˆ†: {lora_avg:.2f}/30")  
        print(f"ğŸ“ˆ æ€»ä½“æ”¹è¿›: {improvement:+.2f} ({improvement/orig_avg*100:+.1f}%)")  
        
        # åˆ†ç±»æ”¹è¿›  
        categories = ['code_quality', 'chair_understanding', 'completeness']  
        category_names = ['ä»£ç è´¨é‡', 'æ¤…å­ç†è§£', 'ä»£ç å®Œæ•´æ€§']  
        
        print(f"\nğŸ“‹ è¯¦ç»†æ”¹è¿›:")  
        for cat, name in zip(categories, category_names):  
            imp = report['summary']['improvement'][f'{cat}_improvement']  
            orig = report['summary']['original_model'][f'avg_{cat}']  
            print(f"  {name}: {imp:+.2f} ({imp/orig*100 if orig > 0 else 0:+.1f}%)")  
        
        print(f"\nğŸ“ ç»“æœæ–‡ä»¶:")  
        print(f"  - å¯è§†åŒ–å›¾è¡¨: ./output/evaluation_results/model_comparison.png")  
        print(f"  - è¯¦ç»†æŠ¥å‘Š: ./output/evaluation_results/evaluation_report.md")  
        print(f"  - JSONæ•°æ®: ./output/evaluation_results/detailed_report.json")  
        
        plt.show()  # æ˜¾ç¤ºå›¾è¡¨  
        
    except Exception as e:  
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")  
        import traceback  
        traceback.print_exc()  

if __name__ == "__main__":
    main()