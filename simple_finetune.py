#!/usr/bin/env python3  
"""  
ç®€åŒ–çš„å¾®è°ƒæ–¹æ³• - é¿å…deviceé”™è¯¯  
"""  

import torch  
from transformers import AutoTokenizer, AutoModelForCausalLM  
import json  
import os  
from torch.optim import AdamW  
from torch.nn import CrossEntropyLoss  

def simple_finetune():  
    print("ğŸ”§ ç®€åŒ–å¾®è°ƒæ¤…å­è®¾è®¡æ¨¡å‹...")  
    
    # æ¸…ç†GPUå†…å­˜  
    torch.cuda.empty_cache()  
    
    # åŠ è½½æ•°æ®  
    data_file = "./output/new_training_data/chair_training_data.json"  
    with open(data_file, 'r', encoding='utf-8') as f:  
        training_data = json.load(f)  
    
    # åªç”¨å¾ˆå°‘çš„æ•°æ®  
    training_data = training_data[:5]  
    print(f"ğŸ“Š ä½¿ç”¨ {len(training_data)} ä¸ªæ ·æœ¬")  
    
    # åŠ è½½æ¨¡å‹ - ä¸ä½¿ç”¨device_map  
    print("ğŸ”„ åŠ è½½æ¨¡å‹...")  
    tokenizer = AutoTokenizer.from_pretrained("../models/BlenderLLM", trust_remote_code=True)  
    
    # ç›´æ¥åŠ è½½åˆ°GPU  
    model = AutoModelForCausalLM.from_pretrained(  
        "../models/BlenderLLM",  
        trust_remote_code=True,  
        torch_dtype=torch.float32,  
        low_cpu_mem_usage=True  
    ).cuda()  
    
    if tokenizer.pad_token is None:  
        tokenizer.pad_token = tokenizer.eos_token  
    
    # åªè®­ç»ƒæœ€åçš„è¾“å‡ºå±‚  
    for param in model.parameters():  
        param.requires_grad = False  
    
    # åªè®­ç»ƒlm_head  
    if hasattr(model, 'lm_head'):  
        for param in model.lm_head.parameters():  
            param.requires_grad = True  
            
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)  
    print(f"ğŸ¯ å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")  
    
    # ä¼˜åŒ–å™¨  
    optimizer = AdamW([p for p in model.parameters() if p.requires_grad], lr=1e-5)  
    loss_fn = CrossEntropyLoss()  
    
    model.train()  
    
    print("ğŸ‹ï¸ å¼€å§‹è®­ç»ƒ...")  
    
    for epoch in range(2):  # åªè®­ç»ƒ2ä¸ªepoch  
        total_loss = 0  
        
        for i, item in enumerate(training_data):  
            text = f"Human: {item['input']}\n\nAssistant: {item['output']}"  
            
            # Tokenize  
            inputs = tokenizer(  
                text,  
                return_tensors="pt",  
                max_length=512,  
                truncation=True,  
                padding=True  
            ).to('cuda')  
            
            # å‰å‘ä¼ æ’­  
            outputs = model(**inputs, labels=inputs['input_ids'])  
            loss = outputs.loss  
            
            if torch.isnan(loss):  
                print(f"âš ï¸ NaN loss at step {i}, skipping...")  
                continue  
            
            # åå‘ä¼ æ’­  
            optimizer.zero_grad()  
            loss.backward()  
            
            # æ¢¯åº¦è£å‰ª  
            torch.nn.utils.clip_grad_norm_(  
                [p for p in model.parameters() if p.requires_grad],   
                max_norm=1.0  
            )  
            
            optimizer.step()  
            
            total_loss += loss.item()  
            print(f"Epoch {epoch+1}, Step {i+1}/{len(training_data)}, Loss: {loss.item():.4f}")  
        
        avg_loss = total_loss / len(training_data)  
        print(f"Epoch {epoch+1} å¹³å‡æŸå¤±: {avg_loss:.4f}")  
    
    print("âœ… è®­ç»ƒå®Œæˆ")  
    
    # ä¿å­˜æ¨¡å‹  
    output_dir = "./output/simple_finetuned_model"  
    os.makedirs(output_dir, exist_ok=True)  
    
    model.save_pretrained(output_dir)  
    tokenizer.save_pretrained(output_dir)  
    
    print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜åˆ°: {output_dir}")  
    
    # æµ‹è¯•  
    print("\nğŸ§ª æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹...")  
    model.eval()  
    
    test_prompt = "Generate chair design: simple wooden chair"  
    
    try:  
        inputs = tokenizer(test_prompt, return_tensors="pt").to('cuda')  
        
        with torch.no_grad():  
            outputs = model.generate(  
                **inputs,  
                max_length=len(inputs['input_ids'][0]) + 200,  
                temperature=0.7,  
                do_sample=True,  
                pad_token_id=tokenizer.eos_token_id  
            )  
        
        result = tokenizer.decode(outputs[0], skip_special_tokens=True)  
        generated = result[len(test_prompt):].strip()  
        
        print(f"ğŸ“ æµ‹è¯•è¾“å…¥: {test_prompt}")  
        print(f"ğŸ¤– ç”Ÿæˆç»“æœ: {generated[:300]}...")  
        
    except Exception as e:  
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")  

if __name__ == "__main__":  
    simple_finetune()  
