#!/usr/bin/env python3  
"""  
æµ‹è¯•è®­ç»ƒå¥½çš„LoRAæ¨¡å‹  
"""  

import torch  
from transformers import AutoTokenizer, AutoModelForCausalLM  
from peft import PeftModel  
import os  

def test_lora_model():  
    print("ğŸ§ª æµ‹è¯•LoRAå¢å¼ºçš„BlenderLLM")  
    print("=" * 40)  
    
    # è®¾ç½®è®¾å¤‡  
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")  
    print(f"ğŸ¯ ä½¿ç”¨è®¾å¤‡: {device}")  
    
    try:  
        # 1. åŠ è½½åŸºç¡€æ¨¡å‹  
        print("\nğŸ”„ åŠ è½½åŸºç¡€æ¨¡å‹...")  
        base_model = AutoModelForCausalLM.from_pretrained(  
            "../models/BlenderLLM",  
            trust_remote_code=True,  
            torch_dtype=torch.float16,  
            device_map={"": 0}  
        )  
        
        # 2. åŠ è½½tokenizer  
        tokenizer = AutoTokenizer.from_pretrained("../models/BlenderLLM", trust_remote_code=True)  
        if tokenizer.pad_token is None:  
            tokenizer.pad_token = tokenizer.eos_token  
        
        # 3. åŠ è½½LoRAé€‚é…å™¨  
        print("ğŸ”§ åŠ è½½LoRAé€‚é…å™¨...")  
        model = PeftModel.from_pretrained(base_model, "./output/lora_blender_enhanced")  
        model.eval()  
        
        print("âœ… LoRAæ¨¡å‹åŠ è½½æˆåŠŸ!")  
        
        # 4. æµ‹è¯•ä¸åŒç±»å‹çš„æ¤…å­è®¾è®¡  
        test_prompts = [  
            "Design a chair: modern minimalist office chair",  
            "Design a chair: comfortable recliner with armrests",  
            "Design a chair: dining chair with tall backrest"  
        ]  
        
        print("\nğŸ¯ æµ‹è¯•æ¤…å­è®¾è®¡ç”Ÿæˆ:")  
        print("=" * 40)  
        
        for i, prompt in enumerate(test_prompts, 1):  
            print(f"\nğŸª‘ æµ‹è¯• {i}: {prompt}")  
            
            # æ„é€ è¾“å…¥  
            input_text = f"User: {prompt}\n\nAssistant:"  
            inputs = tokenizer(input_text, return_tensors="pt").to(device)  
            
            # ç”Ÿæˆå“åº”  
            with torch.no_grad():  
                outputs = model.generate(  
                    **inputs,  
                    max_new_tokens=150,  
                    temperature=0.7,  
                    do_sample=True,  
                    pad_token_id=tokenizer.pad_token_id,  
                    eos_token_id=tokenizer.eos_token_id  
                )  
            
            # è§£ç å“åº”  
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)  
            assistant_response = response.split("Assistant:")[-1].strip()  
            
            print(f"ğŸ”§ ç”Ÿæˆçš„Blenderä»£ç :")  
            print("-" * 30)  
            print(assistant_response[:300] + "..." if len(assistant_response) > 300 else assistant_response)  
            print("-" * 30)  
        
        print("\nâœ… LoRAæ¨¡å‹æµ‹è¯•å®Œæˆ!")  
        
    except Exception as e:  
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")  
        import traceback  
        traceback.print_exc()  

if __name__ == "__main__":  
    test_lora_model()  
