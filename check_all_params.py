#!/usr/bin/env python3  
"""  
è¯¦ç»†æ£€æŸ¥æ‰€æœ‰æ¨¡å‹å‚æ•°çš„çŠ¶æ€  
"""  

import torch  
import os  

def check_params_detailed(params_path, model_name):  
    """è¯¦ç»†æ£€æŸ¥å‚æ•°çŠ¶æ€"""  
    print(f"\nğŸ” æ£€æŸ¥ {model_name}")  
    print("-" * 40)  
    
    if not os.path.exists(params_path):  
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {params_path}")  
        return False  
    
    try:  
        params = torch.load(params_path, map_location='cpu')  
        
        print(f"ğŸ“ å‚æ•°æ–‡ä»¶: {params_path}")  
        print(f"ğŸ”¢ å‚æ•°æ•°é‡: {len(params)}")  
        
        all_good = True  
        
        for name, param in params.items():  
            has_nan = torch.isnan(param).any()  
            has_inf = torch.isinf(param).any()  
            
            # è®¡ç®—ä¸€äº›ç»Ÿè®¡ä¿¡æ¯  
            param_min = param.min().item()  
            param_max = param.max().item()  
            param_mean = param.mean().item()  
            param_std = param.std().item()  
            
            status = "âœ…" if not (has_nan or has_inf) else "âŒ"  
            print(f"{status} {name}:")  
            print(f"    å½¢çŠ¶: {param.shape}")  
            print(f"    NaN: {has_nan}, Inf: {has_inf}")  
            print(f"    èŒƒå›´: [{param_min:.6f}, {param_max:.6f}]")  
            print(f"    å‡å€¼: {param_mean:.6f}, æ ‡å‡†å·®: {param_std:.6f}")  
            
            if has_nan or has_inf:  
                all_good = False  
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å¤§çš„å€¼  
            if abs(param_max) > 1e10 or abs(param_min) > 1e10:  
                print(f"    âš ï¸ å­˜åœ¨å¼‚å¸¸å¤§çš„å€¼")  
                all_good = False  
        
        return all_good  
        
    except Exception as e:  
        print(f"âŒ æ£€æŸ¥å¤±è´¥: {e}")  
        return False  

def main():  
    """ä¸»å‡½æ•°"""  
    print("ğŸ”¬ æ¨¡å‹å‚æ•°å…¨é¢æ£€æŸ¥")  
    print("=" * 60)  
    
    # æ£€æŸ¥å„ç§æ¨¡å‹å‚æ•°  
    models_to_check = [  
        ("æŸåçš„åŸå§‹å¾®è°ƒ", "./output/emergency_model/trainable_params.pt"),  
        ("ä¿®å¤åçš„æ¨¡å‹", "./output/fixed_model/trainable_params.pt"),  
        ("å®‰å…¨æ¨¡å‹", "./output/safe_model/trainable_params.pt"),  
    ]  
    
    results = {}  
    
    for model_name, params_path in models_to_check:  
        is_good = check_params_detailed(params_path, model_name)  
        results[model_name] = is_good  
    
    # æ€»ç»“  
    print("\n" + "="*60)  
    print("ğŸ“Š æ£€æŸ¥ç»“æœæ€»ç»“")  
    print("="*60)  
    
    for model_name, is_good in results.items():  
        status = "âœ… æ­£å¸¸" if is_good else "âŒ æœ‰é—®é¢˜"  
        print(f"{model_name}: {status}")  
    
    # é¢å¤–æ£€æŸ¥ï¼šå¯¹æ¯”åŸå§‹æ¨¡å‹çš„æŸä¸ªå‚æ•°  
    print("\nğŸ” å¯¹æ¯”æ£€æŸ¥:")  
    try:  
        from transformers import AutoModelForCausalLM  
        print("åŠ è½½åŸå§‹æ¨¡å‹è¿›è¡Œå¯¹æ¯”...")  
        original_model = AutoModelForCausalLM.from_pretrained(  
            "../models/BlenderLLM",  
            trust_remote_code=True,  
            torch_dtype=torch.float16,  
            device_map="cpu"  
        )  
        
        # æ£€æŸ¥lm_head.weightå‚æ•°  
        original_lm_head = None  
        for name, param in original_model.named_parameters():  
            if name == "lm_head.weight":  
                original_lm_head = param.clone()  
                break  
        
        if original_lm_head is not None:  
            print(f"\nåŸå§‹ lm_head.weight:")  
            print(f"  å½¢çŠ¶: {original_lm_head.shape}")  
            print(f"  èŒƒå›´: [{original_lm_head.min():.6f}, {original_lm_head.max():.6f}]")  
            print(f"  å‡å€¼: {original_lm_head.mean():.6f}")  
            print(f"  NaN: {torch.isnan(original_lm_head).any()}")  
            print(f"  Inf: {torch.isinf(original_lm_head).any()}")  
        
        del original_model  
        
    except Exception as e:  
        print(f"âš ï¸ æ— æ³•åŠ è½½åŸå§‹æ¨¡å‹è¿›è¡Œå¯¹æ¯”: {e}")  

if __name__ == "__main__":  
    main()  
