#!/usr/bin/env python3  
"""  
ä½¿ç”¨LoRAå¾®è°ƒBlenderLLMæå‡æ¤…å­è®¾è®¡ç†è§£  
ä¿®å¤å¤šGPUè®¾å¤‡å†²çªé—®é¢˜  
"""  

import os  
import torch  
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig  
from peft import LoraConfig, get_peft_model, TaskType  
import json  
from torch.utils.data import Dataset  

class ChairDesignDataset(Dataset):  
    """æ¤…å­è®¾è®¡æ•°æ®é›†"""  
    def __init__(self, data, tokenizer, max_length=512):  
        self.data = data  
        self.tokenizer = tokenizer  
        self.max_length = max_length  
    
    def __len__(self):  
        return len(self.data)  
    
    def __getitem__(self, idx):  
        prompt, target = self.data[idx]  
        
        # æ„é€ å®Œæ•´æ–‡æœ¬  
        text = f"User: {prompt}\n\nAssistant: {target}"  
        
        # Tokenize  
        encoding = self.tokenizer(  
            text,  
            truncation=True,  
            padding='max_length',  
            max_length=self.max_length,  
            return_tensors='pt'  
        )  
        
        return {  
            'input_ids': encoding['input_ids'].flatten(),  
            'attention_mask': encoding['attention_mask'].flatten(),  
            'labels': encoding['input_ids'].flatten()  
        }  

# è‡ªå®šä¹‰Trainerç±» - æ”¾åœ¨è¿™é‡Œï¼  
class FixedTrainer(Trainer):  
    """ä¿®å¤è®¾å¤‡å†²çªçš„Trainer"""  
    def _prepare_inputs(self, inputs):  
        """ç¡®ä¿æ‰€æœ‰è¾“å…¥éƒ½åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š"""  
        for key, value in inputs.items():  
            if isinstance(value, torch.Tensor):  
                inputs[key] = value.to(self.model.device)  
        return inputs  

def load_chair_data_simple():  
    """åŠ è½½æ¤…å­æ•°æ®ï¼Œç®€åŒ–ç‰ˆæœ¬é¿å…å¤æ‚æ€§"""  
    print("ğŸ“Š åŠ è½½æ¤…å­æ•°æ®...")  
    
    data_dir = "./data_grouped"  
    chair_data = []  
    
    for folder_name in os.listdir(data_dir)[:10]:  # å‡å°‘åˆ°10ä¸ªæ ·æœ¬  
        folder_path = os.path.join(data_dir, folder_name)  
        if not os.path.isdir(folder_path):  
            continue  
        
        txt_file = os.path.join(folder_path, f"{folder_name}.txt")  
        
        if os.path.exists(txt_file):  
            try:  
                # è¯»å–æè¿°  
                with open(txt_file, 'r', encoding='utf-8') as f:  
                    description = f.read().strip()  
                
                # ç®€åŒ–çš„æç¤º  
                simplified_prompt = f"Design a chair: {description[:100]}"  
                
                # ç®€åŒ–çš„Blenderä»£ç æ¨¡æ¿  
                simple_code = generate_simple_blender_code(description)  
                
                chair_data.append((simplified_prompt, simple_code))  
                print(f"  âœ… åŠ è½½æ¤…å­: {folder_name[:8]}...")  
                
            except Exception as e:  
                print(f"  âš ï¸ è·³è¿‡ {folder_name}: {e}")  
                continue  
    
    print(f"ğŸ“ˆ æ€»å…±åŠ è½½äº† {len(chair_data)} ä¸ªæ¤…å­æ ·æœ¬")  
    return chair_data  

def generate_simple_blender_code(description):  
    """ç”Ÿæˆç®€åŒ–çš„Blenderä»£ç """  
    
    # æ£€æµ‹å…³é”®ç‰¹å¾  
    has_armrest = 'armrest' in description.lower()  
    is_office_chair = 'office' in description.lower()  
    is_minimalist = 'minimalist' in description.lower()  
    
    code = '''import bpy  

# Clear existing objects  
bpy.ops.object.select_all(action='DESELECT')  
bpy.ops.object.select_by_type(type='MESH')  
bpy.ops.object.delete()  

# Create seat  
bpy.ops.mesh.primitive_cube_add(size=2, location=(0, 0, 0.5))  
seat = bpy.context.active_object  
seat.name = "Seat"  
'''  
    
    if is_minimalist:  
        code += 'seat.scale = (0.4, 0.35, 0.02)\n'  
    else:  
        code += 'seat.scale = (0.45, 0.4, 0.05)\n'  
    
    # æ¤…èƒŒ  
    code += '''  
# Create backrest  
bpy.ops.mesh.primitive_cube_add(size=2, location=(0, -0.35, 0.85))  
backrest = bpy.context.active_object  
backrest.name = "Backrest"  
backrest.scale = (0.4, 0.04, 0.35)  
'''  
    
    # æ¤…è…¿  
    if is_office_chair:  
        code += '''  
# Office chair base  
bpy.ops.mesh.primitive_cylinder_add(radius=0.3, depth=0.05, location=(0, 0, 0.05))  
base = bpy.context.active_object  
base.name = "Base"  
'''  
    else:  
        code += '''  
# Four legs  
leg_positions = [(-0.35, -0.3, 0.25), (0.35, -0.3, 0.25), (-0.35, 0.3, 0.25), (0.35, 0.3, 0.25)]  
for i, pos in enumerate(leg_positions):  
    bpy.ops.mesh.primitive_cylinder_add(radius=0.03, depth=0.5, location=pos)  
    leg = bpy.context.active_object  
    leg.name = f"Leg_{i+1}"  
'''  
    
    return code  

def main():  
    """ä¸»è®­ç»ƒå‡½æ•° - LoRAç‰ˆæœ¬ï¼Œä¿®å¤è®¾å¤‡é—®é¢˜"""  
    print("ğŸ¯ LoRAå¾®è°ƒBlenderLLMæ¤…å­è®¾è®¡ç†è§£")  
    print("=" * 50)  
    
    # è®¾ç½®è®¾å¤‡å’Œç¯å¢ƒ - è¿™äº›æ˜¯Pythonä»£ç ï¼Œä¸æ˜¯å‘½ä»¤è¡Œï¼  
    print("ğŸ”§ è®¾ç½®è®­ç»ƒç¯å¢ƒ...")  
    
    # å¼ºåˆ¶ä½¿ç”¨å•GPU (è¿™æ˜¯Pythonä»£ç ä¸­çš„è®¾ç½®)  
    os.environ["CUDA_VISIBLE_DEVICES"] = "0"  
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")  
    print(f"ğŸ¯ ä½¿ç”¨è®¾å¤‡: {device}")  
    
    # 1. åŠ è½½æ•°æ®  
    chair_data = load_chair_data_simple()  
    if len(chair_data) == 0:  
        print("âŒ æ²¡æœ‰åŠ è½½åˆ°æ•°æ®")  
        return  
    
    print(f"ğŸ“Š è®­ç»ƒæ ·æœ¬æ•°é‡: {len(chair_data)}")  
    
    # æ˜¾ç¤ºæ ·æœ¬  
    print("\nğŸ“ æ ·æœ¬é¢„è§ˆ:")  
    for i, (prompt, code) in enumerate(chair_data[:2]):  
        print(f"\næ ·æœ¬ {i+1}:")  
        print(f"æç¤º: {prompt}")  
        print(f"ä»£ç : {code[:100]}...")  
    
    # 2. é…ç½®é‡åŒ–  
    print("\nğŸ”§ é…ç½®é‡åŒ–...")  
    quantization_config = BitsAndBytesConfig(  
        load_in_8bit=True,  
        llm_int8_threshold=6.0,  
        llm_int8_has_fp16_weight=False,  
    )  
    
    # 3. åŠ è½½æ¨¡å‹å’Œtokenizer  
    print("\nğŸ”„ åŠ è½½BlenderLLM...")  
    try:  
        tokenizer = AutoTokenizer.from_pretrained("../models/BlenderLLM", trust_remote_code=True)  
        if tokenizer.pad_token is None:  
            tokenizer.pad_token = tokenizer.eos_token  
        
        # ä½¿ç”¨å•GPUåŠ è½½ (device_mapè®¾ç½®æ‰€æœ‰å±‚éƒ½åœ¨GPU 0ä¸Š)  
        model = AutoModelForCausalLM.from_pretrained(  
            "../models/BlenderLLM",  
            trust_remote_code=True,  
            quantization_config=quantization_config,  
            torch_dtype=torch.float16,  
            device_map={"": 0},  
        )  
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")  
        
    except Exception as e:  
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")  
        return  
    
    # 4. é…ç½®LoRA  
    print("\nğŸ”§ é…ç½®LoRA...")  
    lora_config = LoraConfig(  
        task_type=TaskType.CAUSAL_LM,  
        r=4,  # è¿›ä¸€æ­¥å‡å°rank  
        lora_alpha=8,  # è¿›ä¸€æ­¥å‡å°alpha  
        lora_dropout=0.1,  
        target_modules=["q_proj", "v_proj"],  
        bias="none"  
    )  
    
    # åº”ç”¨LoRA  
    model = get_peft_model(model, lora_config)  
    model.print_trainable_parameters()  
    
    # 5. å‡†å¤‡æ•°æ®é›†  
    print("\nğŸ“Š å‡†å¤‡æ•°æ®é›†...")  
    dataset = ChairDesignDataset(chair_data, tokenizer, max_length=128)  # è¿›ä¸€æ­¥å‡å°  
    
    # 6. é…ç½®è®­ç»ƒå‚æ•°  
    print("\nâš™ï¸ é…ç½®è®­ç»ƒå‚æ•°...")  
    training_args = TrainingArguments(  
        output_dir="./output/lora_blender_checkpoints",  
        num_train_epochs=1,  
        per_device_train_batch_size=1,  
        gradient_accumulation_steps=1,  # å‡å°‘æ¢¯åº¦ç´¯ç§¯  
        warmup_steps=2,  
        learning_rate=5e-5,  # è¿›ä¸€æ­¥é™ä½å­¦ä¹ ç‡  
        fp16=True,  
        logging_steps=1,  
        save_steps=50,  
        save_total_limit=1,  
        remove_unused_columns=False,  
        dataloader_pin_memory=False,  
        gradient_checkpointing=False,  
        dataloader_num_workers=0,  
        report_to=None,  
    )  
    
    # 7. åˆ›å»ºTrainer (ä½¿ç”¨ä¸Šé¢å®šä¹‰çš„FixedTrainer)  
    print("\nğŸ‹ï¸ åˆ›å»ºTrainer...")  
    trainer = FixedTrainer(  
        model=model,  
        args=training_args,  
        train_dataset=dataset,  
        tokenizer=tokenizer,  # å…ˆç”¨è¿™ä¸ªï¼Œçœ‹æ˜¯å¦è¿˜æŠ¥é”™  
    )  
    
    # 8. å¼€å§‹è®­ç»ƒ  
    print("\nğŸš€ å¼€å§‹LoRAå¾®è°ƒ...")  
    try:  
        trainer.train()  
        print("âœ… è®­ç»ƒå®Œæˆ!")  
        
        # 9. ä¿å­˜LoRAé€‚é…å™¨  
        print("\nğŸ’¾ ä¿å­˜LoRAæ¨¡å‹...")  
        output_dir = "./output/lora_blender_enhanced"  
        os.makedirs(output_dir, exist_ok=True)  
        
        # ä¿å­˜LoRAé€‚é…å™¨  
        model.save_pretrained(output_dir)  
        tokenizer.save_pretrained(output_dir)  
        
        # ä¿å­˜è®­ç»ƒä¿¡æ¯  
        training_info = {  
            "model_type": "BlenderLLM_LoRA",  
            "base_model": "../models/BlenderLLM",  
            "lora_config": {  
                "r": lora_config.r,  
                "lora_alpha": lora_config.lora_alpha,  
                "target_modules": lora_config.target_modules,  
                "lora_dropout": lora_config.lora_dropout  
            },  
            "training_samples": len(chair_data),  
            "epochs": 1,  
            "device": str(device),  
            "enhanced_features": [  
                "Chair design understanding",  
                "Style and feature recognition",   
                "Improved Blender code generation"  
            ]  
        }  
        
        with open(os.path.join(output_dir, "training_info.json"), 'w') as f:  
            json.dump(training_info, f, indent=2)  
        
        print(f"âœ… LoRAæ¨¡å‹ä¿å­˜æˆåŠŸ: {output_dir}")  
        
        # 10. ç®€å•æµ‹è¯•  
        print("\nğŸ§ª æµ‹è¯•LoRAæ¨¡å‹...")  
        model.eval()  
        
        test_prompt = "Design a chair: modern minimalist chair"  
        inputs = tokenizer(f"User: {test_prompt}\n\nAssistant:", return_tensors="pt")  
        # ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®è®¾å¤‡ä¸Š  
        inputs = {k: v.to(device) for k, v in inputs.items()}  
        
        with torch.no_grad():  
            outputs = model.generate(  
                **inputs,  
                max_new_tokens=50,  
                temperature=0.8,  
                do_sample=True,  
                pad_token_id=tokenizer.pad_token_id  
            )  
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)  
        assistant_response = response.split("Assistant:")[-1].strip()  
        
        print(f"ğŸ¯ æµ‹è¯•æç¤º: {test_prompt}")  
        print(f"ğŸ”§ ç”Ÿæˆä»£ç : {assistant_response[:150]}...")  
        
    except Exception as e:  
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")  
        import traceback  
        traceback.print_exc()  
    
    print("\nâœ… LoRAå¾®è°ƒå®Œæˆï¼")  

if __name__ == "__main__":  
    main()  
