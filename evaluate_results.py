# evaluate_results.py  
#!/usr/bin/env python3  
"""  
è¯„ä¼°æ‰¹é‡ç”Ÿæˆç»“æœ  
"""  

import os  
import sys  
import json  
import argparse  
from pathlib import Path  
import matplotlib.pyplot as plt  
import pandas as pd  
import numpy as np  

def main():  
    parser = argparse.ArgumentParser(description='Evaluate Batch Generation Results')  
    parser.add_argument('--batch_results_dir', type=str, required=True,  
                       help='Directory containing batch generation results')  
    parser.add_argument('--output_dir', type=str, required=True,  
                       help='Output directory for evaluation results')  
    
    args = parser.parse_args()  
    
    batch_dir = Path(args.batch_results_dir)  
    output_dir = Path(args.output_dir)  
    output_dir.mkdir(parents=True, exist_ok=True)  
    
    if not batch_dir.exists():  
        print(f"Error: Batch results directory does not exist: {batch_dir}")  
        return  
    
    print(f"Evaluating results from: {batch_dir}")  
    print(f"Output directory: {output_dir}")  
    
    # åŠ è½½æ‰¹é‡å¤„ç†æ‘˜è¦  
    summary_file = batch_dir / "batch_summary.json"  
    if summary_file.exists():  
        with open(summary_file) as f:  
            batch_summary = json.load(f)  
    else:  
        print("Warning: No batch_summary.json found")  
        batch_summary = {}  
    
    # æ”¶é›†æ‰€æœ‰ç»“æœ  
    results = collect_all_results(batch_dir)  
    
    if not results:  
        print("No results found to evaluate")  
        return  
    
    # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š  
    evaluation_report = generate_evaluation_report(results, batch_summary)  
    
    # ä¿å­˜è¯„ä¼°ç»“æœ  
    with open(output_dir / "evaluation_report.json", 'w') as f:  
        json.dump(evaluation_report, f, indent=2, default=str)  
    
    # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨  
    generate_visualizations(results, evaluation_report, output_dir)  
    
    # ç”ŸæˆMarkdownæŠ¥å‘Š  
    generate_markdown_report(evaluation_report, output_dir)  
    
    print("âœ… Evaluation completed successfully!")  
    print(f"Report saved to: {output_dir}")  

def collect_all_results(batch_dir):  
    """æ”¶é›†æ‰€æœ‰ç”Ÿæˆç»“æœ"""  
    results = []  
    
    # éå†æ‰€æœ‰style_*ç›®å½•  
    for style_dir in batch_dir.glob("style_*"):  
        if style_dir.is_dir():  
            result_file = style_dir / "generation_result.json"  
            if result_file.exists():  
                try:  
                    with open(result_file) as f:  
                        result = json.load(f)  
                    
                    # æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶æ˜¯å¦å­˜åœ¨  
                    result['file_exists'] = check_generated_files(style_dir, result)  
                    results.append(result)  
                    
                except Exception as e:  
                    print(f"Error loading result from {style_dir}: {e}")  
    
    return results  

def check_generated_files(style_dir, result):  
    """æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""  
    if result['status'] != 'success' or not result.get('generated_views'):  
        return False  
    
    for view_name, view_path in result['generated_views'].items():  
        if not Path(view_path).exists():  
            return False  
    
    return True  

def generate_evaluation_report(results, batch_summary):  
    """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""  
    total_results = len(results)  
    successful_results = [r for r in results if r['status'] == 'success']  
    failed_results = [r for r in results if r['status'] == 'failed']  
    
    # åŸºç¡€ç»Ÿè®¡  
    success_count = len(successful_results)  
    failure_count = len(failed_results)  
    success_rate = success_count / total_results if total_results > 0 else 0  
    
    # æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥  
    files_exist_count = len([r for r in successful_results if r.get('file_exists', False)])  
    file_existence_rate = files_exist_count / success_count if success_count > 0 else 0  
    
    # åˆ†æå¤±è´¥åŸå›   
    failure_reasons = {}  
    for result in failed_results:  
        error = result.get('error', 'Unknown error')  
        # ç®€åŒ–é”™è¯¯ä¿¡æ¯  
        simplified_error = simplify_error_message(error)  
        failure_reasons[simplified_error] = failure_reasons.get(simplified_error, 0) + 1  
    
    # åˆ†æé£æ ¼ç±»å‹åˆ†å¸ƒ  
    style_analysis = analyze_style_distribution(results)  
    
    # æ€§èƒ½åˆ†æ  
    performance_analysis = analyze_performance(results)  
    
    evaluation_report = {  
        'summary': {  
            'total_processed': total_results,  
            'successful': success_count,  
            'failed': failure_count,  
            'success_rate': success_rate,  
            'files_exist_count': files_exist_count,  
            'file_existence_rate': file_existence_rate  
        },  
        'failure_analysis': {  
            'reasons': failure_reasons,  
            'most_common_failure': max(failure_reasons.items(), key=lambda x: x[1])[0] if failure_reasons else None  
        },  
        'style_analysis': style_analysis,  
        'performance_analysis': performance_analysis,  
        'batch_summary': batch_summary  
    }  
    
    return evaluation_report  

def simplify_error_message(error_msg):  
    """ç®€åŒ–é”™è¯¯ä¿¡æ¯"""  
    error_msg = str(error_msg).lower()  
    
    if 'cuda' in error_msg or 'gpu' in error_msg:  
        return 'GPU/CUDA Error'  
    elif 'memory' in error_msg or 'oom' in error_msg:  
        return 'Memory Error'  
    elif 'model' in error_msg and ('load' in error_msg or 'not found' in error_msg):  
        return 'Model Loading Error'  
    elif 'timeout' in error_msg:  
        return 'Timeout Error'  
    elif 'blender' in error_msg:  
        return 'Blender Execution Error'  
    elif 'generation' in error_msg:  
        return 'Generation Error'  
    else:  
        return 'Other Error'  

def analyze_style_distribution(results):  
    """åˆ†æé£æ ¼åˆ†å¸ƒ"""  
    style_stats = {  
        'by_status': {'success': {}, 'failed': {}},  
        'total_styles': {}  
    }  
    
    for result in results:  
        status = result['status']  
        style_desc = result.get('style_description', 'Unknown')  
        
        # æå–é£æ ¼å…³é”®è¯  
        style_keywords = extract_style_keywords(style_desc)  
        
        for keyword in style_keywords:  
            # æŒ‰çŠ¶æ€ç»Ÿè®¡  
            if keyword not in style_stats['by_status'][status]:  
                style_stats['by_status'][status][keyword] = 0  
            style_stats['by_status'][status][keyword] += 1  
            
            # æ€»ä½“ç»Ÿè®¡  
            if keyword not in style_stats['total_styles']:  
                style_stats['total_styles'][keyword] = 0  
            style_stats['total_styles'][keyword] += 1  
    
    return style_stats  

def extract_style_keywords(style_description):  
    """ä»é£æ ¼æè¿°ä¸­æå–å…³é”®è¯"""  
    keywords = []  
    desc = str(style_description).lower()  
    
    # é£æ ¼å…³é”®è¯æ˜ å°„  
    style_keywords = {  
        'ç°ä»£': ['ç°ä»£', 'modern', 'æç®€', 'minimalist'],  
        'ä¼ ç»Ÿ': ['ä¼ ç»Ÿ', 'traditional', 'å¤å…¸', 'classical'],  
        'å·¥ä¸š': ['å·¥ä¸š', 'industrial', 'é‡‘å±'],  
        'åŠå…¬': ['åŠå…¬', 'office', 'å¯è°ƒèŠ‚'],  
        'å®æœ¨': ['å®æœ¨', 'wood', 'æœ¨è´¨'],  
        'äººä½“å·¥å­¦': ['äººä½“å·¥å­¦', 'ergonomic'],  
        'å¯æŠ˜å ': ['æŠ˜å ', 'foldable'],  
        'å¯è°ƒèŠ‚': ['å¯è°ƒèŠ‚', 'adjustable']  
    }  
    
    for category, terms in style_keywords.items():  
        if any(term in desc for term in terms):  
            keywords.append(category)  
    
    return keywords if keywords else ['å…¶ä»–']  

def analyze_performance(results):  
    """åˆ†ææ€§èƒ½æ•°æ®"""  
    successful_results = [r for r in results if r['status'] == 'success']  
    
    if not successful_results:  
        return {'message': 'No successful results for performance analysis'}  
    
    # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šæ€§èƒ½åˆ†æ  
    # æ¯”å¦‚ç”Ÿæˆæ—¶é—´ã€æ–‡ä»¶å¤§å°ç­‰ï¼ˆå¦‚æœæœ‰è¿™äº›æ•°æ®ï¼‰  
    
    performance_data = {  
        'total_successful': len(successful_results),  
        'average_success_rate': len(successful_results) / len(results) if results else 0  
    }  
    
    return performance_data  

def generate_visualizations(results, evaluation_report, output_dir):  
    """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""  
    import matplotlib  
    matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯  
    
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei']  
    plt.rcParams['axes.unicode_minus'] = False  
    
    # 1. æˆåŠŸç‡é¥¼å›¾  
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))  
    
    # æˆåŠŸ/å¤±è´¥åˆ†å¸ƒ  
    success_data = [  
        evaluation_report['summary']['successful'],  
        evaluation_report['summary']['failed']  
    ]  
    labels = ['Successful', 'Failed']  
    colors = ['#28a745', '#dc3545']  
    
    ax1.pie(success_data, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)  
    ax1.set_title('Generation Success Rate')  
    
    # å¤±è´¥åŸå› åˆ†å¸ƒ  
    if evaluation_report['failure_analysis']['reasons']:  
        failure_reasons = evaluation_report['failure_analysis']['reasons']  
        ax2.bar(range(len(failure_reasons)), list(failure_reasons.values()))  
        ax2.set_xticks(range(len(failure_reasons)))  
        ax2.set_xticklabels(list(failure_reasons.keys()), rotation=45, ha='right')  
        ax2.set_title('Failure Reasons Distribution')  
        ax2.set_ylabel('Count')  
    else:  
        ax2.text(0.5, 0.5, 'No failures to analyze', ha='center', va='center', transform=ax2.transAxes)  
        ax2.set_title('Failure Reasons Distribution')  
    
    plt.tight_layout()  
    plt.savefig(output_dir / 'success_failure_analysis.png', dpi=300, bbox_inches='tight')  
    plt.close()  
    
    # 2. é£æ ¼åˆ†å¸ƒå›¾  
    if evaluation_report['style_analysis']['total_styles']:  
        fig, ax = plt.subplots(figsize=(12, 8))  
        
        style_data = evaluation_report['style_analysis']['total_styles']  
        styles = list(style_data.keys())  
        counts = list(style_data.values())  
        
        bars = ax.bar(styles, counts, color='skyblue')  
        ax.set_title('Style Distribution')  
        ax.set_ylabel('Count')  
        ax.set_xlabel('Style Categories')  
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾  
        for bar in bars:  
            height = bar.get_height()  
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.1,  
                   f'{int(height)}', ha='center', va='bottom')  
        
        plt.xticks(rotation=45, ha='right')  
        plt.tight_layout()  
        plt.savefig(output_dir / 'style_distribution.png', dpi=300, bbox_inches='tight')  
        plt.close()  

def generate_markdown_report(evaluation_report, output_dir):  
    """ç”ŸæˆMarkdownæ ¼å¼çš„è¯„ä¼°æŠ¥å‘Š"""  
    report_file = output_dir / "evaluation_report.md"  
    
    with open(report_file, 'w', encoding='utf-8') as f:  
        f.write("# Chair Style Generation Evaluation Report\n\n")  
        f.write(f"**Generated on:** {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")  
        
        # æ¦‚è¦ç»Ÿè®¡  
        summary = evaluation_report['summary']  
        f.write("## Summary Statistics\n\n")  
        f.write(f"- **Total Processed:** {summary['total_processed']}\n")  
        f.write(f"- **Successful:** {summary['successful']}\n")  
        f.write(f"- **Failed:** {summary['failed']}\n")  
        f.write(f"- **Success Rate:** {summary['success_rate']:.1%}\n")  
        f.write(f"- **Files Generated Successfully:** {summary['files_exist_count']}\n")  
        f.write(f"- **File Generation Rate:** {summary['file_existence_rate']:.1%}\n\n")  
        
        # å¤±è´¥åˆ†æ  
        f.write("## Failure Analysis\n\n")  
        failure_analysis = evaluation_report['failure_analysis']  
        
        if failure_analysis['reasons']:  
            f.write("### Failure Reasons Distribution\n\n")  
            for reason, count in sorted(failure_analysis['reasons'].items(), key=lambda x: x[1], reverse=True):  
                f.write(f"- **{reason}:** {count} cases\n")  
            
            f.write(f"\n**Most Common Failure:** {failure_analysis['most_common_failure']}\n\n")  
        else:  
            f.write("No failures detected! ğŸ‰\n\n")  
        
        # é£æ ¼åˆ†æ  
        f.write("## Style Analysis\n\n")  
        style_analysis = evaluation_report['style_analysis']  
        
        if style_analysis['total_styles']:  
            f.write("### Style Distribution\n\n")  
            f.write("| Style Category | Total Count | Success Rate |\n")  
            f.write("|----------------|-------------|-------------|\n")  
            
            for style in style_analysis['total_styles']:  
                total = style_analysis['total_styles'][style]  
                success = style_analysis['by_status']['success'].get(style, 0)  
                success_rate = success / total if total > 0 else 0  
                f.write(f"| {style} | {total} | {success_rate:.1%} |\n")  
        
        # æ€§èƒ½åˆ†æ  
        f.write("\n## Performance Analysis\n\n")  
        perf = evaluation_report['performance_analysis']  
        
        if isinstance(perf, dict) and 'message' not in perf:  
            f.write(f"- **Successful Generations:** {perf.get('total_successful', 0)}\n")  
            f.write(f"- **Overall Success Rate:** {perf.get('average_success_rate', 0):.1%}\n")  
        else:  
            f.write("Performance data not available.\n")  
        
        # å¯è§†åŒ–å›¾è¡¨  
        f.write("\n## Visualizations\n\n")  
        f.write("![Success/Failure Analysis](success_failure_analysis.png)\n\n")  
        f.write("![Style Distribution](style_distribution.png)\n\n")  
        
        # å»ºè®®å’Œç»“è®º  
        f.write("## Recommendations\n\n")  
        
        if summary['success_rate'] < 0.8:  
            f.write("- ğŸ“ˆ **Improve Success Rate:** Current success rate is below 80%. Consider:\n")  
            f.write("  - Checking model configuration\n")  
            f.write("  - Optimizing input preprocessing\n")  
            f.write("  - Reviewing error patterns\n\n")  
        
        if summary['file_existence_rate'] < summary['success_rate']:  
            f.write("- ğŸ’¾ **File Generation Issues:** Some successful generations didn't produce output files.\n")  
            f.write("  - Check file writing permissions\n")  
            f.write("  - Verify Blender script execution\n\n")  
        
        if failure_analysis.get('most_common_failure'):  
            f.write(f"- ğŸ”§ **Address Common Failures:** Focus on resolving '{failure_analysis['most_common_failure']}' errors.\n\n")  
        
        f.write("## Conclusion\n\n")  
        
        if summary['success_rate'] >= 0.9:  
            f.write("ğŸ‰ **Excellent Performance!** The model is generating high-quality results with minimal failures.\n")  
        elif summary['success_rate'] >= 0.7:  
            f.write("âœ… **Good Performance:** The model is working well with room for minor improvements.\n")  
        elif summary['success_rate'] >= 0.5:  
            f.write("âš ï¸ **Moderate Performance:** The model shows potential but needs optimization.\n")  
        else:  
            f.write("âŒ **Needs Improvement:** Significant issues detected that require attention.\n")  
    
    print(f"ğŸ“„ Markdown report generated: {report_file}")  

if __name__ == "__main__":  
    main()