#!/usr/bin/env python3  
"""  
ä¿®å¤ç‰ˆæœ€å°åŒ–è®­ç»ƒ  
"""  

import torch  
import json  
from transformers import AutoTokenizer, AutoModelForCausalLM  
import os  
import numpy as np  

# å¼ºåˆ¶å•GPU  
os.environ["CUDA_VISIBLE_DEVICES"] = "0"  
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"  

def fixed_minimal_train():  
    print("ğŸª‘ Fixed Minimal Chair Training...")  
    
    # æ¸…ç†å†…å­˜  
    torch.cuda.empty_cache()  
    
    # åŠ è½½æ¨¡å‹  
    print("Loading model...")  
    tokenizer = AutoTokenizer.from_pretrained("../models/BlenderLLM", trust_remote_code=True)  
    
    model = AutoModelForCausalLM.from_pretrained(  
        "../models/BlenderLLM",  
        trust_remote_code=True,  
        torch_dtype=torch.float16,  
        device_map="auto",  
        low_cpu_mem_usage=True  
    )  
    
    if tokenizer.pad_token is None:  
        tokenizer.pad_token = tokenizer.eos_token  
        tokenizer.pad_token_id = tokenizer.eos_token_id  
    
    # å†»ç»“å¤§éƒ¨åˆ†å‚æ•°  
    total_params = 0  
    trainable_params = 0  
    
    for name, param in model.named_parameters():  
        total_params += param.numel()  
        param.requires_grad = False  
    
    # åªè§£å†»æœ€åå‡ å±‚å’Œè¾“å‡ºå±‚  
    for name, param in model.named_parameters():  
        if any(layer in name for layer in ['layers.31', 'layers.30', 'lm_head', 'embed_tokens']):  
            param.requires_grad = True  
            trainable_params += param.numel()  
            print(f"âœ… Training: {name}")  
    
    print(f"ğŸ“Š Total params: {total_params:,}")  
    print(f"ğŸ¯ Trainable params: {trainable_params:,} ({100*trainable_params/total_params:.2f}%)")  
    
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹  
    model.gradient_checkpointing_enable()  
    
    # åŠ è½½å’Œæ¸…ç†æ•°æ®  
    with open('./output/cleaned_data/cleaned_data.json', 'r') as f:  
        data = json.load(f)  
    
    # è¿‡æ»¤å’Œæ¸…ç†æ•°æ®  
    clean_data = []  
    for item in data:  
        input_text = item.get('input', '').strip()  
        output_text = item.get('output', '').strip()  
        
        # è·³è¿‡ç©ºæ•°æ®  
        if not input_text or not output_text:  
            continue  
        
        # é™åˆ¶é•¿åº¦  
        if len(input_text) > 200:  
            input_text = input_text[:200]  
        if len(output_text) > 300:  
            output_text = output_text[:300]  
        
        clean_data.append({  
            'input': input_text,  
            'output': output_text  
        })  
    
    # åªç”¨å‰30ä¸ªæ ·æœ¬è¿›è¡Œå¿«é€Ÿæµ‹è¯•  
    clean_data = clean_data[:30]  
    print(f"Using {len(clean_data)} clean samples")  
    
    # æ˜¾ç¤ºæ ·æœ¬  
    if clean_data:  
        sample = clean_data[0]  
        print(f"ï¿½ï¿½ Sample input: {sample['input'][:80]}...")  
        print(f"ğŸ“ Sample output: {sample['output'][:80]}...")  
    
    # ä¼˜åŒ–å™¨è®¾ç½®  
    optimizer = torch.optim.AdamW(  
        [p for p in model.parameters() if p.requires_grad],   
        lr=1e-6,  # æ›´å°çš„å­¦ä¹ ç‡  
        weight_decay=0.01,  
        eps=1e-8  
    )  
    
    model.train()  
    
    print("ğŸ‹ï¸ Starting training...")  
    
    for epoch in range(1):  
        total_loss = 0  
        valid_steps = 0  
        
        for i, item in enumerate(clean_data):  
            if i % 5 == 0:  
                print(f"Step {i}/{len(clean_data)}")  
                torch.cuda.empty_cache()  
            
            # æ„å»ºè®­ç»ƒæ–‡æœ¬  
            input_text = item['input']  
            output_text = item['output']  
            full_text = f"User: {input_text}\nAssistant: {output_text}"  
            
            # Tokenize  
            inputs = tokenizer(  
                full_text,   
                return_tensors="pt",   
                max_length=256,  # å¢åŠ é•¿åº¦  
                truncation=True,   
                padding=True  
            )  
            inputs = {k: v.to(model.device) for k, v in inputs.items()}  
            
            # å‰å‘ä¼ æ’­  
            try:  
                outputs = model(**inputs, labels=inputs['input_ids'])  
                loss = outputs.loss  
                
                # æ£€æŸ¥æŸå¤±æ˜¯å¦æœ‰æ•ˆ  
                if torch.isnan(loss) or torch.isinf(loss):  
                    print(f"âš ï¸  Skipping step {i} due to invalid loss: {loss}")  
                    continue  
                
                # æ¢¯åº¦è£å‰ªå‰çš„åå‘ä¼ æ’­  
                loss.backward()  
                
                # æ¢¯åº¦è£å‰ª  
                torch.nn.utils.clip_grad_norm_(  
                    [p for p in model.parameters() if p.requires_grad],   
                    max_norm=1.0  
                )  
                
                if (i + 1) % 2 == 0:  # æ¯2æ­¥æ›´æ–°ä¸€æ¬¡  
                    optimizer.step()  
                    optimizer.zero_grad()  
                
                total_loss += loss.item()  
                valid_steps += 1  
                
            except Exception as e:  
                print(f"âš ï¸  Error at step {i}: {e}")  
                optimizer.zero_grad()  
                continue  
        
        avg_loss = total_loss / max(valid_steps, 1)  
        print(f"Epoch completed. Average loss: {avg_loss:.4f} (valid steps: {valid_steps})")  
    
    # ä¿å­˜æ¨¡å‹  
    print("ğŸ’¾ Saving model...")  
    os.makedirs("./output/fixed_minimal_model", exist_ok=True)  
    model.save_pretrained("./output/fixed_minimal_model")  
    tokenizer.save_pretrained("./output/fixed_minimal_model")  
    
    # æµ‹è¯•ç”Ÿæˆ  
    print("ğŸ§ª Testing generation...")  
    model.eval()  
    
    test_prompts = [  
        "User: Generate chair design: modern minimalist\nAssistant:",  
        "User: Create a comfortable office chair\nAssistant:",  
        "User: Design a vintage wooden chair\nAssistant:"  
    ]  
    
    for prompt in test_prompts:  
        try:  
            inputs = tokenizer(prompt, return_tensors="pt")  
            inputs = {k: v.to(model.device) for k, v in inputs.items()}  
            
            with torch.no_grad():  
                outputs = model.generate(  
                    **inputs,  
                    max_length=len(inputs['input_ids'][0]) + 100,  
                    temperature=0.7,  
                    do_sample=True,  
                    pad_token_id=tokenizer.pad_token_id,  
                    eos_token_id=tokenizer.eos_token_id,  
                    repetition_penalty=1.1  
                )  
            
            result = tokenizer.decode(outputs[0], skip_special_tokens=True)  
            generated = result[len(prompt):].strip()  
            print(f"ğŸ“ Prompt: {prompt.split('Assistant:')[0]}...")  
            print(f"   Generated: {generated[:100]}...")  
            print()  
            
        except Exception as e:  
            print(f"âš ï¸  Generation failed for prompt: {e}")  
    
    print("âœ… Fixed minimal training completed!")  

if __name__ == "__main__":  
    fixed_minimal_train()  
