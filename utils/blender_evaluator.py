"""  
Blenderå›¾åƒæ¸²æŸ“å’Œè¯„ä¼°å·¥å…·  
æ•´åˆæ¸²æŸ“å’Œè´¨é‡è¯„ä¼°åŠŸèƒ½  
"""  

import os  
import sys  
sys.path.append('../scripts')  
from scripts.blender_runner import run_blender_script, get_default_blender_path  
from .image_evaluation import ImageQualityEvaluator  

class BlenderImageEvaluator:  
    def __init__(self, blender_executable=None, config_path="../config/default.json"):  
        """  
        åˆå§‹åŒ–Blenderå›¾åƒè¯„ä¼°å™¨  
        
        Args:  
            blender_executable: Blenderå¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„  
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„  
        """  
        # è¯»å–é…ç½®  
        try:  
            import json  
            with open(config_path, 'r') as f:  
                config = json.load(f)  
            self.config = config  
        except:  
            self.config = {}  
        
        # è®¾ç½®Blenderè·¯å¾„  
        if blender_executable:  
            self.blender_executable = blender_executable  
        else:  
            self.blender_executable = self.config.get('blender_config', {}).get('executable_path', get_default_blender_path())  
        
        # è®¾ç½®è¾“å‡ºç›®å½•  
        self.output_folder = self.config.get('blender_config', {}).get('render_output', "./output/evaluation_renders")  
        os.makedirs(self.output_folder, exist_ok=True)  
        
        # åˆå§‹åŒ–å›¾åƒè¯„ä¼°å™¨  
        api_key = self.config.get('evaluation_config', {}).get('openai_api_key')  
        self.image_evaluator = ImageQualityEvaluator(api_key)  
        
        print(f"ðŸ”§ Blenderè·¯å¾„: {self.blender_executable}")  
        print(f"ðŸ“ è¾“å‡ºç›®å½•: {self.output_folder}")  
    
    def run_blender_script_and_render(self, script: str, obj_name: str) -> str:  
        """  
        è¿è¡ŒBlenderè„šæœ¬å¹¶æ¸²æŸ“å›¾åƒ  
        
        Args:  
            script: Blender Pythonè„šæœ¬  
            obj_name: å¯¹è±¡åç§°  
            
        Returns:  
            str: æ¸²æŸ“å›¾åƒè·¯å¾„ï¼Œå¤±è´¥æ—¶è¿”å›žNone  
        """  
        try:  
            # èŽ·å–æ¸²æŸ“åˆ†è¾¨çŽ‡  
            resolution = self.config.get('evaluation_config', {}).get('render_resolution', [512, 512])  
            
            # è¿è¡ŒBlenderè„šæœ¬  
            success = run_blender_script(  
                script=script,  
                obj_name=obj_name,  
                output_folder=self.output_folder,  
                blender_executable=self.blender_executable,  
                save_obj=False,  
                save_image=True,  
                resolution=tuple(resolution)  
            )  
            
            if success:  
                image_path = os.path.join(self.output_folder, f"{obj_name}.png")  
                if os.path.exists(image_path):  
                    return image_path  
                else:  
                    print(f"âŒ å›¾åƒæ–‡ä»¶æœªæ‰¾åˆ°: {image_path}")  
                    return None  
            else:  
                print(f"âŒ Blenderè„šæœ¬æ‰§è¡Œå¤±è´¥: {obj_name}")  
                return None  
                
        except Exception as e:  
            print(f"âŒ æ¸²æŸ“è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}")  
            return None  
    
    def evaluate_image_with_openai(self, image_path: str, prompt: str) -> dict:  
        """  
        ä½¿ç”¨OpenAIè¯„ä¼°å›¾åƒè´¨é‡  
        
        Args:  
            image_path: å›¾åƒæ–‡ä»¶è·¯å¾„  
            prompt: åŽŸå§‹è®¾è®¡æç¤º  
            
        Returns:  
            dict: è¯„ä¼°ç»“æžœ  
        """  
        return self.image_evaluator.evaluate_chair_design(image_path, prompt)  
    
    def batch_evaluate_scripts(self, script_prompt_pairs: list, name_prefix: str = "test") -> dict:  
        """  
        æ‰¹é‡è¯„ä¼°è„šæœ¬ç”Ÿæˆçš„å›¾åƒ  
        
        Args:  
            script_prompt_pairs: [(script, prompt), ...] åˆ—è¡¨  
            name_prefix: æ–‡ä»¶åå‰ç¼€  
            
        Returns:  
            dict: æ‰¹é‡è¯„ä¼°ç»“æžœ  
        """  
        results = []  
        total_score = 0  
        success_count = 0  
        
        for i, (script, prompt) in enumerate(script_prompt_pairs):  
            obj_name = f"{name_prefix}_{i+1:02d}"  
            
            # æ¸²æŸ“å›¾åƒ  
            image_path = self.run_blender_script_and_render(script, obj_name)  
            
            if image_path:  
                # è¯„ä¼°å›¾åƒè´¨é‡  
                evaluation = self.evaluate_image_with_openai(image_path, prompt)  
                results.append({  
                    'obj_name': obj_name,  
                    'prompt': prompt,  
                    'image_path': image_path,  
                    'evaluation': evaluation  
                })  
                
                score = evaluation.get('total_score', 0)  
                total_score += score  
                success_count += 1  
                
                print(f"âœ… {obj_name}: {score}/100")  
            else:  
                print(f"âŒ {obj_name}: æ¸²æŸ“å¤±è´¥")  
                results.append({  
                    'obj_name': obj_name,  
                    'prompt': prompt,  
                    'image_path': None,  
                    'evaluation': {'total_score': 0, 'comments': 'æ¸²æŸ“å¤±è´¥'}  
                })  
        
        return {  
            'individual_results': results,  
            'total_score': total_score,  
            'average_score': total_score / success_count if success_count > 0 else 0,  
            'success_count': success_count,  
            'total_count': len(script_prompt_pairs)  
        }